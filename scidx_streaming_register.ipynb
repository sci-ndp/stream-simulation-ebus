{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98ec0dc",
   "metadata": {},
   "source": [
    "#  scidx_streaming URL Registration\n",
    "\n",
    "This notebook crawls the public index at:\n",
    "\n",
    "**https://horel.chpc.utah.edu/data/meop/data/**\n",
    "\n",
    "It extracts file links (recursively), filters them to only include files whose **filename contains a year ≥ 2020**, and then **registers** those URLs into your **`scidx_streaming`** deployment.\n",
    "\n",
    "> **Note:** You will need valid credentials and the correct POP/API endpoint for your `scidx_streaming` instance. This notebook includes a dry-run mode so you can verify which URLs would be registered before actually registering them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d279d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running locally and you need these packages:\n",
    "# %pip install requests beautifulsoup4 urllib3\n",
    "# %pip install scidx-streaming  # uncomment if you have access to this package\n",
    "\n",
    "import re\n",
    "import queue\n",
    "import urllib.parse as up\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Set, Tuple\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ndp_ep import APIClient\n",
    "from scidx_streaming import StreamingClient\n",
    "import os, datetime\n",
    "import pandas as pd\n",
    "import msgpack\n",
    "import blosc\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "from typing import Dict, Any, List\n",
    "from pathlib import Path\n",
    "from kafka.errors import MessageSizeTooLargeError\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92bd7af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Client initialized. User ID: fc624925-ef09-447d-bf16-378066799275\n"
     ]
    }
   ],
   "source": [
    "# ---- Configuration ----\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# The root MEOP index to crawl\n",
    "BASE_URL = os.getenv(\"BASE_URL\")\n",
    "\n",
    "# Only register a file if its filename contains a year >= MIN_YEAR\n",
    "MIN_YEAR = 2020\n",
    "MAX_YEAR = 2025\n",
    "\n",
    "# Allowed file extensions to consider \n",
    "ALLOWED_EXTENSIONS = {'.csv', '.txt', '.json', '.nc'}\n",
    "\n",
    "# Crawl settings\n",
    "MAX_DEPTH = 3            # set higher if needed; beware of deep trees\n",
    "TIMEOUT = 15             # seconds for HTTP requests\n",
    "RESPECT_HOST = True      # only follow links on the same host as BASE_URL\n",
    "\n",
    "# Registration settings\n",
    "# read token from .env file\n",
    "TOKEN = os.getenv(\"TOKEN\")\n",
    "API_URL = os.getenv(\"API_URL\")\n",
    "SERVER = os.getenv(\"SERVER\")\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_HOST = os.getenv(\"KAFKA_HOST\")\n",
    "KAFKA_PORT = os.getenv(\"KAFKA_PORT\")\n",
    "BOOTSTRAP = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\n",
    "CHUNK_SIZE = 25_000  # starting rows per message\n",
    "SOFT_CAP_BYTES = 950_000  # stay under common 1MB broker limit\n",
    "\n",
    "# initializing ndp_ep APIClient\n",
    "client = APIClient(base_url=API_URL, token=TOKEN)\n",
    "streaming = StreamingClient(client)\n",
    "print(f\"Streaming Client initialized. User ID: {streaming.user_id}\")\n",
    "date_time_now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "org_name = \"ebus_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of organizations\n",
    "organizations = client.list_organizations(server=SERVER)\n",
    "\n",
    "\n",
    "# If the organization already exists, delete it\n",
    "if org_name in organizations:\n",
    "    print(f\"Organization '{org_name}' already exists.\")\n",
    "else:\n",
    "    print(f\"Organization '{org_name}' does not exist. Proceeding to create it.\")\n",
    "    # registering organization\n",
    "    org_data = {\n",
    "        \"name\": org_name,\n",
    "        \"title\": org_name,\n",
    "        \"description\": \"Sumaiya test organization for testing purposes\",\n",
    "    }\n",
    "    try:\n",
    "        client.register_organization(org_data,server=SERVER)\n",
    "        print(f\"Organization '{org_name}' registered successfully.\")\n",
    "    except ValueError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13929017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_host(url_a: str, url_b: str) -> bool:\n",
    "    \"\"\"Return True if url_b is on the same hostname as url_a.\"\"\"\n",
    "    pa = up.urlparse(url_a)\n",
    "    pb = up.urlparse(url_b)\n",
    "    return pa.netloc.lower() == pb.netloc.lower()\n",
    "\n",
    "def is_directory_link(href: str) -> bool:\n",
    "    \"\"\"Heuristic: treat trailing '/' as a directory link.\"\"\"\n",
    "    return href.endswith('/')\n",
    "\n",
    "def is_allowed_file(href: str, allowed_ext: Set[str]) -> bool:\n",
    "    \"\"\"Return True if href ends with one of the allowed extensions.\"\"\"\n",
    "    path = up.urlparse(href).path\n",
    "    for ext in allowed_ext:\n",
    "        if path.lower().endswith(ext.lower()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_years_from_filename(url: str) -> List[int]:\n",
    "    \"\"\"Return all 4-digit years found in the filename part of the URL.\"\"\"\n",
    "    path = up.urlparse(url).path\n",
    "    fname = path.split('/')[-1]\n",
    "    years = re.findall(r'(?:19|20)\\d{2}', fname)\n",
    "    return [int(y) for y in years]\n",
    "\n",
    "def should_register(url: str, min_year: int, max_year: int) -> bool:\n",
    "    years = extract_years_from_filename(url)\n",
    "    if not years:\n",
    "        return False\n",
    "    return min(years) >= min_year and max(years) <= max_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6682887",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CrawlResult:\n",
    "    visited_pages: Set[str]\n",
    "    file_urls: List[str]\n",
    "\n",
    "def crawl_index(start_url: str, max_depth: int = 3, timeout: int = 15, respect_host: bool = True,\n",
    "                allowed_ext: Set[str] = None) -> CrawlResult:\n",
    "    if allowed_ext is None:\n",
    "        allowed_ext = set()\n",
    "    start = up.urlparse(start_url)\n",
    "    start_host = start.netloc\n",
    "\n",
    "    visited_pages: Set[str] = set()\n",
    "    collected_files: List[str] = []\n",
    "\n",
    "    Q = queue.Queue()\n",
    "    Q.put((start_url, 0))\n",
    "\n",
    "    while not Q.empty():\n",
    "        url, depth = Q.get()\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to fetch {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        anchors = soup.find_all('a', href=True)\n",
    "\n",
    "        for a in anchors:\n",
    "            href = up.urljoin(url, a['href'])\n",
    "            if not href.startswith('http'):\n",
    "                continue\n",
    "            if respect_host and not same_host(start_url, href):\n",
    "                continue\n",
    "            if is_directory_link(href):\n",
    "                if depth < max_depth:\n",
    "                    Q.put((href, depth + 1))\n",
    "                continue\n",
    "            if is_allowed_file(href, allowed_ext):\n",
    "                collected_files.append(href)\n",
    "\n",
    "    return CrawlResult(visited_pages=visited_pages, file_urls=sorted(set(collected_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac69ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl = crawl_index(\n",
    "    start_url=BASE_URL,\n",
    "    max_depth=MAX_DEPTH,\n",
    "    timeout=TIMEOUT,\n",
    "    respect_host=RESPECT_HOST,\n",
    "    allowed_ext=ALLOWED_EXTENSIONS\n",
    ")\n",
    "\n",
    "print(f\"Visited pages: {len(crawl.visited_pages)}\")\n",
    "print(f\"Discovered candidate files: {len(crawl.file_urls)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_url(url_list: List[str], min_year: int, max_year: int) -> List[str]:\n",
    "    filtered_urls = [u for u in url_list if should_register(u, min_year, max_year)]\n",
    "    # print(f\"Files containing {min_year} <= year <= {max_year} : {len(filtered_urls)}\\n\")\n",
    "    return filtered_urls\n",
    "\n",
    "\n",
    "def generate_resource_name(url: str) -> str:\n",
    "    path = up.urlparse(url).path\n",
    "    fname = path.split('/')[-1] or \"resource\"\n",
    "    # lowercase + ascii-only (drop non-ascii)\n",
    "    fname = fname.encode(\"ascii\", \"ignore\").decode(\"ascii\").lower()\n",
    "    # replace any disallowed char with '-'\n",
    "    fname = re.sub(r'[^a-z0-9_-]+', '-', fname)\n",
    "    # collapse repeats and trim separators\n",
    "    fname = re.sub(r'[-_]{2,}', '-', fname).strip('-_')\n",
    "    # fallback if empty after sanitization\n",
    "    if not fname:\n",
    "        fname = \"resource\"\n",
    "    return fname\n",
    "\n",
    "\n",
    "def generate_resource_title(url: str) -> str:\n",
    "    path = up.urlparse(url).path\n",
    "    fname = path.split('/')[-1]\n",
    "    title = fname.replace('_', ' ').replace('.csv', '')\n",
    "    return f\"Sensor Data – {title}\"\n",
    "\n",
    "def generate_file_type(url: str) -> str:\n",
    "    return Path(url).suffix.lstrip(\".\").upper() or \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def generate_description_for_file_from_url(url: str) -> str:\n",
    "    parts = []\n",
    "\n",
    "    # VEHICLE:\n",
    "    # 1) BUS/TRX/TRAIN/RAIL with a required numeric id (e.g., BUS01, TRX03, TRAIN02, RAIL1)\n",
    "    # VEHICLE:\n",
    "    # BUS and RAIL require numeric IDs; TRX may appear with or without an ID.\n",
    "    m_bus  = re.search(r'(?:^|[/_])BUS(?P<id>\\d+)(?=[_.\\/]|$)', url, re.IGNORECASE)\n",
    "    m_trx  = re.search(r'(?:^|[/_])TRX(?P<id>\\d*)(?=[_.\\/]|$)', url, re.IGNORECASE)  # id optional\n",
    "    m_rail = re.search(r'(?:^|[/_])RAIL(?P<id>\\d+)(?=[_.\\/]|$)', url, re.IGNORECASE)\n",
    "\n",
    "    if m_bus:\n",
    "        parts.append(f\"Vehicle: Bus {m_bus.group('id')}\")\n",
    "    elif m_trx:\n",
    "        tid = m_trx.group('id')\n",
    "        parts.append(f\"Vehicle: Train {tid}\" if tid else \"Vehicle: Train\")\n",
    "    elif m_rail:\n",
    "        parts.append(f\"Vehicle: Rail {m_rail.group('id')}\")\n",
    "    else:\n",
    "        # EBUS: optional number; if digits follow, include them\n",
    "        m = re.search(r'(?:^|[/_])EBUS(?P<id>\\d*)(?=[_.\\/]|$)', url, re.IGNORECASE)\n",
    "        if m:\n",
    "            eid = m.group('id')\n",
    "            parts.append(f\"Vehicle: E-bus{(' ' + eid) if eid else ''}\")\n",
    "        else:\n",
    "            parts.append(\"Vehicle: Unknown\")\n",
    "\n",
    "    # DATE: try exact range first (YYYYMMDDHHMM_YYYYMMDDHHMM), then monthly (YYYY_MM)\n",
    "    m_range = re.search(r'_(\\d{12})_(\\d{12})(?=[^0-9]|$)', url)\n",
    "    m_month = re.search(r'_(\\d{4})_(\\d{2})(?=[^0-9]|$)', url)\n",
    "\n",
    "    if m_range:\n",
    "        start, end = m_range.groups()\n",
    "        start_fmt = f\"{start[:4]}-{start[4:6]}-{start[6:8]} {start[8:10]}:{start[10:12]}\"\n",
    "        end_fmt   = f\"{end[:4]}-{end[4:6]}-{end[6:8]} {end[8:10]}:{end[10:12]}\"\n",
    "        parts.append(f\"Data period: {start_fmt} → {end_fmt}\")\n",
    "    elif m_month:\n",
    "        year, month = m_month.groups()\n",
    "        parts.append(f\"Data period: {year}-{month}\")\n",
    "    else:\n",
    "        parts.append(\"Data period: Unknown\")\n",
    "\n",
    "    # FILE TYPE\n",
    "    parts.append(f\"File type: {generate_file_type(url)}\")\n",
    "\n",
    "    # FLAGS\n",
    "    low = url.lower()\n",
    "    if \"noqc\" in low:\n",
    "        parts.append(\"File marked 'noqc' (no quality control).\")\n",
    "    if re.search(r'(^|[/_])min([_/\\.]|$)', low):\n",
    "        parts.append(\"File marked 'min' appears to be minute-resolution of data.\")\n",
    "    if \"/meop/\" in low:\n",
    "        parts.append(\"File marked 'meop' (Mobile Environment Observation Platform) where sensors are attached to UTA.\")\n",
    "\n",
    "    # PROCESSING LEVEL (Level 2 / Level 3)\n",
    "    if re.search(r'(?<![a-z0-9])level[-_]?2(?![a-z0-9])', low):\n",
    "        parts.append(\"Data processing level: Level 2 (modified on raw data)\")\n",
    "    elif re.search(r'(?<![a-z0-9])level[-_]?3(?![a-z0-9])', low):\n",
    "        parts.append(\"Data processing level: Level 3 (modified on Level 2 data)\")\n",
    "    else:\n",
    "        parts.append(\"Data processing level: data is not modified.\")\n",
    "\n",
    "    return f\"This dataset is available at {url}. \" + \" \".join(parts)\n",
    "\n",
    "\n",
    "\n",
    "def generate_payloads(filtered_urls: List[str]) -> List[Dict[str, Any]]:\n",
    "    return [\n",
    "        {\n",
    "            'resource_name': generate_resource_name(u),\n",
    "            'resource_title': generate_resource_title(u),\n",
    "            'type': 'url',\n",
    "            'resource_url': u,\n",
    "            'notes': generate_description_for_file_from_url(u),\n",
    "            'file_type': generate_file_type(u),\n",
    "            'owner_org': org_name,\n",
    "        }\n",
    "        for u in filtered_urls\n",
    "    ]\n",
    "\n",
    "def register_in_scidx(payloads) -> List[str]:\n",
    "    \"\"\"Register URL-based data objects in scidx_streaming.\n",
    "    Replace the body with your actual scidx_streaming client calls.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    \n",
    "    for meta in payloads:\n",
    "        try:\n",
    "            response = client.register_url(meta, server=SERVER)\n",
    "            print(response)\n",
    "            ids.append(response[\"id\"])\n",
    "        except Exception as e:\n",
    "            print(str(e)) \n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967430bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read URLs from text file\n",
    "with open('urls_20250828_094831.txt', 'r') as file:\n",
    "    filtered_urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(filtered_urls)} URLs from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc058050",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_urls = generate_url(crawl.file_urls, MIN_YEAR, MAX_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = generate_payloads(filtered_urls)\n",
    "for payload in payloads:\n",
    "    print(payload[\"notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = register_in_scidx(payloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource_url_file = \"urls_\" + date_time_now + \".txt\"\n",
    "# print(resource_url_file)\n",
    "\n",
    "# with open(resource_url_file, 'w') as f:\n",
    "#     for url in filtered_urls:\n",
    "#         f.write(url + '\\n')\n",
    "\n",
    "# payload_file = \"payloads_\" + date_time_now + \".txt\"\n",
    "# print(payload_file)\n",
    "\n",
    "# with open(payload_file, 'w') as f:\n",
    "#     for payload in payloads:\n",
    "#         f.write(str(payload) + '\\n')\n",
    "\n",
    "# resource_name_file = \"names_\" + date_time_now + \".txt\"\n",
    "# print(resource_name_file)\n",
    "\n",
    "# with open(resource_name_file, 'w') as f:\n",
    "#     for meta in payloads:\n",
    "#         f.write(meta[\"resource_name\"] + '\\n')\n",
    "\n",
    "# resource_id_file = \"ids_\" + date_time_now + \".txt\"\n",
    "# print(resource_id_file)\n",
    "\n",
    "# with open(resource_id_file, 'w') as f:\n",
    "#         for id in ids:\n",
    "#             f.write(id + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = client.search_datasets([org_name],server=SERVER)\n",
    "\n",
    "for dataset in search_result:\n",
    "    print(f\"Found dataset: {dataset['id']} - {dataset['name']}\")\n",
    "    if dataset[\"owner_org\"] == org_name:\n",
    "        client.delete_resource_by_name(dataset[\"name\"], server=SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['bus13', 'data is not modified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728215b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_data(data: dict) -> bytes:\n",
    "    packed = msgpack.packb(data, use_bin_type=True)\n",
    "    return blosc.compress(packed, cname=\"zstd\", clevel=5, shuffle=blosc.SHUFFLE)\n",
    "\n",
    "def stream_register(list_of_keywords: list):\n",
    "    search_result = client.search_datasets(list_of_keywords, server=SERVER)\n",
    "    print(f\"Search result count: {len(search_result)}\")\n",
    "    topics = []\n",
    "\n",
    "    for dataset in search_result:\n",
    "        resource_id = dataset[\"id\"]\n",
    "        resource_url = dataset[\"resources\"][0][\"url\"]\n",
    "        resource_name = dataset[\"resources\"][0][\"name\"]\n",
    "        print(f\"Found dataset: {resource_name} - {resource_url}\")\n",
    "\n",
    "        df = pd.read_csv(resource_url, low_memory=False)\n",
    "        total_rows = len(df)\n",
    "        print(f\"Loaded CSV with {total_rows} rows and {len(df.columns)} columns\")\n",
    "\n",
    "        # Kafka Producer\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=BOOTSTRAP,\n",
    "            acks=\"all\",\n",
    "            linger_ms=0,\n",
    "            max_request_size=5 * 1024 * 1024,  # client cap; broker may be lower\n",
    "        )\n",
    "\n",
    "        key = resource_url.encode(\"utf-8\")\n",
    "        topic = resource_name\n",
    "        topics.append(topic)\n",
    "\n",
    "        # ----- adaptive loop (replaces the for-range loop) -----\n",
    "        i = 0\n",
    "        chunk_size = CHUNK_SIZE\n",
    "        min_rows = 1\n",
    "\n",
    "        while i < total_rows:\n",
    "            j = min(i + chunk_size, total_rows)\n",
    "            chunk = df.iloc[i:j]\n",
    "\n",
    "            payload = {\n",
    "                \"values\": chunk.to_dict(orient=\"list\"),\n",
    "                \"stream_info\": {\n",
    "                    \"source_url\": resource_url,\n",
    "                    \"rows\": int(len(chunk)),\n",
    "                    \"cols\": list(chunk.columns),\n",
    "                    \"chunk_index\": int(i // max(1, chunk_size)),\n",
    "                    \"start_row\": int(i),\n",
    "                    \"end_row\": int(j - 1),\n",
    "                    \"encoding\": \"msgpack+blosc(zstd5,shuffle)\",\n",
    "                },\n",
    "            }\n",
    "            blob = compress_data(payload)\n",
    "\n",
    "            # pre-shrink if we're near/over a conservative cap\n",
    "            if len(blob) > SOFT_CAP_BYTES and len(chunk) > min_rows:\n",
    "                ratio = (SOFT_CAP_BYTES * 0.85) / len(blob)\n",
    "                new_size = max(min_rows, int(len(chunk) * max(0.10, min(0.80, ratio))))\n",
    "                print(f\"Chunk ~{len(blob)} bytes > cap; reducing rows {len(chunk)} → {new_size} and retrying.\")\n",
    "                chunk_size = new_size\n",
    "                continue  # retry same offset\n",
    "\n",
    "            try:\n",
    "                producer.send(topic, key=key, value=blob).get(timeout=30)\n",
    "                print(f\"Sent chunk {i // max(1, chunk_size)} with {len(chunk)} rows (compressed: {len(blob)} bytes)\")\n",
    "                i = j  # advance\n",
    "            except MessageSizeTooLargeError:\n",
    "                if len(chunk) <= min_rows:\n",
    "                    # a single row is too large even after compression → cannot proceed\n",
    "                    raise\n",
    "                # halve and retry same offset\n",
    "                new_size = max(min_rows, len(chunk) // 2)\n",
    "                print(f\"Broker rejected message (too large). Reducing rows {len(chunk)} → {new_size} and retrying.\")\n",
    "                chunk_size = new_size\n",
    "                # loop continues with same i\n",
    "\n",
    "        producer.flush()\n",
    "\n",
    "        payload = {\n",
    "            \"topic\": topic,\n",
    "            \"status\": \"active\",\n",
    "            \"format\": \"stream\",\n",
    "            \"url\": BOOTSTRAP,\n",
    "            \"description\": f\"Kafka stream for topic {topic}. This is a general stream without any filters.\",\n",
    "            \"name\": f\"stream_dataset {topic}\"\n",
    "        }\n",
    "        \n",
    "        patch_response = client.patch_general_dataset(\n",
    "            dataset_id=resource_id,\n",
    "            server=SERVER,\n",
    "            data={\"resources\": [payload]}\n",
    "        )\n",
    "        print(f\"Dataset Id: {resource_id}, Dataset Name: {resource_name}, Patch response: {patch_response}\")\n",
    "        # producer.close()  # uncomment if you want to close per dataset\n",
    "        \n",
    "\n",
    "    return topics\n",
    "\n",
    "# Example call\n",
    "topics = stream_register(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddfc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_topic_file = \"topics_\" + date_time_now + \".txt\"\n",
    "print(resource_topic_file)\n",
    "\n",
    "with open(resource_topic_file, 'w') as f:\n",
    "    for topic in topics:\n",
    "        f.write(topic + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56db31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening to Kafka topic bus13_2025_02-csv\n",
      "{'source_url': 'https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv', 'rows': 25000, 'cols': ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged'], 'chunk_index': 0, 'start_row': 0, 'end_row': 24999, 'encoding': 'msgpack+blosc(zstd5,shuffle)'}\n",
      "Source: https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "\n",
      " Binary chunk received: 25000 rows\n",
      "Columns: ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged']\n",
      "Chunk index: 0\n",
      "→ ('UTC', 'ddeg', 'ddeg', 'm', 'm/s', 'deg', 'binary', 'volts', 'degC', 'degC', '%', 'ug/m3', 'ug/m3', 'ug/m3', 'ug/m3', 'L/min', 'degC', '%', 'hpa', 'code', 'ppbv', 'L/min', 'degC', 'hpa', 'binary', 'binary', 'binary')\n",
      "→ ('2025-02-01T00:00:00', '40.753014', '-111.865395', '1317.20', '12.76', '179.70', '1.00', '14.05', '8.04', '5.29', '43.87', '14.60', '32.10', '64.40', '195.90', '1.00', '11.78', '26.70', '654.90', '0.00', '6.80', '1.74', '21.90', '835.30', '0', '0', '0')\n",
      "→ ('2025-02-01T00:00:05', '40.752590', '-111.865387', '1317.50', '5.50', '174.20', '1.00', '14.05', '8.04', '5.29', '43.95', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '4.50', '1.73', '21.90', '835.80', '0', '0', '0')\n",
      "{'source_url': 'https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv', 'rows': 25000, 'cols': ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged'], 'chunk_index': 1, 'start_row': 25000, 'end_row': 49999, 'encoding': 'msgpack+blosc(zstd5,shuffle)'}\n",
      "Source: https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "\n",
      " Binary chunk received: 25000 rows\n",
      "Columns: ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged']\n",
      "Chunk index: 1\n",
      "→ ('2025-02-05T20:19:40', '40.633209', '-111.897972', '1318.20', '5.20', '160.80', '1.00', '13.79', '11.87', '10.51', '71.77', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '24.70', '1.77', '25.40', '827.30', '1', '0', '0')\n",
      "→ ('2025-02-05T20:19:45', '40.633053', '-111.897934', '1318.50', '2.42', '166.90', '1.00', '13.79', '11.88', '10.51', '71.63', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '22.40', '1.78', '25.40', '827.80', '1', '0', '0')\n",
      "→ ('2025-02-05T20:19:50', '40.632954', '-111.897804', '1318.80', '5.81', '106.60', '1.00', '13.79', '11.88', '10.53', '71.63', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '25.50', '1.79', '25.40', '827.60', '1', '0', '0')\n",
      "{'source_url': 'https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv', 'rows': 25000, 'cols': ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged'], 'chunk_index': 2, 'start_row': 50000, 'end_row': 74999, 'encoding': 'msgpack+blosc(zstd5,shuffle)'}\n",
      "Source: https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "\n",
      " Binary chunk received: 25000 rows\n",
      "Columns: ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged']\n",
      "Chunk index: 2\n",
      "→ ('2025-02-07T23:59:05', '40.763817', '-111.909660', '1293.30', '0.00', '150.40', '1.00', '14.00', '6.71', '5.51', '53.64', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '34.60', '1.72', '20.10', '835.20', '0', '0', '1')\n",
      "→ ('2025-02-07T23:59:10', '40.763809', '-111.909660', '1293.40', '0.00', '150.40', '1.00', '14.01', '6.71', '5.51', '53.90', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '32.60', '1.71', '20.10', '834.80', '0', '0', '1')\n",
      "→ ('2025-02-07T23:59:15', '40.763805', '-111.909660', '1293.40', '0.00', '150.40', '1.00', '14.01', '6.71', '5.51', '54.01', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '33.40', '1.71', '20.10', '834.70', '0', '0', '1')\n",
      "{'source_url': 'https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv', 'rows': 25000, 'cols': ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged'], 'chunk_index': 3, 'start_row': 75000, 'end_row': 99999, 'encoding': 'msgpack+blosc(zstd5,shuffle)'}\n",
      "Source: https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "\n",
      " Binary chunk received: 25000 rows\n",
      "Columns: ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged']\n",
      "Chunk index: 3\n",
      "→ ('2025-02-12T10:21:15', '-9999.000000', '-9999.000000', '-9999.00', '-9999.00', '-9999.00', '0.00', '14.75', '-4.77', '-5.97', '69.56', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '1.53', '-3.30', '834.80', '0', '0', '1')\n",
      "→ ('2025-02-12T10:21:20', '-9999.000000', '-9999.000000', '-9999.00', '-9999.00', '-9999.00', '0.00', '14.74', '-4.78', '-5.97', '69.54', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '1.54', '-3.30', '827.40', '0', '0', '1')\n",
      "→ ('2025-02-12T10:21:25', '-9999.000000', '-9999.000000', '-9999.00', '-9999.00', '-9999.00', '0.00', '14.73', '-4.78', '-5.97', '69.53', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '1.53', '-3.20', '827.80', '0', '0', '1')\n",
      "{'source_url': 'https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv', 'rows': 25000, 'cols': ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged'], 'chunk_index': 4, 'start_row': 100000, 'end_row': 124999, 'encoding': 'msgpack+blosc(zstd5,shuffle)'}\n",
      "Source: https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "\n",
      " Binary chunk received: 25000 rows\n",
      "Columns: ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged']\n",
      "Chunk index: 4\n",
      "→ ('2025-02-14T03:04:15', '40.761478', '-111.910629', '1269.80', '0.00', '57.20', '1.00', '14.00', '5.61', '2.81', '88.20', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '19.20', '1.60', '17.40', '810.30', '1', '0', '1')\n",
      "→ ('2025-02-14T03:04:20', '40.761429', '-111.910698', '1269.70', '0.46', '202.20', '1.00', '14.00', '5.61', '2.83', '88.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '23.90', '1.61', '17.40', '810.20', '1', '0', '1')\n",
      "→ ('2025-02-14T03:04:25', '40.761402', '-111.910728', '1269.60', '0.00', '202.20', '1.00', '14.00', '5.61', '2.81', '87.90', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '17.90', '1.61', '17.40', '811.30', '1', '0', '1')\n",
      "{'source_url': 'https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv', 'rows': 25000, 'cols': ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged'], 'chunk_index': 5, 'start_row': 125000, 'end_row': 149999, 'encoding': 'msgpack+blosc(zstd5,shuffle)'}\n",
      "Source: https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "\n",
      " Binary chunk received: 25000 rows\n",
      "Columns: ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged']\n",
      "Chunk index: 5\n",
      "→ ('2025-02-19T11:27:35', '40.761406', '-111.910789', '1265.70', '0.00', '46.70', '1.00', '14.14', '3.80', '1.58', '56.50', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '11.40', '1.67', '15.20', '841.30', '0', '0', '1')\n",
      "→ ('2025-02-19T11:27:40', '40.761402', '-111.910789', '1265.70', '0.00', '46.70', '1.00', '14.14', '3.80', '1.58', '56.52', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '11.30', '1.67', '15.20', '841.30', '0', '0', '1')\n",
      "→ ('2025-02-19T11:27:45', '40.761395', '-111.910797', '1265.70', '0.00', '46.70', '1.00', '14.14', '3.80', '1.57', '56.56', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '11.70', '1.66', '15.20', '841.90', '0', '0', '1')\n",
      "{'source_url': 'https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv', 'rows': 25000, 'cols': ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged'], 'chunk_index': 6, 'start_row': 150000, 'end_row': 174999, 'encoding': 'msgpack+blosc(zstd5,shuffle)'}\n",
      "Source: https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "\n",
      " Binary chunk received: 25000 rows\n",
      "Columns: ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged']\n",
      "Chunk index: 6\n",
      "→ ('2025-02-21T10:47:00', '40.761501', '-111.910896', '1302.00', '0.00', '4.80', '1.00', '14.13', '2.16', '0.51', '97.60', '1.60', '1.80', '2.00', '2.30', '1.00', '6.39', '31.50', '660.80', '0.00', '-9999.00', '1.48', '10.90', '815.60', '1', '0', '1')\n",
      "→ ('2025-02-21T10:47:05', '40.761459', '-111.910866', '1301.50', '0.00', '4.80', '1.00', '14.13', '2.16', '0.51', '97.60', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '1.47', '10.90', '815.50', '1', '0', '1')\n",
      "→ ('2025-02-21T10:47:10', '40.761406', '-111.910851', '1300.80', '0.00', '133.30', '1.00', '14.13', '2.16', '0.51', '97.60', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '1.48', '10.90', '814.40', '1', '0', '1')\n",
      "{'source_url': 'https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv', 'rows': 25000, 'cols': ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged'], 'chunk_index': 7, 'start_row': 175000, 'end_row': 199999, 'encoding': 'msgpack+blosc(zstd5,shuffle)'}\n",
      "Source: https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "\n",
      " Binary chunk received: 25000 rows\n",
      "Columns: ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged']\n",
      "Chunk index: 7\n",
      "→ ('2025-02-24T21:26:30', '40.710934', '-111.865509', '1301.70', '0.00', '175.20', '1.00', '13.54', '21.96', '18.89', '24.30', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '35.60', '1.60', '34.60', '802.70', '0', '0', '0')\n",
      "→ ('2025-02-24T21:26:35', '40.710934', '-111.865509', '1301.70', '0.00', '175.20', '1.00', '13.54', '21.96', '18.89', '24.22', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '35.60', '1.59', '34.60', '802.50', '0', '0', '0')\n",
      "→ ('2025-02-24T21:26:40', '40.710934', '-111.865509', '1301.70', '0.00', '175.20', '1.00', '13.55', '21.96', '18.89', '24.21', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '36.10', '1.61', '34.60', '802.50', '0', '0', '0')\n",
      "{'source_url': 'https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv', 'rows': 25000, 'cols': ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged'], 'chunk_index': 8, 'start_row': 200000, 'end_row': 224999, 'encoding': 'msgpack+blosc(zstd5,shuffle)'}\n",
      "Source: https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "\n",
      " Binary chunk received: 25000 rows\n",
      "Columns: ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged']\n",
      "Chunk index: 8\n",
      "→ ('2025-02-26T19:24:30', '40.773571', '-111.865623', '1375.50', '8.03', '180.90', '1.00', '13.89', '12.32', '8.10', '34.30', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '45.80', '1.76', '24.90', '836.40', '0', '0', '0')\n",
      "→ ('2025-02-26T19:24:35', '40.773216', '-111.865623', '1371.90', '7.10', '181.40', '1.00', '13.89', '12.32', '8.14', '34.22', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '41.30', '1.75', '24.90', '836.60', '0', '0', '0')\n",
      "→ ('2025-02-26T19:24:40', '40.772968', '-111.865646', '1371.30', '3.04', '178.80', '1.00', '13.90', '12.32', '8.18', '33.95', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '38.10', '1.75', '24.90', '836.60', '0', '0', '0')\n",
      "{'source_url': 'https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv', 'rows': 6650, 'cols': ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged'], 'chunk_index': 9, 'start_row': 225000, 'end_row': 231649, 'encoding': 'msgpack+blosc(zstd5,shuffle)'}\n",
      "Source: https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "\n",
      " Binary chunk received: 6650 rows\n",
      "Columns: ['Timestamp', 'Latitude', 'Longitude', 'Elevation', 'GPS_Speed', 'GPS_Direction', 'GPS_RMC_Valid', 'Battery_Voltage', 'Bus_Box_Temperature', 'Bus_Top_Temperature', 'Bus_Top_Relative_Humidity', 'ES405_PM1_Concentration', 'ES405_PM2.5_Concentration', 'ES405_PM4_Concentration', 'ES405_PM10_Concentration', 'ES405_Air_Flow_Rate', 'ES405_Internal_Air_Temperature', 'ES405_Internal_Relative_Humidity', 'ES405_Internal_Air_Pressure', 'ES405_Error_Code', '2B_Ozone_Concentration', '2B_Air_Flow_Rate', '2B_Internal_Air_Temperature', '2B_Internal_Air_Pressure', 'PM2.5_Data_Flagged', 'Ozone_Data_Flagged', 'GPS_Data_Flagged']\n",
      "Chunk index: 9\n",
      "→ ('2025-02-28T14:45:50', '40.764027', '-111.909683', '1284.90', '0.00', '164.70', '1.00', '14.25', '0.73', '0.89', '55.09', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '1.70', '1.66', '14.00', '843.70', '0', '0', '1')\n",
      "→ ('2025-02-28T14:45:55', '40.764027', '-111.909683', '1284.90', '0.00', '164.70', '1.00', '14.24', '0.73', '0.90', '55.16', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '-9999.00', '5.30', '1.67', '14.00', '844.10', '0', '0', '1')\n",
      "→ ('2025-02-28T14:46:00', '40.764027', '-111.909683', '1284.90', '0.00', '164.70', '1.00', '14.25', '0.73', '0.90', '55.22', '5.20', '8.70', '15.80', '54.80', '1.00', '5.22', '30.90', '660.40', '0.00', '3.80', '1.67', '14.00', '844.10', '0', '0', '1')\n"
     ]
    }
   ],
   "source": [
    "def try_decompress(blob: bytes):\n",
    "    try:\n",
    "        unpacked = blosc.decompress(blob)\n",
    "        return msgpack.unpackb(unpacked, raw=False)\n",
    "    except Exception:\n",
    "        return None  # Not a compressed binary message\n",
    "\n",
    "def stream_consumption(topics: List[str]):\n",
    "    for topic in topics:\n",
    "        print(f\"Listening to Kafka topic {topic}\")\n",
    "        consumer = KafkaConsumer(\n",
    "            topic,\n",
    "            bootstrap_servers=f\"{BOOTSTRAP}\",\n",
    "            auto_offset_reset='earliest',\n",
    "            group_id=None,\n",
    "            value_deserializer=lambda x: x  # Raw bytes; decode manually\n",
    "        )\n",
    "\n",
    "        seen_chunks = set()\n",
    "        for message in consumer:\n",
    "            raw = message.value\n",
    "\n",
    "            # Try decompressing as binary\n",
    "            data = try_decompress(raw)\n",
    "            if data:\n",
    "                info = data.get(\"stream_info\", {})\n",
    "                print(info)\n",
    "                print(f\"Source: {info.get('source_url', 'N/A')}\")\n",
    "                print(f\"\\n Binary chunk received: {info.get('rows', '?')} rows\")\n",
    "                print(f\"Columns: {info.get('cols', '?')}\")\n",
    "                chunk_index = info.get('chunk_index', 'N/A')\n",
    "                print(f\"Chunk index: {chunk_index}\")\n",
    "                if chunk_index in seen_chunks:\n",
    "                    print(\"Duplicate chunk index detected; stopping consumption.\")\n",
    "                    break\n",
    "                seen_chunks.add(chunk_index)\n",
    "\n",
    "                preview = list(zip(*data[\"values\"].values()))[:3]\n",
    "                for row in preview:\n",
    "                    print(\"→\", row)\n",
    "            else:\n",
    "                # Fallback: treat as plain UTF-8 text\n",
    "                try:\n",
    "                    text = raw.decode(\"utf-8\")\n",
    "                    print(f\"Text message: {text}\")\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(f\"Unrecognized message format: {e}\")\n",
    "\n",
    "# read topics from topic file\n",
    "with open(\"topics_20250830_090935.txt\", \"r\") as f:\n",
    "    topics = [line.strip() for line in f.readlines()]\n",
    "stream_consumption([topics[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c49bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"stream_dataset\",\"bus13\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144bfaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset: f78a704c-6753-4784-ba0a-468371ab88e6 - bus13_2025_03-csv\n",
      "Found dataset: 6b3ebce3-dc4c-41bf-aae0-2bc2e5d1165f - bus13_2025_02-csv\n",
      "Found dataset: 646e2c90-cb81-4d6e-a631-6e191d7b0aca - bus13_2025_01-csv\n",
      "Found dataset: 680d4ea7-118b-4578-996e-e36b7023c237 - bus13_2024_12-csv\n",
      "Found dataset: 11c70de5-61b4-49fb-9d04-8347f41099c6 - bus13_2024_11-csv\n",
      "Found dataset: 9f5accf8-e78d-46a8-a493-e974dbcfb7f9 - bus13_2024_10-csv\n",
      "Found dataset: 144ae773-eb90-4296-9ed3-e63ba718fe16 - bus13_2024_09-csv\n",
      "Found dataset: 89778dab-ce08-4ddf-b970-ce660431a974 - bus13_2024_08-csv\n",
      "Found dataset: 04ab5eea-9de2-4727-8b42-8b2eae008d48 - bus13_2024_07-csv\n",
      "Found dataset: c0337729-05e6-4442-a68e-d4b194f10349 - bus13_2024_06-csv\n",
      "Found dataset: 2a9b3705-49f7-439e-a28a-76ddc59ecc1a - bus13_2024_05-csv\n",
      "Found dataset: 62a51e2d-9af2-4b66-a0ab-e92c96c282f4 - bus13_2024_04-csv\n",
      "Found dataset: 3ba199c8-f179-44ab-8d10-adfe1abca6b2 - bus13_noqc_202409040000_202409050600-csv\n"
     ]
    }
   ],
   "source": [
    "result=client.search_datasets(keywords,server=SERVER)\n",
    "\n",
    "for dataset in result:\n",
    "    print(f\"Found dataset: {dataset['id']} - {dataset['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af3ca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream created: data_stream_fc624925-ef09-447d-bf16-378066799275_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "stream = await streaming.create_kafka_stream(\n",
    "    keywords=keywords,\n",
    "    match_all=True,\n",
    "    filter_semantics=[]\n",
    ")\n",
    "\n",
    "topic = stream.data_stream_id\n",
    "print(f\"Stream created: {topic}\")\n",
    "# Start consuming the filtered Kafka stream\n",
    "consumer = streaming.consume_kafka_messages(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1dad47f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m df=consumer.dataframe\n\u001b[32m      3\u001b[39m df = pd.DataFrame(df)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = pd.DataFrame(\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.to_dict())\n\u001b[32m      5\u001b[39m df.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/pandas/core/indexing.py:1752\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index by location index with a non-integer key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._ixs(key, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/pandas/core/indexing.py:1685\u001b[39m, in \u001b[36m_iLocIndexer._validate_integer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1683\u001b[39m len_axis = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj._get_axis(axis))\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key >= len_axis \u001b[38;5;129;01mor\u001b[39;00m key < -len_axis:\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msingle positional indexer is out-of-bounds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Get the data from the consumer\n",
    "df=consumer.dataframe\n",
    "df = pd.DataFrame(df)\n",
    "df = pd.DataFrame(df.iloc[0].to_dict())\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "admin = AdminClient({'bootstrap.servers': '10.244.2.206:9092'})\n",
    "\n",
    "\n",
    "fs = admin.delete_topics(topics, operation_timeout=30)\n",
    "\n",
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()  # raises exception if failed\n",
    "        print(f\"Topic '{topic}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete topic '{topic}': {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
