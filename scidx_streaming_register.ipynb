{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98ec0dc",
   "metadata": {},
   "source": [
    "#  scidx_streaming URL Registration\n",
    "\n",
    "This notebook crawls the public index at:\n",
    "\n",
    "**https://horel.chpc.utah.edu/data/meop/data/**\n",
    "\n",
    "It extracts file links (recursively), filters them to only include files whose **filename contains a year ≥ 2020**, and then **registers** those URLs into your **`scidx_streaming`** deployment.\n",
    "\n",
    "> **Note:** You will need valid credentials and the correct POP/API endpoint for your `scidx_streaming` instance. This notebook includes a dry-run mode so you can verify which URLs would be registered before actually registering them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92bd7af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Client initialized. User ID: fc624925-ef09-447d-bf16-378066799275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3416335/2659213675.py:31: UserWarning: Could not determine API version from status endpoint. Version compatibility cannot be verified.\n",
      "  client = APIClient(base_url=API_URL, token=TOKEN)\n",
      "/uufs/chpc.utah.edu/common/home/u1494915/stream-simulation-ebus/.venv/lib/python3.12/site-packages/scidx_streaming/client/init_client.py:49: UserWarning: Could not determine API version from status endpoint. Version compatibility cannot be verified.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "from ndp_ep import APIClient\n",
    "from scidx_streaming import StreamingClient\n",
    "import os, datetime\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from helper_func import register_kafka\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "# ---- Configuration ----\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "# Registration settings\n",
    "# read token from .env file\n",
    "TOKEN = os.getenv(\"TOKEN\")\n",
    "API_URL = os.getenv(\"API_URL\")\n",
    "SERVER = os.getenv(\"SERVER\")\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_HOST = os.getenv(\"KAFKA_HOST\")\n",
    "KAFKA_PORT = os.getenv(\"KAFKA_PORT\")\n",
    "BOOTSTRAP = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\n",
    "CHUNK_SIZE = 25_000  # starting rows per message\n",
    "SOFT_CAP_BYTES = 950_000  # stay under common 1MB broker limit\n",
    "\n",
    "# initializing ndp_ep APIClient\n",
    "client = APIClient(base_url=API_URL, token=TOKEN)\n",
    "streaming = StreamingClient(client)\n",
    "print(f\"Streaming Client initialized. User ID: {streaming.user_id}\")\n",
    "date_time_now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "org_name = \"kafka_stream\"\n",
    "\n",
    "bus13_urls = [\n",
    "    # \"https://horel.chpc.utah.edu/data/meop/level3/ebus_2024/ebus_min_2024_12.csv\",\n",
    "    # \"https://horel.chpc.utah.edu/data/meop/level3/ebus_2025/ebus_min_2025_01.csv\",\n",
    "    \"https://horel.chpc.utah.edu/data/meop/level3/ebus_2025/ebus_min_2025_02.csv\",\n",
    "    # \"https://horel.chpc.utah.edu/data/meop/data/BUS12_2024_12.csv\",\n",
    "    # \"https://horel.chpc.utah.edu/data/meop/data/BUS12_2025_01.csv\"\n",
    "]\n",
    "\n",
    "# pandas configuration\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ee1046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'c45ca757-7f10-413f-9d21-946274d203a0'}\n"
     ]
    }
   ],
   "source": [
    "# list all topic of kafka\n",
    "register_kafka(bus13_urls, org_name, client, BOOTSTRAP, KAFKA_HOST, KAFKA_PORT, SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "beb3a2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'c45ca757-7f10-413f-9d21-946274d203a0', 'name': 'kafka_ebus_min_2025_02-csv', 'title': 'Kafka Sensor Data – ebus min 2025 02', 'owner_org': 'kafka_stream', 'notes': \"The kafka stream is generated from csv dataset. This dataset is available at https://horel.chpc.utah.edu/data/meop/level3/ebus_2025/ebus_min_2025_02.csv. Vehicle: E-bus Data period: 2025-02 File type: CSV File marked 'min' appears to be minute-resolution of data. File marked 'meop' (Mobile Environment Observation Platform) where sensors are attached to UTA. Data processing level: Level 3 (modified on Level 2 data)\", 'resources': [{'id': 'd9c20e63-cb0c-4223-845d-f14af04872f1', 'url': '', 'name': 'ebus_min_2025_02-csv', 'description': 'Kafka topic ebus_min_2025_02-csv hosted at 10.244.2.206:9092', 'format': 'kafka'}], 'extras': {'host': '10.244.2.206', 'port': '9092', 'topic': 'ebus_min_2025_02-csv'}}]\n"
     ]
    }
   ],
   "source": [
    "# print(client.search_datasets(\"kafka_ebus_min_2024_12-csv\", server=SERVER))\n",
    "# print(client.search_datasets(\"kafka_ebus_min_2025_01-csv\", server=SERVER))\n",
    "print(client.search_datasets(\"kafka_ebus_min_2025_02-csv\", server=SERVER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "28beca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Topic 'data_stream_fc624925-ef09-447d-bf16-378066799275_3' does not exist. Skipping deletion.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': \"Stream 'data_stream_fc624925-ef09-447d-bf16-378066799275_3' already deleted or does not exist.\"}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await streaming.delete_stream(\"data_stream_fc624925-ef09-447d-bf16-378066799275_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9af3ca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream created: data_stream_fc624925-ef09-447d-bf16-378066799275_4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stream = await streaming.create_kafka_stream(\n",
    "    # keywords = [\"kafka_ebus_min_2024_12\"],\n",
    "    # keywords=[\"kafka_ebus_min_2025_01-csv\"],\n",
    "    keywords=[\"kafka_ebus_min_2025_02-csv\"],\n",
    "    match_all=True,\n",
    "    filter_semantics=[]\n",
    ")\n",
    "\n",
    "topic = stream.data_stream_id\n",
    "print(f\"Stream created: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3176351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df row_count: 651\n",
      "df row_count: 1302\n",
      "Reached file size > 200MB (269.80 MB)\n"
     ]
    }
   ],
   "source": [
    "# --- Topic Selection ---\n",
    "try:\n",
    "    topic = input(\"Enter new topic name e.g data_stream_fc624925-ef09-447d-bf16-378066799275_x: \").strip()\n",
    "except Exception as e:\n",
    "    exit\n",
    "\n",
    "# --- File Path Selection ---\n",
    "try:\n",
    "    file_path = input(\"Enter new file path e.g: raw_stream/sensor_data_x.csv: \").strip()\n",
    "except Exception:\n",
    "    exit\n",
    "\n",
    "# --- Kafka Consumption and Writing ---\n",
    "consumer = streaming.consume_kafka_messages(topic)\n",
    "time.sleep(5)\n",
    "df = None\n",
    "row_count = 0\n",
    "count = 1\n",
    "max_size_bytes = 200 * 1024 * 1024  # 200 MB\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if os.path.exists(file_path) and os.path.getsize(file_path) > max_size_bytes:\n",
    "            print(f\"Reached file size > 200MB ({os.path.getsize(file_path) / (1024*1024):.2f} MB)\")\n",
    "            break\n",
    "\n",
    "        df = consumer.dataframe\n",
    "        if df is not None and not df.empty:\n",
    "            df.to_csv(file_path, index=False, mode=\"a\")\n",
    "            row_count += len(df)\n",
    "            if row_count > count * 200:\n",
    "                count += 1\n",
    "                print(f\"df row_count: {row_count}\")\n",
    "        del df\n",
    "        df = None\n",
    "except KeyboardInterrupt as e:\n",
    "    print(\"Interrupted:\", e)\n",
    "finally:\n",
    "    consumer.stop()\n",
    "    if df is not None and not df.empty:\n",
    "        row_count += len(df)\n",
    "        print(f\"df row_count: {row_count}\")\n",
    "        df.to_csv(file_path, index=False, mode=\"a\")\n",
    "        del df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
