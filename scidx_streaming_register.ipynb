{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98ec0dc",
   "metadata": {},
   "source": [
    "#  scidx_streaming URL Registration\n",
    "\n",
    "This notebook crawls the public index at:\n",
    "\n",
    "**https://horel.chpc.utah.edu/data/meop/data/**\n",
    "\n",
    "It extracts file links (recursively), filters them to only include files whose **filename contains a year ≥ 2020**, and then **registers** those URLs into your **`scidx_streaming`** deployment.\n",
    "\n",
    "> **Note:** You will need valid credentials and the correct POP/API endpoint for your `scidx_streaming` instance. This notebook includes a dry-run mode so you can verify which URLs would be registered before actually registering them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d279d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If running locally and you need these packages:\n",
    "# %pip install requests beautifulsoup4 urllib3\n",
    "# %pip install scidx-streaming  # uncomment if you have access to this package\n",
    "\n",
    "import re\n",
    "import queue\n",
    "import urllib.parse as up\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Set, Tuple\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ndp_ep import APIClient\n",
    "from scidx_streaming import StreamingClient\n",
    "import os, datetime\n",
    "import pandas as pd\n",
    "import msgpack\n",
    "import blosc\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "from typing import Dict, Any, List\n",
    "from pathlib import Path\n",
    "from kafka.errors import MessageSizeTooLargeError\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92bd7af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Client initialized. User ID: fc624925-ef09-447d-bf16-378066799275\n"
     ]
    }
   ],
   "source": [
    "# ---- Configuration ----\n",
    "\n",
    "# The root MEOP index to crawl\n",
    "BASE_URL = 'https://horel.chpc.utah.edu/data/meop/'\n",
    "\n",
    "# Only register a file if its filename contains a year >= MIN_YEAR\n",
    "MIN_YEAR = 2020\n",
    "MAX_YEAR = 2025\n",
    "\n",
    "# Allowed file extensions to consider \n",
    "ALLOWED_EXTENSIONS = {'.csv', '.txt', '.json', '.nc'}\n",
    "\n",
    "# Crawl settings\n",
    "MAX_DEPTH = 3            # set higher if needed; beware of deep trees\n",
    "TIMEOUT = 15             # seconds for HTTP requests\n",
    "RESPECT_HOST = True      # only follow links on the same host as BASE_URL\n",
    "\n",
    "# Registration settings\n",
    "# read token from .env file\n",
    "load_dotenv(override=True)\n",
    "TOKEN = os.getenv(\"TOKEN\")\n",
    "API_URL = '10.244.2.206:8003/'  # <-- change to your POP API base URL\n",
    "SERVER = \"local\"\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_HOST = \"10.244.2.206\"\n",
    "KAFKA_PORT = 9092\n",
    "BOOTSTRAP = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\n",
    "CHUNK_SIZE = 25_000  # starting rows per message\n",
    "SOFT_CAP_BYTES = 950_000  # stay under common 1MB broker limit\n",
    "\n",
    "# initializing ndp_ep APIClient\n",
    "client = APIClient(base_url=API_URL, token=TOKEN)\n",
    "streaming = StreamingClient(client)\n",
    "print(f\"Streaming Client initialized. User ID: {streaming.user_id}\")\n",
    "date_time_now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "org_name = \"ebus_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of organizations\n",
    "organizations = client.list_organizations(server=SERVER)\n",
    "\n",
    "\n",
    "# If the organization already exists, delete it\n",
    "if org_name in organizations:\n",
    "    print(f\"Organization '{org_name}' already exists.\")\n",
    "else:\n",
    "    print(f\"Organization '{org_name}' does not exist. Proceeding to create it.\")\n",
    "    # registering organization\n",
    "    org_data = {\n",
    "        \"name\": org_name,\n",
    "        \"title\": org_name,\n",
    "        \"description\": \"Sumaiya test organization for testing purposes\",\n",
    "    }\n",
    "    try:\n",
    "        client.register_organization(org_data,server=SERVER)\n",
    "        print(f\"Organization '{org_name}' registered successfully.\")\n",
    "    except ValueError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13929017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_host(url_a: str, url_b: str) -> bool:\n",
    "    \"\"\"Return True if url_b is on the same hostname as url_a.\"\"\"\n",
    "    pa = up.urlparse(url_a)\n",
    "    pb = up.urlparse(url_b)\n",
    "    return pa.netloc.lower() == pb.netloc.lower()\n",
    "\n",
    "def is_directory_link(href: str) -> bool:\n",
    "    \"\"\"Heuristic: treat trailing '/' as a directory link.\"\"\"\n",
    "    return href.endswith('/')\n",
    "\n",
    "def is_allowed_file(href: str, allowed_ext: Set[str]) -> bool:\n",
    "    \"\"\"Return True if href ends with one of the allowed extensions.\"\"\"\n",
    "    path = up.urlparse(href).path\n",
    "    for ext in allowed_ext:\n",
    "        if path.lower().endswith(ext.lower()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_years_from_filename(url: str) -> List[int]:\n",
    "    \"\"\"Return all 4-digit years found in the filename part of the URL.\"\"\"\n",
    "    path = up.urlparse(url).path\n",
    "    fname = path.split('/')[-1]\n",
    "    years = re.findall(r'(?:19|20)\\d{2}', fname)\n",
    "    return [int(y) for y in years]\n",
    "\n",
    "def should_register(url: str, min_year: int, max_year: int) -> bool:\n",
    "    years = extract_years_from_filename(url)\n",
    "    if not years:\n",
    "        return False\n",
    "    return min(years) >= min_year and max(years) <= max_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6682887",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CrawlResult:\n",
    "    visited_pages: Set[str]\n",
    "    file_urls: List[str]\n",
    "\n",
    "def crawl_index(start_url: str, max_depth: int = 3, timeout: int = 15, respect_host: bool = True,\n",
    "                allowed_ext: Set[str] = None) -> CrawlResult:\n",
    "    if allowed_ext is None:\n",
    "        allowed_ext = set()\n",
    "    start = up.urlparse(start_url)\n",
    "    start_host = start.netloc\n",
    "\n",
    "    visited_pages: Set[str] = set()\n",
    "    collected_files: List[str] = []\n",
    "\n",
    "    Q = queue.Queue()\n",
    "    Q.put((start_url, 0))\n",
    "\n",
    "    while not Q.empty():\n",
    "        url, depth = Q.get()\n",
    "        if url in visited_pages:\n",
    "            continue\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to fetch {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        anchors = soup.find_all('a', href=True)\n",
    "\n",
    "        for a in anchors:\n",
    "            href = up.urljoin(url, a['href'])\n",
    "            if not href.startswith('http'):\n",
    "                continue\n",
    "            if respect_host and not same_host(start_url, href):\n",
    "                continue\n",
    "            if is_directory_link(href):\n",
    "                if depth < max_depth:\n",
    "                    Q.put((href, depth + 1))\n",
    "                continue\n",
    "            if is_allowed_file(href, allowed_ext):\n",
    "                collected_files.append(href)\n",
    "\n",
    "    return CrawlResult(visited_pages=visited_pages, file_urls=sorted(set(collected_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac69ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl = crawl_index(\n",
    "    start_url=BASE_URL,\n",
    "    max_depth=MAX_DEPTH,\n",
    "    timeout=TIMEOUT,\n",
    "    respect_host=RESPECT_HOST,\n",
    "    allowed_ext=ALLOWED_EXTENSIONS\n",
    ")\n",
    "\n",
    "print(f\"Visited pages: {len(crawl.visited_pages)}\")\n",
    "print(f\"Discovered candidate files: {len(crawl.file_urls)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_url(url_list: List[str], min_year: int, max_year: int) -> List[str]:\n",
    "    filtered_urls = [u for u in url_list if should_register(u, min_year, max_year)]\n",
    "    # print(f\"Files containing {min_year} <= year <= {max_year} : {len(filtered_urls)}\\n\")\n",
    "    return filtered_urls\n",
    "\n",
    "\n",
    "def generate_resource_name(url: str) -> str:\n",
    "    path = up.urlparse(url).path\n",
    "    fname = path.split('/')[-1] or \"resource\"\n",
    "    # lowercase + ascii-only (drop non-ascii)\n",
    "    fname = fname.encode(\"ascii\", \"ignore\").decode(\"ascii\").lower()\n",
    "    # replace any disallowed char with '-'\n",
    "    fname = re.sub(r'[^a-z0-9_-]+', '-', fname)\n",
    "    # collapse repeats and trim separators\n",
    "    fname = re.sub(r'[-_]{2,}', '-', fname).strip('-_')\n",
    "    # fallback if empty after sanitization\n",
    "    if not fname:\n",
    "        fname = \"resource\"\n",
    "    return fname\n",
    "\n",
    "\n",
    "def generate_resource_title(url: str) -> str:\n",
    "    path = up.urlparse(url).path\n",
    "    fname = path.split('/')[-1]\n",
    "    title = fname.replace('_', ' ').replace('.csv', '')\n",
    "    return f\"Sensor Data – {title}\"\n",
    "\n",
    "def generate_file_type(url: str) -> str:\n",
    "    return Path(url).suffix.lstrip(\".\").upper() or \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def generate_description_for_file_from_url(url: str) -> str:\n",
    "    parts = []\n",
    "\n",
    "    # VEHICLE:\n",
    "    # 1) BUS/TRX/TRAIN/RAIL with a required numeric id (e.g., BUS01, TRX03, TRAIN02, RAIL1)\n",
    "    # VEHICLE:\n",
    "    # BUS and RAIL require numeric IDs; TRX may appear with or without an ID.\n",
    "    m_bus  = re.search(r'(?:^|[/_])BUS(?P<id>\\d+)(?=[_.\\/]|$)', url, re.IGNORECASE)\n",
    "    m_trx  = re.search(r'(?:^|[/_])TRX(?P<id>\\d*)(?=[_.\\/]|$)', url, re.IGNORECASE)  # id optional\n",
    "    m_rail = re.search(r'(?:^|[/_])RAIL(?P<id>\\d+)(?=[_.\\/]|$)', url, re.IGNORECASE)\n",
    "\n",
    "    if m_bus:\n",
    "        parts.append(f\"Vehicle: Bus {m_bus.group('id')}\")\n",
    "    elif m_trx:\n",
    "        tid = m_trx.group('id')\n",
    "        parts.append(f\"Vehicle: Train {tid}\" if tid else \"Vehicle: Train\")\n",
    "    elif m_rail:\n",
    "        parts.append(f\"Vehicle: Rail {m_rail.group('id')}\")\n",
    "    else:\n",
    "        # EBUS: optional number; if digits follow, include them\n",
    "        m = re.search(r'(?:^|[/_])EBUS(?P<id>\\d*)(?=[_.\\/]|$)', url, re.IGNORECASE)\n",
    "        if m:\n",
    "            eid = m.group('id')\n",
    "            parts.append(f\"Vehicle: E-bus{(' ' + eid) if eid else ''}\")\n",
    "        else:\n",
    "            parts.append(\"Vehicle: Unknown\")\n",
    "\n",
    "    # DATE: try exact range first (YYYYMMDDHHMM_YYYYMMDDHHMM), then monthly (YYYY_MM)\n",
    "    m_range = re.search(r'_(\\d{12})_(\\d{12})(?=[^0-9]|$)', url)\n",
    "    m_month = re.search(r'_(\\d{4})_(\\d{2})(?=[^0-9]|$)', url)\n",
    "\n",
    "    if m_range:\n",
    "        start, end = m_range.groups()\n",
    "        start_fmt = f\"{start[:4]}-{start[4:6]}-{start[6:8]} {start[8:10]}:{start[10:12]}\"\n",
    "        end_fmt   = f\"{end[:4]}-{end[4:6]}-{end[6:8]} {end[8:10]}:{end[10:12]}\"\n",
    "        parts.append(f\"Data period: {start_fmt} → {end_fmt}\")\n",
    "    elif m_month:\n",
    "        year, month = m_month.groups()\n",
    "        parts.append(f\"Data period: {year}-{month}\")\n",
    "    else:\n",
    "        parts.append(\"Data period: Unknown\")\n",
    "\n",
    "    # FILE TYPE\n",
    "    parts.append(f\"File type: {generate_file_type(url)}\")\n",
    "\n",
    "    # FLAGS\n",
    "    low = url.lower()\n",
    "    if \"noqc\" in low:\n",
    "        parts.append(\"File marked 'noqc' (no quality control).\")\n",
    "    if re.search(r'(^|[/_])min([_/\\.]|$)', low):\n",
    "        parts.append(\"File marked 'min' appears to be minute-resolution of data.\")\n",
    "    if \"/meop/\" in low:\n",
    "        parts.append(\"File marked 'meop' (Mobile Environment Observation Platform) where sensors are attached to UTA.\")\n",
    "\n",
    "    # PROCESSING LEVEL (Level 2 / Level 3)\n",
    "    if re.search(r'(?<![a-z0-9])level[-_]?2(?![a-z0-9])', low):\n",
    "        parts.append(\"Data processing level: Level 2 (modified on raw data)\")\n",
    "    elif re.search(r'(?<![a-z0-9])level[-_]?3(?![a-z0-9])', low):\n",
    "        parts.append(\"Data processing level: Level 3 (modified on Level 2 data)\")\n",
    "    else:\n",
    "        parts.append(\"Data processing level: data is not modified.\")\n",
    "\n",
    "    return f\"This dataset is available at {url}. \" + \" \".join(parts)\n",
    "\n",
    "\n",
    "\n",
    "def generate_payloads(filtered_urls: List[str]) -> List[Dict[str, Any]]:\n",
    "    return [\n",
    "        {\n",
    "            'resource_name': generate_resource_name(u),\n",
    "            'resource_title': generate_resource_title(u),\n",
    "            'type': 'url',\n",
    "            'resource_url': u,\n",
    "            'notes': generate_description_for_file_from_url(u),\n",
    "            'file_type': generate_file_type(u),\n",
    "            'owner_org': org_name,\n",
    "        }\n",
    "        for u in filtered_urls\n",
    "    ]\n",
    "\n",
    "def register_in_scidx(payloads) -> List[str]:\n",
    "    \"\"\"Register URL-based data objects in scidx_streaming.\n",
    "    Replace the body with your actual scidx_streaming client calls.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    \n",
    "    for meta in payloads:\n",
    "        try:\n",
    "            response = client.register_url(meta, server=SERVER)\n",
    "            print(response)\n",
    "            ids.append(response[\"id\"])\n",
    "        except Exception as e:\n",
    "            print(str(e)) \n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967430bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read URLs from text file\n",
    "with open('urls_20250828_094831.txt', 'r') as file:\n",
    "    filtered_urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(filtered_urls)} URLs from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc058050",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_urls = generate_url(crawl.file_urls, MIN_YEAR, MAX_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = generate_payloads(filtered_urls)\n",
    "for payload in payloads:\n",
    "    print(payload[\"notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = register_in_scidx(payloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource_url_file = \"urls_\" + date_time_now + \".txt\"\n",
    "# print(resource_url_file)\n",
    "\n",
    "# with open(resource_url_file, 'w') as f:\n",
    "#     for url in filtered_urls:\n",
    "#         f.write(url + '\\n')\n",
    "\n",
    "# payload_file = \"payloads_\" + date_time_now + \".txt\"\n",
    "# print(payload_file)\n",
    "\n",
    "# with open(payload_file, 'w') as f:\n",
    "#     for payload in payloads:\n",
    "#         f.write(str(payload) + '\\n')\n",
    "\n",
    "# resource_name_file = \"names_\" + date_time_now + \".txt\"\n",
    "# print(resource_name_file)\n",
    "\n",
    "# with open(resource_name_file, 'w') as f:\n",
    "#     for meta in payloads:\n",
    "#         f.write(meta[\"resource_name\"] + '\\n')\n",
    "\n",
    "resource_id_file = \"ids_\" + date_time_now + \".txt\"\n",
    "print(resource_id_file)\n",
    "\n",
    "with open(resource_id_file, 'w') as f:\n",
    "        for id in ids:\n",
    "            f.write(id + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = client.search_datasets([org_name],server=SERVER)\n",
    "\n",
    "for dataset in search_result:\n",
    "    print(f\"Found dataset: {dataset['id']} - {dataset['name']}\")\n",
    "    if dataset[\"owner_org\"] == org_name:\n",
    "        client.delete_resource_by_name(dataset[\"name\"], server=SERVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55ea5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['bus13', 'data is not modified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "728215b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search result count: 13\n",
      "Found dataset: bus13_2025_03-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_03.csv\n",
      "Loaded CSV with 4255 rows and 27 columns\n",
      "Sent chunk 0 with 4255 rows (compressed: 75999 bytes)\n",
      "Dataset Id: f78a704c-6753-4784-ba0a-468371ab88e6, Dataset Name: bus13_2025_03-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2025_02-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_02.csv\n",
      "Loaded CSV with 231650 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 399062 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 421468 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 413221 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 427736 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 405341 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 398850 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 423835 bytes)\n",
      "Sent chunk 7 with 25000 rows (compressed: 415323 bytes)\n",
      "Sent chunk 8 with 25000 rows (compressed: 418961 bytes)\n",
      "Sent chunk 9 with 6650 rows (compressed: 118609 bytes)\n",
      "Dataset Id: 6b3ebce3-dc4c-41bf-aae0-2bc2e5d1165f, Dataset Name: bus13_2025_02-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2025_01-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2025_01.csv\n",
      "Loaded CSV with 288066 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 409715 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 330100 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 404773 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 425912 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 380207 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 422966 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 434396 bytes)\n",
      "Sent chunk 7 with 25000 rows (compressed: 395834 bytes)\n",
      "Sent chunk 8 with 25000 rows (compressed: 405420 bytes)\n",
      "Sent chunk 9 with 25000 rows (compressed: 421507 bytes)\n",
      "Sent chunk 10 with 25000 rows (compressed: 424379 bytes)\n",
      "Sent chunk 11 with 13066 rows (compressed: 235397 bytes)\n",
      "Dataset Id: 646e2c90-cb81-4d6e-a631-6e191d7b0aca, Dataset Name: bus13_2025_01-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2024_12-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2024_12.csv\n",
      "Loaded CSV with 322769 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 370543 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 441718 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 419307 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 345600 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 425149 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 426865 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 430242 bytes)\n",
      "Sent chunk 7 with 25000 rows (compressed: 431681 bytes)\n",
      "Sent chunk 8 with 25000 rows (compressed: 437295 bytes)\n",
      "Sent chunk 9 with 25000 rows (compressed: 404415 bytes)\n",
      "Sent chunk 10 with 25000 rows (compressed: 377588 bytes)\n",
      "Sent chunk 11 with 25000 rows (compressed: 384609 bytes)\n",
      "Sent chunk 12 with 22769 rows (compressed: 387746 bytes)\n",
      "Dataset Id: 680d4ea7-118b-4578-996e-e36b7023c237, Dataset Name: bus13_2024_12-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2024_11-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2024_11.csv\n",
      "Loaded CSV with 218506 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 397004 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 430922 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 420113 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 466908 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 392308 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 421607 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 422330 bytes)\n",
      "Sent chunk 7 with 25000 rows (compressed: 374546 bytes)\n",
      "Sent chunk 8 with 18506 rows (compressed: 315006 bytes)\n",
      "Dataset Id: 11c70de5-61b4-49fb-9d04-8347f41099c6, Dataset Name: bus13_2024_11-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2024_10-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2024_10.csv\n",
      "Loaded CSV with 186622 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 434826 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 416296 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 426475 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 417905 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 414379 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 404566 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 419093 bytes)\n",
      "Sent chunk 7 with 11622 rows (compressed: 201069 bytes)\n",
      "Dataset Id: 9f5accf8-e78d-46a8-a493-e974dbcfb7f9, Dataset Name: bus13_2024_10-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2024_09-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2024_09.csv\n",
      "Loaded CSV with 247554 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 420913 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 419524 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 401946 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 420340 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 414495 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 413655 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 430863 bytes)\n",
      "Sent chunk 7 with 25000 rows (compressed: 424449 bytes)\n",
      "Sent chunk 8 with 25000 rows (compressed: 414791 bytes)\n",
      "Sent chunk 9 with 22554 rows (compressed: 379426 bytes)\n",
      "Dataset Id: 144ae773-eb90-4296-9ed3-e63ba718fe16, Dataset Name: bus13_2024_09-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2024_08-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2024_08.csv\n",
      "Loaded CSV with 241739 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 433915 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 419709 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 427485 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 427819 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 428620 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 416581 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 422733 bytes)\n",
      "Sent chunk 7 with 25000 rows (compressed: 414607 bytes)\n",
      "Sent chunk 8 with 25000 rows (compressed: 413963 bytes)\n",
      "Sent chunk 9 with 16739 rows (compressed: 283050 bytes)\n",
      "Dataset Id: 89778dab-ce08-4ddf-b970-ce660431a974, Dataset Name: bus13_2024_08-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2024_07-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2024_07.csv\n",
      "Loaded CSV with 238100 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 407128 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 395905 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 343030 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 380851 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 435845 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 415345 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 408470 bytes)\n",
      "Sent chunk 7 with 25000 rows (compressed: 403501 bytes)\n",
      "Sent chunk 8 with 25000 rows (compressed: 427125 bytes)\n",
      "Sent chunk 9 with 13100 rows (compressed: 226850 bytes)\n",
      "Dataset Id: 04ab5eea-9de2-4727-8b42-8b2eae008d48, Dataset Name: bus13_2024_07-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2024_06-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2024_06.csv\n",
      "Loaded CSV with 249955 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 400288 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 422654 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 431964 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 424310 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 421891 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 419863 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 424730 bytes)\n",
      "Sent chunk 7 with 25000 rows (compressed: 431626 bytes)\n",
      "Sent chunk 8 with 25000 rows (compressed: 423021 bytes)\n",
      "Sent chunk 9 with 24955 rows (compressed: 395624 bytes)\n",
      "Dataset Id: c0337729-05e6-4442-a68e-d4b194f10349, Dataset Name: bus13_2024_06-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2024_05-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2024_05.csv\n",
      "Loaded CSV with 187570 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 440736 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 412005 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 437845 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 399441 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 441495 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 422570 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 418799 bytes)\n",
      "Sent chunk 7 with 12570 rows (compressed: 221584 bytes)\n",
      "Dataset Id: 2a9b3705-49f7-439e-a28a-76ddc59ecc1a, Dataset Name: bus13_2024_05-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_2024_04-csv - https://horel.chpc.utah.edu/data/meop/data/BUS13_2024_04.csv\n",
      "Loaded CSV with 202359 rows and 27 columns\n",
      "Sent chunk 0 with 25000 rows (compressed: 454644 bytes)\n",
      "Sent chunk 1 with 25000 rows (compressed: 421534 bytes)\n",
      "Sent chunk 2 with 25000 rows (compressed: 438719 bytes)\n",
      "Sent chunk 3 with 25000 rows (compressed: 451772 bytes)\n",
      "Sent chunk 4 with 25000 rows (compressed: 420213 bytes)\n",
      "Sent chunk 5 with 25000 rows (compressed: 434260 bytes)\n",
      "Sent chunk 6 with 25000 rows (compressed: 401270 bytes)\n",
      "Sent chunk 7 with 25000 rows (compressed: 401082 bytes)\n",
      "Sent chunk 8 with 2359 rows (compressed: 45362 bytes)\n",
      "Dataset Id: 62a51e2d-9af2-4b66-a0ab-e92c96c282f4, Dataset Name: bus13_2024_04-csv, Patch response: {'message': 'Dataset updated successfully'}\n",
      "Found dataset: bus13_noqc_202409040000_202409050600-csv - https://horel.chpc.utah.edu/data/meop/d_20240904/BUS13_noqc_202409040000_202409050600.csv\n",
      "Loaded CSV with 15419 rows and 27 columns\n",
      "Sent chunk 0 with 15419 rows (compressed: 267145 bytes)\n",
      "Dataset Id: 3ba199c8-f179-44ab-8d10-adfe1abca6b2, Dataset Name: bus13_noqc_202409040000_202409050600-csv, Patch response: {'message': 'Dataset updated successfully'}\n"
     ]
    }
   ],
   "source": [
    "def compress_data(data: dict) -> bytes:\n",
    "    packed = msgpack.packb(data, use_bin_type=True)\n",
    "    return blosc.compress(packed, cname=\"zstd\", clevel=5, shuffle=blosc.SHUFFLE)\n",
    "\n",
    "def stream_register(list_of_keywords: list):\n",
    "    search_result = client.search_datasets(list_of_keywords, server=SERVER)\n",
    "    print(f\"Search result count: {len(search_result)}\")\n",
    "    topics = []\n",
    "\n",
    "    for dataset in search_result:\n",
    "        resource_id = dataset[\"id\"]\n",
    "        resource_url = dataset[\"resources\"][0][\"url\"]\n",
    "        resource_name = dataset[\"resources\"][0][\"name\"]\n",
    "        print(f\"Found dataset: {resource_name} - {resource_url}\")\n",
    "\n",
    "        df = pd.read_csv(resource_url, low_memory=False)\n",
    "        total_rows = len(df)\n",
    "        print(f\"Loaded CSV with {total_rows} rows and {len(df.columns)} columns\")\n",
    "\n",
    "        # Kafka Producer\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=BOOTSTRAP,\n",
    "            acks=\"all\",\n",
    "            linger_ms=0,\n",
    "            max_request_size=5 * 1024 * 1024,  # client cap; broker may be lower\n",
    "        )\n",
    "\n",
    "        key = resource_url.encode(\"utf-8\")\n",
    "        topic = resource_name\n",
    "        topics.append(topic)\n",
    "\n",
    "        # ----- adaptive loop (replaces the for-range loop) -----\n",
    "        i = 0\n",
    "        chunk_size = CHUNK_SIZE\n",
    "        min_rows = 1\n",
    "\n",
    "        while i < total_rows:\n",
    "            j = min(i + chunk_size, total_rows)\n",
    "            chunk = df.iloc[i:j]\n",
    "\n",
    "            payload = {\n",
    "                \"values\": chunk.to_dict(orient=\"list\"),\n",
    "                \"stream_info\": {\n",
    "                    \"source_url\": resource_url,\n",
    "                    \"rows\": int(len(chunk)),\n",
    "                    \"cols\": list(chunk.columns),\n",
    "                    \"chunk_index\": int(i // max(1, chunk_size)),\n",
    "                    \"start_row\": int(i),\n",
    "                    \"end_row\": int(j - 1),\n",
    "                    \"encoding\": \"msgpack+blosc(zstd5,shuffle)\",\n",
    "                },\n",
    "            }\n",
    "            blob = compress_data(payload)\n",
    "\n",
    "            # pre-shrink if we're near/over a conservative cap\n",
    "            if len(blob) > SOFT_CAP_BYTES and len(chunk) > min_rows:\n",
    "                ratio = (SOFT_CAP_BYTES * 0.85) / len(blob)\n",
    "                new_size = max(min_rows, int(len(chunk) * max(0.10, min(0.80, ratio))))\n",
    "                print(f\"Chunk ~{len(blob)} bytes > cap; reducing rows {len(chunk)} → {new_size} and retrying.\")\n",
    "                chunk_size = new_size\n",
    "                continue  # retry same offset\n",
    "\n",
    "            try:\n",
    "                producer.send(topic, key=key, value=blob).get(timeout=30)\n",
    "                print(f\"Sent chunk {i // max(1, chunk_size)} with {len(chunk)} rows (compressed: {len(blob)} bytes)\")\n",
    "                i = j  # advance\n",
    "            except MessageSizeTooLargeError:\n",
    "                if len(chunk) <= min_rows:\n",
    "                    # a single row is too large even after compression → cannot proceed\n",
    "                    raise\n",
    "                # halve and retry same offset\n",
    "                new_size = max(min_rows, len(chunk) // 2)\n",
    "                print(f\"Broker rejected message (too large). Reducing rows {len(chunk)} → {new_size} and retrying.\")\n",
    "                chunk_size = new_size\n",
    "                # loop continues with same i\n",
    "\n",
    "        producer.flush()\n",
    "\n",
    "        payload = {\n",
    "            \"topic\": topic,\n",
    "            \"status\": \"active\",\n",
    "            \"format\": \"stream\",\n",
    "            \"url\": BOOTSTRAP,\n",
    "            \"description\": f\"Kafka stream for topic {topic}. This is a general stream without any filters.\",\n",
    "            \"name\": f\"stream_dataset {topic}\"\n",
    "        }\n",
    "        \n",
    "        patch_response = client.patch_general_dataset(\n",
    "            dataset_id=resource_id,\n",
    "            server=SERVER,\n",
    "            data={\"resources\": [payload]}\n",
    "        )\n",
    "        print(f\"Dataset Id: {resource_id}, Dataset Name: {resource_name}, Patch response: {patch_response}\")\n",
    "        # producer.close()  # uncomment if you want to close per dataset\n",
    "        \n",
    "\n",
    "    return topics\n",
    "\n",
    "# Example call\n",
    "topics = stream_register(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ddfc44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topics_20250830_090935.txt\n"
     ]
    }
   ],
   "source": [
    "resource_topic_file = \"topics_\" + date_time_now + \".txt\"\n",
    "print(resource_topic_file)\n",
    "\n",
    "with open(resource_topic_file, 'w') as f:\n",
    "    for topic in topics:\n",
    "        f.write(topic + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "144bfaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset: 62a51e2d-9af2-4b66-a0ab-e92c96c282f4 - bus13_2024_04-csv\n",
      "Found dataset: 2a9b3705-49f7-439e-a28a-76ddc59ecc1a - bus13_2024_05-csv\n",
      "Found dataset: c0337729-05e6-4442-a68e-d4b194f10349 - bus13_2024_06-csv\n",
      "Found dataset: 04ab5eea-9de2-4727-8b42-8b2eae008d48 - bus13_2024_07-csv\n",
      "Found dataset: 89778dab-ce08-4ddf-b970-ce660431a974 - bus13_2024_08-csv\n",
      "Found dataset: 144ae773-eb90-4296-9ed3-e63ba718fe16 - bus13_2024_09-csv\n",
      "Found dataset: 9f5accf8-e78d-46a8-a493-e974dbcfb7f9 - bus13_2024_10-csv\n",
      "Found dataset: 11c70de5-61b4-49fb-9d04-8347f41099c6 - bus13_2024_11-csv\n",
      "Found dataset: 680d4ea7-118b-4578-996e-e36b7023c237 - bus13_2024_12-csv\n",
      "Found dataset: 646e2c90-cb81-4d6e-a631-6e191d7b0aca - bus13_2025_01-csv\n",
      "Found dataset: 6b3ebce3-dc4c-41bf-aae0-2bc2e5d1165f - bus13_2025_02-csv\n",
      "Found dataset: f78a704c-6753-4784-ba0a-468371ab88e6 - bus13_2025_03-csv\n",
      "Found dataset: 3ba199c8-f179-44ab-8d10-adfe1abca6b2 - bus13_noqc_202409040000_202409050600-csv\n"
     ]
    }
   ],
   "source": [
    "result=client.search_datasets([\"stream_dataset\",\"bus13\"],server=SERVER)\n",
    "\n",
    "for dataset in result:\n",
    "    print(f\"Found dataset: {dataset['id']} - {dataset['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9af3ca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream created: data_stream_fc624925-ef09-447d-bf16-378066799275_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n",
      "Error decompressing message: int is not allowed for map key when strict_map_key=True\n",
      "Decompression failed: int is not allowed for map key when strict_map_key=True. Attempting UTF-8 decode.\n",
      "Failed to decode message: 'utf-8' codec can't decode byte 0x91 in position 2: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "stream = await streaming.create_kafka_stream(\n",
    "    keywords=keywords,\n",
    "    match_all=True,\n",
    "    filter_semantics=[]\n",
    ")\n",
    "\n",
    "topic = stream.data_stream_id\n",
    "print(f\"Stream created: {topic}\")\n",
    "# Start consuming the filtered Kafka stream\n",
    "consumer = streaming.consume_kafka_messages(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1dad47f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m df=consumer.dataframe\n\u001b[32m      3\u001b[39m df = pd.DataFrame(df)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = pd.DataFrame(\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.to_dict())\n\u001b[32m      5\u001b[39m df.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/pandas/core/indexing.py:1752\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index by location index with a non-integer key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._ixs(key, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/pandas/core/indexing.py:1685\u001b[39m, in \u001b[36m_iLocIndexer._validate_integer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1683\u001b[39m len_axis = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj._get_axis(axis))\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key >= len_axis \u001b[38;5;129;01mor\u001b[39;00m key < -len_axis:\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msingle positional indexer is out-of-bounds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Get the data from the consumer\n",
    "df=consumer.dataframe\n",
    "df = pd.DataFrame(df)\n",
    "df = pd.DataFrame(df.iloc[0].to_dict())\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "admin = AdminClient({'bootstrap.servers': '10.244.2.206:9092'})\n",
    "\n",
    "\n",
    "fs = admin.delete_topics(topics, operation_timeout=30)\n",
    "\n",
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()  # raises exception if failed\n",
    "        print(f\"Topic '{topic}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete topic '{topic}': {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
