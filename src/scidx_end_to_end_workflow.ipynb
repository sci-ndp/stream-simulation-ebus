{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10c37dd",
   "metadata": {},
   "source": [
    "# Forecast → Hotspots → Sensor Selection (SciDX) Workflow\n",
    "\n",
    "This notebook is the **end-to-end workflow** of the project using the same scripts in this repo:\n",
    "\n",
    "1. **(Optional) Download CAMS inputs** → `download_cams.py`  \n",
    "2. **Run Aurora** to generate regional forecast CSVs → `run_aurora.py` / `aurora_runner.py`  \n",
    "3. **Detect hotspot events** from forecast CSVs → `event_detection.py`  \n",
    "4. **Register hotspot CSV** into SciDX (with `updated_at` in `extras`) → `register_hotspots_csv_scidx.py`  \n",
    "5. **Register sensor sources** (e.g., Synoptic WebSockets) into SciDX → `register_synoptic_websockets.py`  \n",
    "6. **Orchestrate:** read hotspots + find nearest sensors → `orchestrate_hotspots_to_sensors.py`\n",
    "\n",
    "> Goal of the notebook: make it easy to see **what runs when**, **what files are produced**, and **how hotspot → nearby sensor selection works**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0867f157",
   "metadata": {},
   "source": [
    "## 0) Setup: environment + config\n",
    "\n",
    "This demo needs:\n",
    "\n",
    "- `API_URL`, `TOKEN` in your environment (for SciDX / NDP EP)\n",
    "- optional `SERVER` (defaults to `local`)\n",
    "- `config.yaml` at the project root with:\n",
    "  - `region.name` (e.g., `utah`)\n",
    "  - `hotspot.url` (where the hotspot CSV is hosted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bdc0570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API_URL: 10.244.2.206:8003\n",
      "SERVER : local\n",
      "TOKEN  : set\n",
      "region     : utah\n",
      "hotspot.url: https://drive.google.com/uc?export=download&id=1Hx1RIthTSPyw8qupKudDsiPZaabMvGa7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "API_URL = os.environ.get(\"API_URL\", \"\").strip()\n",
    "TOKEN  = os.environ.get(\"TOKEN\", \"\").strip()\n",
    "SERVER = (os.environ.get(\"SERVER\") or \"local\").strip()\n",
    "\n",
    "print(\"API_URL:\", API_URL)\n",
    "print(\"SERVER :\", SERVER)\n",
    "print(\"TOKEN  :\", \"set\" if TOKEN else \"MISSING\")\n",
    "\n",
    "# Load config.yaml\n",
    "project_root = Path.cwd()\n",
    "cfg_path = project_root.parent / \"config.yaml\"\n",
    "\n",
    "with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "region = str(cfg.get(\"region\", {}).get(\"name\", \"\")).strip()\n",
    "hotspot_url = str(cfg.get(\"hotspot\", {}).get(\"url\", \"\")).strip()\n",
    "\n",
    "print(\"region     :\", region)\n",
    "print(\"hotspot.url:\", hotspot_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07fbc1a",
   "metadata": {},
   "source": [
    "## 1) Connect to SciDX (catalog/control plane)\n",
    "\n",
    "We use:\n",
    "- `ndp_ep.APIClient` for dataset search/metadata\n",
    "- `scidx_streaming.StreamingClient` for streaming-related helpers and registration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8549e0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming user_id: 987104e7-e6d3-47f2-82a0-0d3f620aea70\n"
     ]
    }
   ],
   "source": [
    "from ndp_ep import APIClient\n",
    "from scidx_streaming import StreamingClient\n",
    "\n",
    "client = APIClient(base_url=API_URL, token=TOKEN)\n",
    "streaming = StreamingClient(client)\n",
    "\n",
    "print(\"Streaming user_id:\", streaming.user_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc67df3",
   "metadata": {},
   "source": [
    "## 2) (Optional) Download CAMS inputs\n",
    "\n",
    "If you already have CAMS NetCDF files locally, you can skip this section.\n",
    "\n",
    "Otherwise run the helper script which pulls CAMS data needed for Aurora.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa71eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the script as-is (recommended if you're following the repo workflow)\n",
    "!python download_cams.py\n",
    "\n",
    "\n",
    "print(\"Downloaded data successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8f6b0",
   "metadata": {},
   "source": [
    "## 3) Run Aurora → produce regional forecast artifact(s)\n",
    "\n",
    "This stage turns CAMS inputs into a **regional forecast CSV** for your configured region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c969d58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Aurora for 2025-10-20 → /uufs/chpc.utah.edu/common/home/u1494915/stream-simulation-ebus/data/processed/predictions/2025-10-20_0000-1200_12h_utah.csv\n",
      "Loading static variables...\n",
      "Loading CAMS datasets...\n",
      "Loading Aurora model...\n",
      "Surface steps: 13 Atmos steps: 5\n",
      "Processing hour 1/12\n",
      "Processing hour 2/12\n",
      "Processing hour 3/12\n",
      "Processing hour 4/12\n",
      "Processing hour 5/12\n",
      "Processing hour 6/12\n",
      "Processing hour 7/12\n",
      "Processing hour 8/12\n",
      "Processing hour 9/12\n",
      "Processing hour 10/12\n",
      "Processing hour 11/12\n",
      "Processing hour 12/12\n",
      "Processing hour 1/12\n",
      "Processing hour 2/12\n",
      "Processing hour 3/12\n",
      "Processing hour 4/12\n",
      "Processing hour 5/12\n",
      "Processing hour 6/12\n",
      "Processing hour 7/12\n",
      "Processing hour 8/12\n",
      "Processing hour 9/12\n",
      "Processing hour 10/12\n",
      "Processing hour 11/12\n",
      "Processing hour 12/12\n",
      "\n",
      "Saved hourly predictions with all surface variables to /uufs/chpc.utah.edu/common/home/u1494915/stream-simulation-ebus/data/processed/predictions/2025-10-20_0000-1200_12h_utah.csv\n",
      "A forecast CSV artifact created.\n"
     ]
    }
   ],
   "source": [
    "# Run Aurora using the repo script (preferred)\n",
    "!python run_aurora.py\n",
    "\n",
    "\n",
    "print(\"A forecast CSV artifact created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e273959",
   "metadata": {},
   "source": [
    "## 4) Event detection on forecast output → generate hotspot CSV\n",
    "\n",
    "This stage reads the forecast artifact and produces a **hotspot events CSV** with columns like:\n",
    "- `timestamp`\n",
    "- `lat_min`, `lat_max`, `lon_min`, `lon_max`\n",
    "- optionally `pm25_max`, etc.\n",
    "\n",
    "Run the repo script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0817c241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[event_detection] Processing /uufs/chpc.utah.edu/common/home/u1494915/stream-simulation-ebus/data/processed/predictions/2025-10-20_0000-1200_12h_utah.csv\n",
      "[event_detection] Found 65 persistent hotspots → /uufs/chpc.utah.edu/common/home/u1494915/stream-simulation-ebus/data/processed/hotspots/2025-10-20_utah_hotspots.csv\n",
      "After running, the hotspot CSV should exist, you need to upload the csv to your desired location and copy the link.\n"
     ]
    }
   ],
   "source": [
    "# Preferred: run the repo script\n",
    "!python event_detection.py\n",
    "\n",
    "print(\"After running, the hotspot CSV should exist, you need to upload the csv to your desired location and copy the link.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48416264",
   "metadata": {},
   "source": [
    "## 5) Register hotspot CSV as a SciDX dataset (adds `updated_at` in extras)\n",
    "\n",
    "This makes the hotspot artifact **discoverable** via SciDX search:\n",
    "- `extras.dataset_kind = hotspot` (or `hotspots`, depending on your convention)\n",
    "- `extras.region = utah`\n",
    "- `extras.updated_at = <UTC ISO timestamp>`  ✅ used for picking the latest run\n",
    "\n",
    "Run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0097dc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:scidx_streaming.client.init_client:Extracted user ID: 987104e7-e6d3-47f2-82a0-0d3f620aea70\n",
      "INFO:scidx_streaming.client.init_client:Kafka details set: HOST=localhost, PORT=9092, PREFIX=data_stream_, MAX_STREAMS=10\n",
      "INFO:register_hotspots_csv:Streaming client initialized. user_id=987104e7-e6d3-47f2-82a0-0d3f620aea70\n",
      "INFO:scidx_streaming.client.registration:Dataset 'hotspots_utah' created successfully with 1 resource(s).\n",
      "INFO:register_hotspots_csv:Registered hotspot CSV dataset 'hotspots_utah' from https://drive.google.com/uc?export=download&id=1hUfbphCjCPkdgS2n9SE1hjugkNxq22EZ\n",
      "After running, 'hotspots_<region>' should be created/updated in SciDX.\n"
     ]
    }
   ],
   "source": [
    "# Preferred: run repo script\n",
    "!python register_hotspots_csv_scidx.py\n",
    "\n",
    "print(\"After running, 'hotspots_<region>' should be created/updated in SciDX.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcde5c",
   "metadata": {},
   "source": [
    "## 6) Register sensor sources (e.g., Synoptic WebSockets) in SciDX\n",
    "\n",
    "This step registers sensor *methods* (WebSocket URLs + station metadata).\n",
    "Then you can discover them using:\n",
    "\n",
    "`streaming.search_consumption_methods(terms=[\"sensor\", \"utah\"])`\n",
    "\n",
    "Run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4ac6178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:scidx_streaming.client.init_client:Extracted user ID: 987104e7-e6d3-47f2-82a0-0d3f620aea70\n",
      "INFO:scidx_streaming.client.init_client:Kafka details set: HOST=localhost, PORT=9092, PREFIX=data_stream_, MAX_STREAMS=10\n",
      "INFO:register_synoptic_websockets:Streaming Client initialized. User ID: 987104e7-e6d3-47f2-82a0-0d3f620aea70\n",
      "INFO:scidx_streaming.client.registration:Dataset 'synoptic_push_qcv_aq' created successfully with 1 resource(s).\n",
      "INFO:register_synoptic_websockets:Registered dataset=synoptic_push_qcv_aq (station=QCV)\n",
      "INFO:scidx_streaming.client.registration:Dataset 'synoptic_push_qnr_aq' created successfully with 1 resource(s).\n",
      "INFO:register_synoptic_websockets:Registered dataset=synoptic_push_qnr_aq (station=QNR)\n",
      "INFO:scidx_streaming.client.registration:Dataset 'synoptic_push_qhw_aq' created successfully with 1 resource(s).\n",
      "INFO:register_synoptic_websockets:Registered dataset=synoptic_push_qhw_aq (station=QHW)\n",
      "INFO:scidx_streaming.client.registration:Dataset 'synoptic_push_quttc_aq' created successfully with 1 resource(s).\n",
      "INFO:register_synoptic_websockets:Registered dataset=synoptic_push_quttc_aq (station=QUTTC)\n",
      "INFO:scidx_streaming.client.registration:Dataset 'synoptic_push_qrp_aq' created successfully with 1 resource(s).\n",
      "INFO:register_synoptic_websockets:Registered dataset=synoptic_push_qrp_aq (station=QRP)\n",
      "INFO:register_synoptic_websockets:Done registering Synoptic WebSockets from /uufs/chpc.utah.edu/common/home/u1494915/stream-simulation-ebus/config.yaml\n",
      "Found methods: 5\n",
      "Example: synoptic_push_quttc_aq\n"
     ]
    }
   ],
   "source": [
    "# Preferred: run repo script\n",
    "!python register_synoptic_websockets.py\n",
    "\n",
    "# Quick check (should return multiple sensors if already registered):\n",
    "methods = streaming.search_consumption_methods(terms=[\"sensor\", region])\n",
    "print(\"Found methods:\", len(methods))\n",
    "if methods:\n",
    "    print(\"Example:\", methods[0][\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bcb04",
   "metadata": {},
   "source": [
    "## 7) Pick the *latest* hotspot dataset by `extras.updated_at`\n",
    "\n",
    "When multiple hotspot datasets exist (e.g., repeated registrations), pick the most recent one.\n",
    "\n",
    "This uses `client.search_datasets(...)` because `updated_at` is stored in dataset **extras**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f4fc5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotspot datasets found: 1\n",
      "Latest hotspot dataset: hotspots_utah\n",
      "updated_at: 2026-01-12T05:49:36.253413+00:00\n",
      "resource url: https://drive.google.com/uc?export=download&id=1hUfbphCjCPkdgS2n9SE1hjugkNxq22EZ\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_iso(dt: str):\n",
    "    if not dt:\n",
    "        return None\n",
    "    return datetime.fromisoformat(str(dt).replace(\"Z\", \"+00:00\"))\n",
    "\n",
    "hotspot_candidates = client.search_datasets(\n",
    "    terms=[\"hotspot\", region],\n",
    "    keys=[\"extras_dataset_kind\", \"extras_region\"],\n",
    "    server=SERVER,\n",
    ")\n",
    "\n",
    "# If your dataset_kind is singular (\"hotspot\"), fall back:\n",
    "if not hotspot_candidates:\n",
    "    hotspot_candidates = client.search_datasets(\n",
    "        terms=[\"hotspot\", region],\n",
    "        keys=[\"extras_dataset_kind\", \"extras_region\"],\n",
    "        server=SERVER,\n",
    "    )\n",
    "\n",
    "print(\"Hotspot datasets found:\", len(hotspot_candidates))\n",
    "\n",
    "# Keep only those that have extras.updated_at\n",
    "hotspot_with_time = [\n",
    "    d for d in hotspot_candidates\n",
    "    if isinstance(d.get(\"extras\"), dict) and d[\"extras\"].get(\"updated_at\")\n",
    "]\n",
    "\n",
    "hotspot_with_time.sort(\n",
    "    key=lambda d: parse_iso(d[\"extras\"][\"updated_at\"]),\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "latest_hotspot_ds = hotspot_with_time[0]\n",
    "print(\"Latest hotspot dataset:\", latest_hotspot_ds[\"name\"])\n",
    "print(\"updated_at:\", latest_hotspot_ds[\"extras\"][\"updated_at\"])\n",
    "print(\"resource url:\", latest_hotspot_ds[\"resources\"][0][\"url\"] if latest_hotspot_ds.get(\"resources\") else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6cdbf",
   "metadata": {},
   "source": [
    "## 8) Load hotspot CSV correctly (Google Drive note)\n",
    "\n",
    "If your hotspot CSV is hosted on Google Drive, the `.../view?usp=sharing` URL is **not** a raw CSV.\n",
    "Pandas will read HTML and throw parsing errors.\n",
    "\n",
    "This helper converts a Drive *view* link into a *direct download* link.\n",
    "If you get 404, it's usually because:\n",
    "- the file is not shared publicly / accessible to your environment, or\n",
    "- the ID is wrong, or\n",
    "- your environment can't reach Google Drive.\n",
    "\n",
    "In that case: host the CSV on a plain HTTP endpoint (or S3/GitHub raw) for easiest ingestion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90fcea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hotspot CSV from: https://drive.google.com/uc?export=download&id=1hUfbphCjCPkdgS2n9SE1hjugkNxq22EZ\n",
      "Rows: 126 Cols: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-20T03:00:00+00:00</td>\n",
       "      <td>38.8</td>\n",
       "      <td>248.0</td>\n",
       "      <td>10.482469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-20T15:00:00+00:00</td>\n",
       "      <td>38.8</td>\n",
       "      <td>248.0</td>\n",
       "      <td>10.464170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-20T03:00:00+00:00</td>\n",
       "      <td>38.8</td>\n",
       "      <td>248.4</td>\n",
       "      <td>10.732413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-20T15:00:00+00:00</td>\n",
       "      <td>38.8</td>\n",
       "      <td>248.4</td>\n",
       "      <td>10.619511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-10-20T03:00:00+00:00</td>\n",
       "      <td>38.8</td>\n",
       "      <td>248.8</td>\n",
       "      <td>10.234383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp   lat    lon       pm25\n",
       "0  2025-10-20T03:00:00+00:00  38.8  248.0  10.482469\n",
       "1  2025-10-20T15:00:00+00:00  38.8  248.0  10.464170\n",
       "2  2025-10-20T03:00:00+00:00  38.8  248.4  10.732413\n",
       "3  2025-10-20T15:00:00+00:00  38.8  248.4  10.619511\n",
       "4  2025-10-20T03:00:00+00:00  38.8  248.8  10.234383"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def drive_view_to_direct(url: str) -> str:\n",
    "    # From: https://drive.google.com/file/d/<ID>/view?usp=sharing\n",
    "    m = re.search(r\"/file/d/([^/]+)/\", url)\n",
    "    if not m:\n",
    "        return url\n",
    "    file_id = m.group(1)\n",
    "    return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "\n",
    "def read_csv_safely(url: str) -> pd.DataFrame:\n",
    "    # Try direct url first\n",
    "    try_url = drive_view_to_direct(url)\n",
    "    r = requests.get(try_url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    # If HTML page returned, pandas will likely fail; detect quickly:\n",
    "    ctype = r.headers.get(\"Content-Type\",\"\")\n",
    "    if \"text/html\" in ctype.lower():\n",
    "        raise RuntimeError(f\"Got HTML instead of CSV from {try_url}. Check sharing/hosting.\")\n",
    "    from io import StringIO\n",
    "    return pd.read_csv(StringIO(r.text))\n",
    "\n",
    "# Use resource URL from SciDX if available; fallback to config hotspot_url\n",
    "hotspot_resource_url = None\n",
    "if latest_hotspot_ds.get(\"resources\"):\n",
    "    hotspot_resource_url = latest_hotspot_ds[\"resources\"][0].get(\"url\")\n",
    "\n",
    "chosen_url = hotspot_resource_url or hotspot_url\n",
    "print(\"Loading hotspot CSV from:\", chosen_url)\n",
    "\n",
    "hotspots_df = read_csv_safely(chosen_url)\n",
    "print(\"Rows:\", len(hotspots_df), \"Cols:\", len(hotspots_df.columns))\n",
    "hotspots_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcf0c5",
   "metadata": {},
   "source": [
    "## 9) Discover sensors (from SciDX) + compute nearest sensor per hotspot\n",
    "\n",
    "This mirrors your core research goal:\n",
    "- take a hotspot bounding box\n",
    "- compute its centroid\n",
    "- find the nearest registered sensor (by haversine distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b814411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor methods found: 5\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Hotspot CSV missing columns: {'lon_max', 'lat_min', 'lat_max', 'lon_min'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m required_cols = {\u001b[33m\"\u001b[39m\u001b[33mlat_min\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mlat_max\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mlon_min\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mlon_max\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     34\u001b[39m missing = required_cols - \u001b[38;5;28mset\u001b[39m(hotspots_df.columns)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHotspot CSV missing columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m has_ts = \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hotspots_df.columns\n\u001b[32m     38\u001b[39m has_pm25 = \u001b[33m\"\u001b[39m\u001b[33mpm25_max\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hotspots_df.columns\n",
      "\u001b[31mAssertionError\u001b[39m: Hotspot CSV missing columns: {'lon_max', 'lat_min', 'lat_max', 'lon_min'}"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    p1 = math.radians(lat1)\n",
    "    p2 = math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dl = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dphi / 2) ** 2 + math.cos(p1) * math.cos(p2) * math.sin(dl / 2) ** 2\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n",
    "\n",
    "sensor_methods = streaming.search_consumption_methods(terms=[\"sensor\", region])\n",
    "print(\"Sensor methods found:\", len(sensor_methods))\n",
    "\n",
    "# Build a simple list of sensors with coordinates\n",
    "sensors = []\n",
    "for ds in sensor_methods:\n",
    "    res = (ds.get(\"resources\") or [{}])[0]\n",
    "    cfg = (res.get(\"config\") or {})\n",
    "    lat = cfg.get(\"latitude\")\n",
    "    lon = cfg.get(\"longitude\")\n",
    "    if lat is None or lon is None:\n",
    "        continue\n",
    "    sensors.append({\n",
    "        \"name\": ds.get(\"name\"),\n",
    "        \"station_id\": cfg.get(\"station_id\"),\n",
    "        \"lat\": float(lat),\n",
    "        \"lon\": float(lon),\n",
    "    })\n",
    "\n",
    "assert sensors, \"No sensors with coordinates found.\"\n",
    "\n",
    "required_cols = {\"lat_min\",\"lat_max\",\"lon_min\",\"lon_max\"}\n",
    "missing = required_cols - set(hotspots_df.columns)\n",
    "assert not missing, f\"Hotspot CSV missing columns: {missing}\"\n",
    "\n",
    "has_ts = \"timestamp\" in hotspots_df.columns\n",
    "has_pm25 = \"pm25_max\" in hotspots_df.columns\n",
    "\n",
    "for _, row in hotspots_df.iterrows():\n",
    "    center_lat = (float(row[\"lat_min\"]) + float(row[\"lat_max\"])) / 2.0\n",
    "    center_lon = (float(row[\"lon_min\"]) + float(row[\"lon_max\"])) / 2.0\n",
    "\n",
    "    nearest = min(sensors, key=lambda s: haversine_km(center_lat, center_lon, s[\"lat\"], s[\"lon\"]))\n",
    "    d_km = haversine_km(center_lat, center_lon, nearest[\"lat\"], nearest[\"lon\"])\n",
    "\n",
    "    ts_part = f\"{row['timestamp']} | \" if has_ts else \"\"\n",
    "    pm_part = f\"PM2.5 max={row['pm25_max']} → \" if has_pm25 else \"→ \"\n",
    "\n",
    "    print(f\"[HOTSPOT] {ts_part}{pm_part}{nearest['station_id']} ({d_km:.1f} km)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0cd76a",
   "metadata": {},
   "source": [
    "## 10) Next step: schedule a job on Sage for the selected sensor(s)\n",
    "\n",
    "At this point you have, for each hotspot:\n",
    "- hotspot time + bounding box\n",
    "- nearest sensor station_id (+ coordinates)\n",
    "\n",
    "Your **next integration point** is to take `station_id` and call scheduler / Sage job launcher, e.g.:\n",
    "\n",
    "- create a job payload that includes: station_id, time window, variables, output location\n",
    "- submit the job\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
