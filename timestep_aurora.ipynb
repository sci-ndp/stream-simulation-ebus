{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77b0b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface variable periods = [             0  3600000000000  7200000000000 10800000000000\n",
      " 14400000000000 18000000000000 21600000000000 25200000000000\n",
      " 28800000000000 32400000000000 36000000000000 39600000000000\n",
      " 43200000000000 46800000000000 50400000000000 54000000000000\n",
      " 57600000000000 61200000000000 64800000000000 68400000000000\n",
      " 72000000000000 75600000000000 79200000000000 82800000000000\n",
      " 86400000000000], atmospheric variable periods = [             0 10800000000000 21600000000000 32400000000000\n",
      " 43200000000000 54000000000000 64800000000000 75600000000000\n",
      " 86400000000000]\n",
      "Saved: predictions/aurora_hourly_utah_2025-10-20.csv  (rows=3456)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import xarray as xr\n",
    "\n",
    "from aurora import Batch, Metadata, AuroraAirPollution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# ------------------ paths ------------------\n",
    "download_path = Path(\"~/downloads/cams\").expanduser()\n",
    "surf_path = download_path / \"2025-10-20-cams_24h_forecast_utah-surface-level.nc\"\n",
    "atmo_path = download_path / \"2025-10-20-cams_24h_forecast_utah-atmospheric.nc\"\n",
    "static_path = download_path / \"aurora-0.4-air-pollution-static.pickle\"\n",
    "\n",
    "out_dir = Path(\"./predictions\").expanduser()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = out_dir / \"aurora_hourly_utah_2025-10-20.csv\"\n",
    "\n",
    "# ------------------ load static ------------------\n",
    "with open(static_path, \"rb\") as f:\n",
    "    static_vars_global = pickle.load(f)\n",
    "\n",
    "# ------------------ open CAMS ------------------\n",
    "surf_var_all = xr.open_dataset(surf_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "atmos_var_all = xr.open_dataset(atmo_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "\n",
    "# model\n",
    "model = AuroraAirPollution().eval()\n",
    "ps = int(model.patch_size)\n",
    "\n",
    "print(\n",
    "    f\"surface variable periods = {surf_var_all.forecast_period.values}, \"\n",
    "    f\"atmospheric variable periods = {atmos_var_all.forecast_period.values}\"\n",
    ")\n",
    "\n",
    "# Extract arrays\n",
    "surf_periods = surf_var_all[\"forecast_period\"].values\n",
    "atmos_periods = atmos_var_all[\"forecast_period\"].values\n",
    "\n",
    "# For each surface period, find the index of the latest (previous) atmospheric period\n",
    "atmos_for_surf_idx = np.searchsorted(atmos_periods, surf_periods, side=\"right\") - 1\n",
    "\n",
    "def crop_to_patch(ds, patch_size, lat_name=\"latitude\", lon_name=\"longitude\"):\n",
    "    H = ds.sizes[lat_name]; W = ds.sizes[lon_name]\n",
    "    Hc = H - (H % patch_size)\n",
    "    Wc = W - (W % patch_size)\n",
    "    if Hc == H and Wc == W:\n",
    "        return ds\n",
    "    return ds.isel({lat_name: slice(0, Hc), lon_name: slice(0, Wc)})\n",
    "\n",
    "def crop_static_vars(static_vars, ref_ds, patch_size, lat_name=\"latitude\", lon_name=\"longitude\"):\n",
    "    # NOTE: This simply crops to match the ref grid size; it assumes the static arrays are\n",
    "    # already on a compatible grid. Kept minimal per your request.\n",
    "    H = ref_ds.sizes[lat_name] - (ref_ds.sizes[lat_name] % patch_size)\n",
    "    W = ref_ds.sizes[lon_name] - (ref_ds.sizes[lon_name] % patch_size)\n",
    "    cropped = {}\n",
    "    for k, v in static_vars.items():\n",
    "        arr = np.asarray(v)\n",
    "        if arr.ndim == 2:\n",
    "            cropped[k] = arr[:H, :W]\n",
    "        elif arr.ndim == 3:\n",
    "            cropped[k] = arr[:, :H, :W]\n",
    "        else:\n",
    "            # leave non-2D/3D entries unchanged\n",
    "            cropped[k] = v\n",
    "    return cropped\n",
    "\n",
    "# --- helpers to build T=2 histories ---\n",
    "def T2_surf(prev_ds, curr_ds, var):\n",
    "    a = prev_ds[var].values\n",
    "    b = curr_ds[var].values\n",
    "    # ensure 2D (H,W)\n",
    "    if a.ndim == 3: a = a[0]\n",
    "    if b.ndim == 3: b = b[0]\n",
    "    assert a.ndim == 2 and b.ndim == 2, f\"{var} must be 2D after squeeze (got {a.shape} & {b.shape})\"\n",
    "    ab = np.stack([a, b], axis=0)  # (T=2,H,W)\n",
    "    return torch.from_numpy(ab[None].astype(np.float32))  # (1,2,H,W)\n",
    "\n",
    "def ensure_LHW(x):\n",
    "    # make (L,H,W)\n",
    "    arr = np.asarray(x)\n",
    "    if arr.ndim == 4:  # (time, L,H,W)\n",
    "        arr = arr[0]\n",
    "    if arr.ndim == 3:\n",
    "        return arr\n",
    "    raise ValueError(f\"Expected 3D or 4D atmos var, got {arr.shape}\")\n",
    "\n",
    "def T2_atmo(prev_ds, curr_ds, var):\n",
    "    a = ensure_LHW(prev_ds[var].values)\n",
    "    b = ensure_LHW(curr_ds[var].values)\n",
    "    ab = np.stack([a, b], axis=0)  # (T=2,L,H,W)\n",
    "    return torch.from_numpy(ab[None].astype(np.float32))  # (1,2,L,H,W)\n",
    "\n",
    "def curr2d(t):\n",
    "    return t[0, 0].cpu().numpy().astype(np.float32)\n",
    "\n",
    "# ------------------ loop with T=2 input ------------------\n",
    "rows = []\n",
    "\n",
    "for surf_idx, atmos_idx in enumerate(atmos_for_surf_idx):\n",
    "    # need a PREVIOUS time step for BOTH surf and atmos histories\n",
    "    if surf_idx == 0 or atmos_idx < 0:\n",
    "        continue  # skip first since no prev step\n",
    "\n",
    "    surf_prev = surf_var_all.isel(forecast_period=surf_idx - 1)\n",
    "    surf_curr = surf_var_all.isel(forecast_period=surf_idx)\n",
    "    atmos_prev = atmos_var_all.isel(forecast_period=atmos_idx - 1 if atmos_idx > 0 else 0)\n",
    "    atmos_curr = atmos_var_all.isel(forecast_period=atmos_idx)\n",
    "\n",
    "    # crop all to multiples of patch size\n",
    "    surf_prev  = crop_to_patch(surf_prev,  ps)\n",
    "    surf_curr  = crop_to_patch(surf_curr,  ps)\n",
    "    atmos_prev = crop_to_patch(atmos_prev, ps)\n",
    "    atmos_curr = crop_to_patch(atmos_curr, ps)\n",
    "\n",
    "    # use current atmos grid for metadata; normalize lon to [0,360)\n",
    "    lat_vals = atmos_curr.latitude.values\n",
    "    lon_vals = atmos_curr.longitude.values\n",
    "    lon_vals = np.where(lon_vals < 0, lon_vals + 360, lon_vals)\n",
    "\n",
    "    # crop static to current SURF grid size (matches atmos after crop_to_patch)\n",
    "    static_vars = crop_static_vars(static_vars_global, surf_curr, ps)\n",
    "\n",
    "    # build T=2 tensors\n",
    "    surf_vars = {\n",
    "        \"2t\":    T2_surf(surf_prev,  surf_curr,  \"t2m\"),\n",
    "        \"10u\":   T2_surf(surf_prev,  surf_curr,  \"u10\"),\n",
    "        \"10v\":   T2_surf(surf_prev,  surf_curr,  \"v10\"),\n",
    "        \"msl\":   T2_surf(surf_prev,  surf_curr,  \"msl\"),\n",
    "        \"pm1\":   T2_surf(surf_prev,  surf_curr,  \"pm1\"),\n",
    "        \"pm2p5\": T2_surf(surf_prev,  surf_curr,  \"pm2p5\"),\n",
    "        \"pm10\":  T2_surf(surf_prev,  surf_curr,  \"pm10\"),\n",
    "        \"tcco\":  T2_surf(surf_prev,  surf_curr,  \"tcco\"),\n",
    "        \"tc_no\": T2_surf(surf_prev,  surf_curr,  \"tc_no\"),\n",
    "        \"tcno2\": T2_surf(surf_prev,  surf_curr,  \"tcno2\"),\n",
    "        \"gtco3\": T2_surf(surf_prev,  surf_curr,  \"gtco3\"),\n",
    "        \"tcso2\": T2_surf(surf_prev,  surf_curr,  \"tcso2\"),\n",
    "    }\n",
    "\n",
    "    atmos_vars = {\n",
    "        \"t\":   T2_atmo(atmos_prev, atmos_curr, \"t\"),\n",
    "        \"u\":   T2_atmo(atmos_prev, atmos_curr, \"u\"),\n",
    "        \"v\":   T2_atmo(atmos_prev, atmos_curr, \"v\"),\n",
    "        \"q\":   T2_atmo(atmos_prev, atmos_curr, \"q\"),\n",
    "        \"z\":   T2_atmo(atmos_prev, atmos_curr, \"z\"),\n",
    "        \"co\":  T2_atmo(atmos_prev, atmos_curr, \"co\"),\n",
    "        \"no\":  T2_atmo(atmos_prev, atmos_curr, \"no\"),\n",
    "        \"no2\": T2_atmo(atmos_prev, atmos_curr, \"no2\"),\n",
    "        \"go3\": T2_atmo(atmos_prev, atmos_curr, \"go3\"),\n",
    "        \"so2\": T2_atmo(atmos_prev, atmos_curr, \"so2\"),\n",
    "    }\n",
    "\n",
    "    # convert static dict to tensors (leave shapes (H,W) or (C,H,W) as-is)\n",
    "    static_tensors = {}\n",
    "    for k, v in static_vars.items():\n",
    "        static_tensors[k] = torch.from_numpy(np.asarray(v).astype(np.float32))\n",
    "\n",
    "    # timestamps (prev, curr) as tuple\n",
    "    t_prev = np.asarray(surf_prev.valid_time.values, dtype=\"datetime64[s]\")\n",
    "    t_curr = np.asarray(surf_curr.valid_time.values, dtype=\"datetime64[s]\")\n",
    "    t_prev = t_prev.tolist()[-1] if t_prev.ndim else t_prev.item()\n",
    "    t_curr = t_curr.tolist()[-1] if t_curr.ndim else t_curr.item()\n",
    "\n",
    "    batch = Batch(\n",
    "        surf_vars=surf_vars,\n",
    "        static_vars=static_tensors,\n",
    "        atmos_vars=atmos_vars,\n",
    "        metadata=Metadata(\n",
    "            lat=torch.from_numpy(lat_vals),\n",
    "            lon=torch.from_numpy(lon_vals),\n",
    "            time=(t_prev, t_curr),  # T=2 history\n",
    "            atmos_levels=tuple(int(level) for level in atmos_curr.pressure_level.values),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        pred = model(batch)\n",
    "\n",
    "    pm1  = curr2d(pred.surf_vars[\"pm1\"])   * 1e9\n",
    "    pm25 = curr2d(pred.surf_vars[\"pm2p5\"]) * 1e9\n",
    "    pm10 = curr2d(pred.surf_vars[\"pm10\"])  * 1e9\n",
    "\n",
    "    # gases (surface stream keys used by the model output)\n",
    "    co   = curr2d(pred.surf_vars[\"tcco\"])\n",
    "    no   = curr2d(pred.surf_vars[\"tc_no\"])\n",
    "    no2  = curr2d(pred.surf_vars[\"tcno2\"])\n",
    "    o3   = curr2d(pred.surf_vars[\"gtco3\"])\n",
    "    so2  = curr2d(pred.surf_vars[\"tcso2\"])\n",
    "\n",
    "    # build 2D lat/lon grids to match (H, W) → then flatten\n",
    "    H, W = pm25.shape\n",
    "    LAT2D, LON2D = np.meshgrid(lat_vals, lon_vals, indexing=\"ij\")\n",
    "\n",
    "    row_df = pd.DataFrame({\n",
    "        \"timestamp\":     np.repeat(np.datetime64(t_curr), H * W),\n",
    "        \"lat\":           LAT2D.ravel(),\n",
    "        \"lon\":           LON2D.ravel(),\n",
    "        \"pm1_ugm3\":      pm1.ravel(),\n",
    "        \"pm2p5_ugm3\":    pm25.ravel(),\n",
    "        \"pm10_ugm3\":     pm10.ravel(),\n",
    "        \"co\":            co.ravel(),\n",
    "        \"no\":            no.ravel(),\n",
    "        \"no2\":           no2.ravel(),\n",
    "        \"o3\":            o3.ravel(),\n",
    "        \"so2\":           so2.ravel(),\n",
    "    })\n",
    "\n",
    "    rows.append(row_df)\n",
    "    del batch, pred, row_df, pm1, pm25, pm10, co, no, no2, o3, so2\n",
    "    gc.collect()\n",
    "\n",
    "# ---- after the loop, write CSV (kept minimal) ----\n",
    "if rows:\n",
    "    df = pd.concat(rows, ignore_index=True)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}  (rows={len(df)})\")\n",
    "else:\n",
    "    print(\"No rows were produced (likely because the first step was skipped and/or no matching periods).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fedfaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_size: 3\n",
      "Prelim slice size (HxW): 1 x 1\n",
      "Window after crop (HxW): 24 x 24\n",
      "Saved CSV: predictions/pollutant_prediction_utah.csv\n",
      "Final working size (HxW): 24 x 24  (ps=3, min required: 24)\n",
      "             timestamp  latitude  longitude  pm2p5_ugm3  pm1_ugm3  pm10_ugm3  \\\n",
      "0  2025-10-20 00:00:00      36.4      243.2    6.144454  2.515953   4.962630   \n",
      "1  2025-10-20 00:00:00      36.4      243.6    6.554677  3.670983   7.366880   \n",
      "2  2025-10-20 00:00:00      36.4      244.0    7.013986  3.739067   7.631770   \n",
      "3  2025-10-20 00:00:00      36.4      244.4    6.963577  3.815977   7.465333   \n",
      "4  2025-10-20 00:00:00      36.4      244.8    7.344985  4.195292   7.300032   \n",
      "5  2025-10-20 00:00:00      36.4      245.2    7.183073  3.435723   5.515376   \n",
      "6  2025-10-20 00:00:00      36.4      245.6    6.622933  2.241485   3.401483   \n",
      "7  2025-10-20 00:00:00      36.4      246.0    6.147038  1.801929   2.471069   \n",
      "8  2025-10-20 00:00:00      36.4      246.4    5.931744  1.503060   2.060887   \n",
      "9  2025-10-20 00:00:00      36.4      246.8    5.405423  1.307036   2.122733   \n",
      "\n",
      "          ozone  nitrogen_dioxide  nitrogen_monoxide  carbon_monoxide  \\\n",
      "0  8.035612e-08      2.233387e-10       1.710365e-11     8.876351e-08   \n",
      "1  8.248930e-08      2.516964e-10       2.042189e-11     9.176075e-08   \n",
      "2  8.449174e-08      2.454010e-10       2.061373e-11     9.413385e-08   \n",
      "3  8.892888e-08      7.356116e-10       5.587086e-11     9.794282e-08   \n",
      "4  9.233644e-08      3.221360e-09       2.144214e-10     1.080116e-07   \n",
      "5  9.386481e-08      3.387293e-09       2.138529e-10     1.106657e-07   \n",
      "6  9.487135e-08      1.194914e-09       6.709033e-11     1.062586e-07   \n",
      "7  9.228872e-08      5.552474e-10       3.245848e-11     1.049632e-07   \n",
      "8  8.761069e-08      2.321991e-10       1.594547e-11     1.012961e-07   \n",
      "9  8.507983e-08      1.718741e-10       1.178169e-11     9.576026e-08   \n",
      "\n",
      "   sulfur_dioxide  \n",
      "0    2.182583e-10  \n",
      "1    2.265432e-10  \n",
      "2    2.229337e-10  \n",
      "3    5.737713e-10  \n",
      "4    1.104930e-09  \n",
      "5    9.973826e-10  \n",
      "6    5.673906e-10  \n",
      "7    4.039089e-10  \n",
      "8    2.859304e-10  \n",
      "9    2.378551e-10  \n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# ALL DATA Create UTAH/SLC SLICE csv contains hour timestamp\n",
    "# PM2.5 from CAMS via Microsoft Aurora — Utah/SLC subset\n",
    "# Robust single full-window inference with >= 8 * patch_size per dim\n",
    "# - Ensures sizes are multiples of patch_size and lat↓ / lon↑\n",
    "# - Returns index slices to cut static grids consistently\n",
    "# - NO PLOT; CSV includes pm1, pm10, ozone, NO2, NO, CO, SO2 columns\n",
    "# - CSV now saves a timestamp with hour (YYYY-MM-DD HH:MM:SS)\n",
    "#####################################\n",
    "\n",
    "import gc, math, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "\n",
    "from aurora import Batch, Metadata, AuroraAirPollution\n",
    "\n",
    "# ------------------ config / paths ------------------\n",
    "download_path = Path(\"~/downloads/cams\").expanduser()\n",
    "surf_path = download_path / \"2025-10-20-cams-surface-level.nc\"\n",
    "atmo_path = download_path / \"2025-10-20-cams-atmospheric.nc\"\n",
    "static_path = hf_hub_download(\"microsoft/aurora\", \"aurora-0.4-air-pollution-static.pickle\")\n",
    "\n",
    "out_dir = Path(\"./predictions\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = out_dir / \"pollutant_prediction_utah.csv\"\n",
    "\n",
    "# ------------------ REGION BOUNDS (degrees) ------------------\n",
    "# Use 0..360 longitudes \n",
    "UTAH_BBOX = dict(lat_min=37.0, lat_max=42.1, lon_min=245.9, lon_max=251.0)\n",
    "SLC_BBOX  = dict(lat_min=40.4, lat_max=41.1, lon_min=247.7, lon_max=248.4)\n",
    "\n",
    "# Choose region here:\n",
    "BBOX = SLC_BBOX   # or UTAH_BBOX\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def detect_lon_domain(ds_lon: np.ndarray) -> str:\n",
    "    lon_min = float(np.nanmin(ds_lon)); lon_max = float(np.nanmax(ds_lon))\n",
    "    return \"0_360\" if lon_min >= 0 and lon_max > 180 else \"-180_180\"\n",
    "\n",
    "def to_dataset_lon(lon_vals, target_domain: str):\n",
    "    \"\"\"Map longitude(s) into the dataset's domain. Accepts scalar/array.\"\"\"\n",
    "    arr = np.asarray(lon_vals, dtype=float)\n",
    "    if target_domain == \"0_360\":\n",
    "        arr = arr % 360.0\n",
    "        arr = np.where(arr < 0, arr + 360.0, arr)\n",
    "    else:\n",
    "        arr = ((arr + 180.0) % 360.0) - 180.0\n",
    "    return float(arr) if np.ndim(lon_vals) == 0 else arr\n",
    "\n",
    "def ensure_model_orientation(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Ensure lon strictly increasing, lat strictly decreasing (Aurora requirement).\"\"\"\n",
    "    out = ds\n",
    "    if out.longitude.size >= 2 and not np.all(np.diff(out.longitude.values) > 0):\n",
    "        out = out.isel(longitude=np.argsort(out.longitude.values))\n",
    "    if out.latitude.size >= 2 and not np.all(np.diff(out.latitude.values) < 0):\n",
    "        out = out.isel(latitude=slice(None, None, -1))\n",
    "    if out.latitude.size >= 2:\n",
    "        assert np.all(np.diff(out.latitude.values) < 0), \"lat must be strictly decreasing\"\n",
    "    if out.longitude.size >= 2:\n",
    "        assert np.all(np.diff(out.longitude.values) > 0), \"lon must be strictly increasing\"\n",
    "    return out\n",
    "\n",
    "def slice_bbox_value(ds: xr.Dataset, bbox: dict) -> xr.Dataset:\n",
    "    \"\"\"Value-based bbox slice (handles lon domain + any lat order).\"\"\"\n",
    "    lat = ds.latitude; lon = ds.longitude\n",
    "    lon_domain = detect_lon_domain(lon.values)\n",
    "    lon_min_ds, lon_max_ds = to_dataset_lon([bbox[\"lon_min\"], bbox[\"lon_max\"]], lon_domain)\n",
    "    lat_min, lat_max = bbox[\"lat_min\"], bbox[\"lat_max\"]\n",
    "\n",
    "    if lat[0] > lat[-1]:\n",
    "        lat_slice = slice(lat_max, lat_min)  # descending\n",
    "    else:\n",
    "        lat_slice = slice(lat_min, lat_max)  # ascending\n",
    "\n",
    "    if lon_min_ds <= lon_max_ds:\n",
    "        lon_slice = slice(lon_min_ds, lon_max_ds)\n",
    "        out = ds.sel(latitude=lat_slice, longitude=lon_slice)\n",
    "    else:\n",
    "        left  = ds.sel(latitude=lat_slice, longitude=slice(lon_min_ds, float(lon.values.max())))\n",
    "        right = ds.sel(latitude=lat_slice, longitude=slice(float(lon.values.min()), lon_max_ds))\n",
    "        out = xr.concat([left, right], dim=\"longitude\")\n",
    "\n",
    "    if out.sizes.get(\"latitude\", 0) == 0 or out.sizes.get(\"longitude\", 0) == 0:\n",
    "        raise RuntimeError(\"BBox slice returned empty selection. Check bounds & lon domain.\")\n",
    "    return out\n",
    "\n",
    "def nearest_index(vec, value):\n",
    "    return int(np.abs(vec - value).argmin())\n",
    "\n",
    "def roundup_to_multiple(n, m):\n",
    "    \"\"\"Smallest multiple of m that is >= n.\"\"\"\n",
    "    return ((int(n) + m - 1) // m) * m\n",
    "\n",
    "def expand_region_indices(base_lat, base_lon, center_lat_raw, center_lon_raw, target_h, target_w):\n",
    "    \"\"\"\n",
    "    Compute (i_start, i_end, j_start, j_end) on the ORIGINAL grid\n",
    "    around (center_lat_raw, center_lon_raw) for a target_h x target_w window.\n",
    "    \"\"\"\n",
    "    latv = base_lat; lonv = base_lon\n",
    "    dom = detect_lon_domain(lonv)\n",
    "    center_lon = float(to_dataset_lon(center_lon_raw, dom))\n",
    "    center_lat = float(center_lat_raw)\n",
    "\n",
    "    i0 = nearest_index(latv, center_lat)\n",
    "    j0 = nearest_index(lonv, center_lon)\n",
    "\n",
    "    half_h = target_h // 2\n",
    "    half_w = target_w // 2\n",
    "\n",
    "    i_start = max(0, i0 - half_h)\n",
    "    i_end   = min(latv.size, i0 + half_h + (target_h % 2 != 0))\n",
    "    if i_end - i_start < target_h:\n",
    "        deficit = target_h - (i_end - i_start)\n",
    "        i_start = max(0, i_start - deficit//2)\n",
    "        i_end   = min(latv.size, i_end + math.ceil(deficit/2))\n",
    "\n",
    "    j_start = max(0, j0 - half_w)\n",
    "    j_end   = min(lonv.size, j0 + half_w + (target_w % 2 != 0))\n",
    "    if j_end - j_start < target_w:\n",
    "        deficit = target_w - (j_end - j_start)\n",
    "        j_start = max(0, j_start - deficit//2)\n",
    "        j_end   = min(lonv.size, j_end + math.ceil(deficit/2))\n",
    "\n",
    "    # clamp + ensure non-empty\n",
    "    i_start = max(0, min(i_start, latv.size-1))\n",
    "    i_end   = max(i_start+1, min(i_end,   latv.size))\n",
    "    j_start = max(0, min(j_start, lonv.size-1))\n",
    "    j_end   = max(j_start+1, min(j_end,   lonv.size))\n",
    "    return i_start, i_end, j_start, j_end\n",
    "\n",
    "def sizes_multiple_of_ps(ds: xr.Dataset, ps: int) -> bool:\n",
    "    H = ds.sizes[\"latitude\"]; W = ds.sizes[\"longitude\"]\n",
    "    return (H % ps == 0) and (W % ps == 0)\n",
    "\n",
    "def crop_to_multiple_of_ps(ds: xr.Dataset, ps: int) -> xr.Dataset:\n",
    "    \"\"\"Crop dataset so H and W are multiples of patch size (keeps top-left corner).\"\"\"\n",
    "    H = ds.sizes[\"latitude\"]; W = ds.sizes[\"longitude\"]\n",
    "    Hc = (H // ps) * ps\n",
    "    Wc = (W // ps) * ps\n",
    "    return ds.isel(latitude=slice(0, Hc), longitude=slice(0, Wc))\n",
    "\n",
    "def safe_time_tuple(ds):\n",
    "    if \"valid_time\" in ds:\n",
    "        vt = np.asarray(ds.valid_time.values)\n",
    "        if vt.ndim == 0: vt = vt[None]\n",
    "        return tuple(pd.to_datetime(vt).to_pydatetime())\n",
    "    return tuple()\n",
    "\n",
    "# ------------------ load static vars ------------------\n",
    "with open(static_path, \"rb\") as f:\n",
    "    static_vars_full = pickle.load(f)\n",
    "\n",
    "# ------------------ open datasets ------------------\n",
    "surf_base = xr.open_dataset(surf_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "atmo_base = xr.open_dataset(atmo_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "# single analysis time for this demo\n",
    "if \"forecast_period\" in surf_base.dims: surf_base = surf_base.isel(forecast_period=0)\n",
    "if \"forecast_period\" in atmo_base.dims: atmo_base = atmo_base.isel(forecast_period=0)\n",
    "\n",
    "# ------------------ model & patch size ------------------\n",
    "model = AuroraAirPollution()\n",
    "model.load_checkpoint()\n",
    "model.eval()\n",
    "ps = int(model.patch_size)\n",
    "print(\"patch_size:\", ps)\n",
    "\n",
    "# Swin3D uses multiple spatial downsamplings; require >= 8 patches per dim\n",
    "MIN_PATCHES_PER_DIM = 8\n",
    "min_cells = MIN_PATCHES_PER_DIM * ps\n",
    "\n",
    "# ------------------ prelim slice (diag) ------------------\n",
    "surf_reg0 = slice_bbox_value(surf_base, BBOX)\n",
    "atmo_reg0 = slice_bbox_value(atmo_base, BBOX)\n",
    "H0, W0 = surf_reg0.sizes[\"latitude\"], surf_reg0.sizes[\"longitude\"]\n",
    "print(f\"Prelim slice size (HxW): {H0} x {W0}\")\n",
    "\n",
    "# ------------------ compute target sizes ------------------\n",
    "target_H = max(min_cells, roundup_to_multiple(max(H0, min_cells), ps))\n",
    "target_W = max(min_cells, roundup_to_multiple(max(W0, min_cells), ps))\n",
    "\n",
    "# ------------------ build single window by indices (and keep slices) ----------\n",
    "lat_center_raw = 0.5 * (BBOX[\"lat_min\"] + BBOX[\"lat_max\"])\n",
    "lon_center_raw = 0.5 * (BBOX[\"lon_min\"] + BBOX[\"lon_max\"])\n",
    "\n",
    "# index expansion on ORIGINAL grids (use surf_base as reference for both)\n",
    "i_start, i_end, j_start, j_end = expand_region_indices(\n",
    "    surf_base.latitude.values, surf_base.longitude.values,\n",
    "    lat_center_raw, lon_center_raw,\n",
    "    target_H, target_W\n",
    ")\n",
    "\n",
    "surf_win = surf_base.isel(latitude=slice(i_start, i_end), longitude=slice(j_start, j_end))\n",
    "atmo_win = atmo_base.isel(latitude=slice(i_start, i_end), longitude=slice(j_start, j_end))\n",
    "\n",
    "# enforce orientation\n",
    "surf_win = ensure_model_orientation(surf_win)\n",
    "atmo_win = ensure_model_orientation(atmo_win)\n",
    "\n",
    "# crop to multiples of ps\n",
    "surf_ds = crop_to_multiple_of_ps(surf_win, ps)\n",
    "atmo_ds = crop_to_multiple_of_ps(atmo_win, ps)\n",
    "\n",
    "H, W = surf_ds.sizes[\"latitude\"], surf_ds.sizes[\"longitude\"]\n",
    "print(f\"Window after crop (HxW): {H} x {W}\")\n",
    "assert H >= min_cells and W >= min_cells, f\"Need >= {min_cells} cells per dim (got {H}x{W}, ps={ps})\"\n",
    "assert sizes_multiple_of_ps(surf_ds, ps) and sizes_multiple_of_ps(atmo_ds, ps), \"Not multiples of patch size.\"\n",
    "\n",
    "# ------------------ cut static grids to the same indices ------------------\n",
    "def slice_static_like(i_start, i_end, j_start, j_end, orient_like: xr.Dataset, ps: int):\n",
    "    cut = {}\n",
    "    for k, arr in static_vars_full.items():\n",
    "        if arr.ndim == 3:  # (C,H,W)\n",
    "            arr_cut = arr[:, i_start:i_end, j_start:j_end]\n",
    "        elif arr.ndim == 2:  # (H,W)\n",
    "            arr_cut = arr[i_start:i_end, j_start:j_end]\n",
    "        else:\n",
    "            cut[k] = arr\n",
    "            continue\n",
    "        # mirror latitude flip done by ensure_model_orientation (if any)\n",
    "        flipped_lat = not (np.all(np.diff(surf_win.latitude.values) < 0)) if surf_win.latitude.size >= 2 else False\n",
    "        if flipped_lat:\n",
    "            if arr_cut.ndim == 3:\n",
    "                arr_cut = arr_cut[:, ::-1, :]\n",
    "            else:\n",
    "                arr_cut = arr_cut[::-1, :]\n",
    "        # crop to multiples of ps\n",
    "        if arr_cut.ndim == 3:\n",
    "            Hc = (arr_cut.shape[1] // ps) * ps\n",
    "            Wc = (arr_cut.shape[2] // ps) * ps\n",
    "            arr_cut = arr_cut[:, :Hc, :Wc]\n",
    "        elif arr_cut.ndim == 2:\n",
    "            Hc = (arr_cut.shape[0] // ps) * ps\n",
    "            Wc = (arr_cut.shape[1] // ps) * ps\n",
    "            arr_cut = arr_cut[:Hc, :Wc]\n",
    "        cut[k] = arr_cut\n",
    "    return cut\n",
    "\n",
    "static_vars_tile = slice_static_like(i_start, i_end, j_start, j_end, surf_ds, ps)\n",
    "\n",
    "# ------------------ single full-window inference ------------------\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "surf_vars = {\n",
    "    \"2t\":    torch.from_numpy(surf_ds[\"t2m\" ].values[None]),\n",
    "    \"10u\":   torch.from_numpy(surf_ds[\"u10\" ].values[None]),\n",
    "    \"10v\":   torch.from_numpy(surf_ds[\"v10\" ].values[None]),\n",
    "    \"msl\":   torch.from_numpy(surf_ds[\"msl\" ].values[None]),\n",
    "    \"pm1\":   torch.from_numpy(surf_ds[\"pm1\" ].values[None]),\n",
    "    \"pm2p5\": torch.from_numpy(surf_ds[\"pm2p5\"].values[None]),\n",
    "    \"pm10\":  torch.from_numpy(surf_ds[\"pm10\"].values[None]),\n",
    "    \"tcco\":  torch.from_numpy(surf_ds[\"tcco\"].values[None]),\n",
    "    \"tc_no\": torch.from_numpy(surf_ds[\"tc_no\"].values[None]),\n",
    "    \"tcno2\": torch.from_numpy(surf_ds[\"tcno2\"].values[None]),\n",
    "    \"gtco3\": torch.from_numpy(surf_ds[\"gtco3\"].values[None]),\n",
    "    \"tcso2\": torch.from_numpy(surf_ds[\"tcso2\"].values[None]),\n",
    "}\n",
    "atmos_vars = {\n",
    "    \"t\":   torch.from_numpy(atmo_ds[\"t\" ].values[None]),\n",
    "    \"u\":   torch.from_numpy(atmo_ds[\"u\" ].values[None]),\n",
    "    \"v\":   torch.from_numpy(atmo_ds[\"v\" ].values[None]),\n",
    "    \"q\":   torch.from_numpy(atmo_ds[\"q\" ].values[None]),\n",
    "    \"z\":   torch.from_numpy(atmo_ds[\"z\"].values[None]),\n",
    "    \"co\":  torch.from_numpy(atmo_ds[\"co\"].values[None]),\n",
    "    \"no\":  torch.from_numpy(atmo_ds[\"no\"].values[None]),\n",
    "    \"no2\": torch.from_numpy(atmo_ds[\"no2\"].values[None]),\n",
    "    \"go3\": torch.from_numpy(atmo_ds[\"go3\"].values[None]),\n",
    "    \"so2\": torch.from_numpy(atmo_ds[\"so2\"].values[None]),\n",
    "}\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars=surf_vars,\n",
    "    static_vars={k: torch.from_numpy(v) for k, v in static_vars_tile.items()},\n",
    "    atmos_vars=atmos_vars,\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(atmo_ds.latitude.values),   # strictly decreasing\n",
    "        lon=torch.from_numpy(atmo_ds.longitude.values),  # strictly increasing\n",
    "        time=safe_time_tuple(atmo_ds),\n",
    "        atmos_levels=tuple(int(x) for x in atmo_ds.pressure_level.values),\n",
    "    ),\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    pred = model(batch)\n",
    "\n",
    "# PM2.5 prediction (kg/m^3 -> µg/m^3)\n",
    "pm25_kgm3 = pred.surf_vars[\"pm2p5\"][0, 0].cpu().numpy().astype(np.float32)\n",
    "pm25_ugm3 = pm25_kgm3 / 1e-9\n",
    "\n",
    "# ------------------ assemble CSV (no plotting) ------------------\n",
    "# Consistent orientation for export: lat increasing for human readability\n",
    "lat_vec = atmo_ds.latitude.values.copy()   # decreasing\n",
    "lon_vec = atmo_ds.longitude.values.copy()  # increasing\n",
    "lat_out = lat_vec.copy()\n",
    "pm25_out = pm25_ugm3.copy()\n",
    "flip_lat = lat_out[0] > lat_out[-1]\n",
    "if flip_lat:\n",
    "    lat_out = lat_out[::-1]\n",
    "    pm25_out = pm25_out[::-1, :]\n",
    "\n",
    "# Helper to extract + orient 2D fields like PM1/PM10 (surface) in µg/m^3\n",
    "def surf_pm_as_ugm3(varname):\n",
    "    arr = surf_ds[varname].values  # (time?, H, W) or (H,W)\n",
    "    if arr.ndim == 3: arr = arr[0]\n",
    "    arr = arr / 1e-9  # kg/m^3 -> µg/m^3\n",
    "    if flip_lat: arr = arr[::-1, :]\n",
    "    return arr\n",
    "\n",
    "pm1_ugm3  = surf_pm_as_ugm3(\"pm1\")\n",
    "pm10_ugm3 = surf_pm_as_ugm3(\"pm10\")\n",
    "\n",
    "# Near-surface gases from the atmospheric cube:\n",
    "# Pick the highest pressure level (closest to the surface)\n",
    "plevs = atmo_ds.pressure_level.values\n",
    "k_surface = int(np.argmax(plevs))  # highest pressure\n",
    "\n",
    "def atmo_surface_field(name):\n",
    "    v = atmo_ds[name].values  # (plev, H, W) or (time?,plev,H,W)\n",
    "    if v.ndim == 4: v = v[0, k_surface]\n",
    "    elif v.ndim == 3: v = v[k_surface]\n",
    "    else: raise RuntimeError(f\"Unexpected dims for {name}: {v.shape}\")\n",
    "    if flip_lat: v = v[::-1, :]\n",
    "    return v\n",
    "\n",
    "ozone = atmo_surface_field(\"go3\")\n",
    "nitrogen_dioxide = atmo_surface_field(\"no2\")\n",
    "nitrogen_monoxide = atmo_surface_field(\"no\")\n",
    "carbon_monoxide   = atmo_surface_field(\"co\")\n",
    "sulfur_dioxide    = atmo_surface_field(\"so2\")\n",
    "\n",
    "# Build 2D lon/lat grids\n",
    "LON2D, LAT2D = np.meshgrid(lon_vec, lat_out)\n",
    "\n",
    "# --- timestamp with hour (ISO-like string) ---\n",
    "# Use valid_time from the atmo_ds (after isel); if absent, set None\n",
    "try:\n",
    "    vt = np.asarray(atmo_ds.valid_time.values)\n",
    "    if vt.ndim == 0: \n",
    "        vt = vt[None]\n",
    "    # choose the first (only) time\n",
    "    sel_time_iso = pd.to_datetime(vt[0]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "except Exception:\n",
    "    sel_time_iso = None\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"timestamp\":          sel_time_iso,  # string column -> always shows hour\n",
    "    \"latitude\":           LAT2D.ravel(),\n",
    "    \"longitude\":          LON2D.ravel(),\n",
    "    \"pm2p5_ugm3\":         pm25_out.ravel(),\n",
    "    \"pm1_ugm3\":           pm1_ugm3.ravel(),\n",
    "    \"pm10_ugm3\":          pm10_ugm3.ravel(),\n",
    "    \"ozone\":              ozone.ravel(),\n",
    "    \"nitrogen_dioxide\":   nitrogen_dioxide.ravel(),\n",
    "    \"nitrogen_monoxide\":  nitrogen_monoxide.ravel(),\n",
    "    \"carbon_monoxide\":    carbon_monoxide.ravel(),\n",
    "    \"sulfur_dioxide\":     sulfur_dioxide.ravel(),\n",
    "})\n",
    "\n",
    "# Drop NaNs and save\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved CSV: {csv_path}\")\n",
    "print(f\"Final working size (HxW): {pm25_kgm3.shape[0]} x {pm25_kgm3.shape[1]}  (ps={ps}, min required: {MIN_PATCHES_PER_DIM*ps})\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fdfb9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_size: 3\n",
      "Prelim slice size (HxW): 2 x 1\n",
      "Window after crop (HxW): 12 x 12\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Need >= 24 cells per dim (got 12x12, ps=3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 210\u001b[39m\n\u001b[32m    208\u001b[39m H, W = surf_ds.sizes[\u001b[33m\"\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m\"\u001b[39m], surf_ds.sizes[\u001b[33m\"\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    209\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWindow after crop (HxW): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m x \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m H >= min_cells \u001b[38;5;129;01mand\u001b[39;00m W >= min_cells, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNeed >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cells per dim (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, ps=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sizes_multiple_of_ps(surf_ds, ps) \u001b[38;5;129;01mand\u001b[39;00m sizes_multiple_of_ps(atmo_ds, ps), \u001b[33m\"\u001b[39m\u001b[33mNot multiples of patch size.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# ------------------ cut static grids to the same indices ------------------\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: Need >= 24 cells per dim (got 12x12, ps=3)"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "\n",
    "# UTAH DATA with LEAD TIME\n",
    "# PM2.5 from CAMS via Microsoft Aurora — Utah/SLC subset\n",
    "# Robust single full-window inference with >= 8 * patch_size per dim\n",
    "# - Ensures sizes are multiples of patch_size and lat↓ / lon↑\n",
    "# - Returns index slices to cut static grids consistently\n",
    "# - NO PLOT; CSV includes pm1, pm10, ozone, NO2, NO, CO, SO2 columns\n",
    "\n",
    "#####################################\n",
    "\n",
    "import gc, math, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "\n",
    "from aurora import Batch, Metadata, AuroraAirPollution\n",
    "\n",
    "# ------------------ config / paths ------------------\n",
    "download_path = Path(\"~/downloads/cams\").expanduser()\n",
    "surf_path = download_path / \"2025-10-20-cams_24h_forecast_utah-surface-level.nc\"\n",
    "atmo_path = download_path / \"2025-10-20-cams_24h_forecast_utah-atmospheric.nc\"\n",
    "static_path = hf_hub_download(\"microsoft/aurora\", \"aurora-0.4-air-pollution-static.pickle\")\n",
    "\n",
    "out_dir = Path(\"./predictions\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = out_dir / \"pollutant_prediction_utah_hourly.csv\"\n",
    "\n",
    "# ------------------ REGION BOUNDS (degrees) ------------------\n",
    "# Use 0..360 longitudes \n",
    "UTAH_BBOX = dict(lat_min=37.0, lat_max=42.1, lon_min=245.9, lon_max=251.0)\n",
    "SLC_BBOX  = dict(lat_min=40.4, lat_max=41.1, lon_min=247.7, lon_max=248.4)\n",
    "\n",
    "# Choose region here:\n",
    "BBOX = SLC_BBOX   # or UTAH_BBOX\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def detect_lon_domain(ds_lon: np.ndarray) -> str:\n",
    "    lon_min = float(np.nanmin(ds_lon)); lon_max = float(np.nanmax(ds_lon))\n",
    "    return \"0_360\" if lon_min >= 0 and lon_max > 180 else \"-180_180\"\n",
    "\n",
    "def to_dataset_lon(lon_vals, target_domain: str):\n",
    "    \"\"\"Map longitude(s) into the dataset's domain. Accepts scalar/array.\"\"\"\n",
    "    arr = np.asarray(lon_vals, dtype=float)\n",
    "    if target_domain == \"0_360\":\n",
    "        arr = arr % 360.0\n",
    "        arr = np.where(arr < 0, arr + 360.0, arr)\n",
    "    else:\n",
    "        arr = ((arr + 180.0) % 360.0) - 180.0\n",
    "    return float(arr) if np.ndim(lon_vals) == 0 else arr\n",
    "\n",
    "def ensure_model_orientation(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Ensure lon strictly increasing, lat strictly decreasing (Aurora requirement).\"\"\"\n",
    "    out = ds\n",
    "    if out.longitude.size >= 2 and not np.all(np.diff(out.longitude.values) > 0):\n",
    "        out = out.isel(longitude=np.argsort(out.longitude.values))\n",
    "    if out.latitude.size >= 2 and not np.all(np.diff(out.latitude.values) < 0):\n",
    "        out = out.isel(latitude=slice(None, None, -1))\n",
    "    if out.latitude.size >= 2:\n",
    "        assert np.all(np.diff(out.latitude.values) < 0), \"lat must be strictly decreasing\"\n",
    "    if out.longitude.size >= 2:\n",
    "        assert np.all(np.diff(out.longitude.values) > 0), \"lon must be strictly increasing\"\n",
    "    return out\n",
    "\n",
    "def slice_bbox_value(ds: xr.Dataset, bbox: dict) -> xr.Dataset:\n",
    "    \"\"\"Value-based bbox slice (handles lon domain + any lat order).\"\"\"\n",
    "    lat = ds.latitude; lon = ds.longitude\n",
    "    lon_domain = detect_lon_domain(lon.values)\n",
    "    lon_min_ds, lon_max_ds = to_dataset_lon([bbox[\"lon_min\"], bbox[\"lon_max\"]], lon_domain)\n",
    "    lat_min, lat_max = bbox[\"lat_min\"], bbox[\"lat_max\"]\n",
    "\n",
    "    if lat[0] > lat[-1]:\n",
    "        lat_slice = slice(lat_max, lat_min)  # descending\n",
    "    else:\n",
    "        lat_slice = slice(lat_min, lat_max)  # ascending\n",
    "\n",
    "    if lon_min_ds <= lon_max_ds:\n",
    "        lon_slice = slice(lon_min_ds, lon_max_ds)\n",
    "        out = ds.sel(latitude=lat_slice, longitude=lon_slice)\n",
    "    else:\n",
    "        left  = ds.sel(latitude=lat_slice, longitude=slice(lon_min_ds, float(lon.values.max())))\n",
    "        right = ds.sel(latitude=lat_slice, longitude=slice(float(lon.values.min()), lon_max_ds))\n",
    "        out = xr.concat([left, right], dim=\"longitude\")\n",
    "\n",
    "    if out.sizes.get(\"latitude\", 0) == 0 or out.sizes.get(\"longitude\", 0) == 0:\n",
    "        raise RuntimeError(\"BBox slice returned empty selection. Check bounds & lon domain.\")\n",
    "    return out\n",
    "\n",
    "def nearest_index(vec, value):\n",
    "    return int(np.abs(vec - value).argmin())\n",
    "\n",
    "def roundup_to_multiple(n, m):\n",
    "    \"\"\"Smallest multiple of m that is >= n.\"\"\"\n",
    "    return ((int(n) + m - 1) // m) * m\n",
    "\n",
    "def expand_region_indices(base_lat, base_lon, center_lat_raw, center_lon_raw, target_h, target_w):\n",
    "    \"\"\"\n",
    "    Compute (i_start, i_end, j_start, j_end) on the ORIGINAL grid\n",
    "    around (center_lat_raw, center_lon_raw) for a target_h x target_w window.\n",
    "    \"\"\"\n",
    "    latv = base_lat; lonv = base_lon\n",
    "    dom = detect_lon_domain(lonv)\n",
    "    center_lon = float(to_dataset_lon(center_lon_raw, dom))\n",
    "    center_lat = float(center_lat_raw)\n",
    "\n",
    "    i0 = nearest_index(latv, center_lat)\n",
    "    j0 = nearest_index(lonv, center_lon)\n",
    "\n",
    "    half_h = target_h // 2\n",
    "    half_w = target_w // 2\n",
    "\n",
    "    i_start = max(0, i0 - half_h)\n",
    "    i_end   = min(latv.size, i0 + half_h + (target_h % 2 != 0))\n",
    "    if i_end - i_start < target_h:\n",
    "        deficit = target_h - (i_end - i_start)\n",
    "        i_start = max(0, i_start - deficit//2)\n",
    "        i_end   = min(latv.size, i_end + math.ceil(deficit/2))\n",
    "\n",
    "    j_start = max(0, j0 - half_w)\n",
    "    j_end   = min(lonv.size, j0 + half_w + (target_w % 2 != 0))\n",
    "    if j_end - j_start < target_w:\n",
    "        deficit = target_w - (j_end - j_start)\n",
    "        j_start = max(0, j_start - deficit//2)\n",
    "        j_end   = min(lonv.size, j_end + math.ceil(deficit/2))\n",
    "\n",
    "    # clamp + ensure non-empty\n",
    "    i_start = max(0, min(i_start, latv.size-1))\n",
    "    i_end   = max(i_start+1, min(i_end,   latv.size))\n",
    "    j_start = max(0, min(j_start, lonv.size-1))\n",
    "    j_end   = max(j_start+1, min(j_end,   lonv.size))\n",
    "    return i_start, i_end, j_start, j_end\n",
    "\n",
    "def sizes_multiple_of_ps(ds: xr.Dataset, ps: int) -> bool:\n",
    "    H = ds.sizes[\"latitude\"]; W = ds.sizes[\"longitude\"]\n",
    "    return (H % ps == 0) and (W % ps == 0)\n",
    "\n",
    "def crop_to_multiple_of_ps(ds: xr.Dataset, ps: int) -> xr.Dataset:\n",
    "    \"\"\"Crop dataset so H and W are multiples of patch size (keeps top-left corner).\"\"\"\n",
    "    H = ds.sizes[\"latitude\"]; W = ds.sizes[\"longitude\"]\n",
    "    Hc = (H // ps) * ps\n",
    "    Wc = (W // ps) * ps\n",
    "    return ds.isel(latitude=slice(0, Hc), longitude=slice(0, Wc))\n",
    "\n",
    "def safe_time_tuple(ds):\n",
    "    if \"valid_time\" in ds:\n",
    "        vt = np.asarray(ds.valid_time.values)\n",
    "        if vt.ndim == 0: vt = vt[None]\n",
    "        return tuple(pd.to_datetime(vt).to_pydatetime())\n",
    "    return tuple()\n",
    "\n",
    "# ------------------ load static vars ------------------\n",
    "with open(static_path, \"rb\") as f:\n",
    "    static_vars_full = pickle.load(f)\n",
    "\n",
    "# ------------------ open datasets ------------------\n",
    "surf_base = xr.open_dataset(surf_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "atmo_base = xr.open_dataset(atmo_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "if \"forecast_period\" in surf_base.dims: surf_base = surf_base.isel(forecast_period=0)\n",
    "if \"forecast_period\" in atmo_base.dims: atmo_base = atmo_base.isel(forecast_period=0)\n",
    "\n",
    "# ------------------ model & patch size ------------------\n",
    "model = AuroraAirPollution()\n",
    "model.load_checkpoint()\n",
    "model.eval()\n",
    "ps = int(model.patch_size)\n",
    "print(\"patch_size:\", ps)\n",
    "\n",
    "# Swin3D uses multiple spatial downsamplings; require >= 8 patches per dim\n",
    "MIN_PATCHES_PER_DIM = 8\n",
    "min_cells = MIN_PATCHES_PER_DIM * ps\n",
    "\n",
    "# ------------------ prelim slice (diag) ------------------\n",
    "surf_reg0 = slice_bbox_value(surf_base, BBOX)\n",
    "atmo_reg0 = slice_bbox_value(atmo_base, BBOX)\n",
    "H0, W0 = surf_reg0.sizes[\"latitude\"], surf_reg0.sizes[\"longitude\"]\n",
    "print(f\"Prelim slice size (HxW): {H0} x {W0}\")\n",
    "\n",
    "# ------------------ compute target sizes ------------------\n",
    "target_H = max(min_cells, roundup_to_multiple(max(H0, min_cells), ps))\n",
    "target_W = max(min_cells, roundup_to_multiple(max(W0, min_cells), ps))\n",
    "\n",
    "# ------------------ build single window by indices (and keep slices) ----------\n",
    "lat_center_raw = 0.5 * (BBOX[\"lat_min\"] + BBOX[\"lat_max\"])\n",
    "lon_center_raw = 0.5 * (BBOX[\"lon_min\"] + BBOX[\"lon_max\"])\n",
    "\n",
    "# index expansion on ORIGINAL grids (use surf_base as reference for both)\n",
    "i_start, i_end, j_start, j_end = expand_region_indices(\n",
    "    surf_base.latitude.values, surf_base.longitude.values,\n",
    "    lat_center_raw, lon_center_raw,\n",
    "    target_H, target_W\n",
    ")\n",
    "\n",
    "surf_win = surf_base.isel(latitude=slice(i_start, i_end), longitude=slice(j_start, j_end))\n",
    "atmo_win = atmo_base.isel(latitude=slice(i_start, i_end), longitude=slice(j_start, j_end))\n",
    "\n",
    "# enforce orientation\n",
    "surf_win = ensure_model_orientation(surf_win)\n",
    "atmo_win = ensure_model_orientation(atmo_win)\n",
    "\n",
    "# crop to multiples of ps\n",
    "surf_ds = crop_to_multiple_of_ps(surf_win, ps)\n",
    "atmo_ds = crop_to_multiple_of_ps(atmo_win, ps)\n",
    "\n",
    "H, W = surf_ds.sizes[\"latitude\"], surf_ds.sizes[\"longitude\"]\n",
    "print(f\"Window after crop (HxW): {H} x {W}\")\n",
    "assert H >= min_cells and W >= min_cells, f\"Need >= {min_cells} cells per dim (got {H}x{W}, ps={ps})\"\n",
    "assert sizes_multiple_of_ps(surf_ds, ps) and sizes_multiple_of_ps(atmo_ds, ps), \"Not multiples of patch size.\"\n",
    "\n",
    "# ------------------ cut static grids to the same indices ------------------\n",
    "def slice_static_like(i_start, i_end, j_start, j_end, orient_like: xr.Dataset, ps: int):\n",
    "    cut = {}\n",
    "    for k, arr in static_vars_full.items():\n",
    "        if arr.ndim == 3:  # (C,H,W)\n",
    "            arr_cut = arr[:, i_start:i_end, j_start:j_end]\n",
    "        elif arr.ndim == 2:  # (H,W)\n",
    "            arr_cut = arr[i_start:i_end, j_start:j_end]\n",
    "        else:\n",
    "            cut[k] = arr\n",
    "            continue\n",
    "        # mirror latitude flip done by ensure_model_orientation (if any)\n",
    "        flipped_lat = not (np.all(np.diff(surf_win.latitude.values) < 0)) if surf_win.latitude.size >= 2 else False\n",
    "        if flipped_lat:\n",
    "            if arr_cut.ndim == 3:\n",
    "                arr_cut = arr_cut[:, ::-1, :]\n",
    "            else:\n",
    "                arr_cut = arr_cut[::-1, :]\n",
    "        # crop to multiples of ps\n",
    "        if arr_cut.ndim == 3:\n",
    "            Hc = (arr_cut.shape[1] // ps) * ps\n",
    "            Wc = (arr_cut.shape[2] // ps) * ps\n",
    "            arr_cut = arr_cut[:, :Hc, :Wc]\n",
    "        elif arr_cut.ndim == 2:\n",
    "            Hc = (arr_cut.shape[0] // ps) * ps\n",
    "            Wc = (arr_cut.shape[1] // ps) * ps\n",
    "            arr_cut = arr_cut[:Hc, :Wc]\n",
    "        cut[k] = arr_cut\n",
    "    return cut\n",
    "\n",
    "static_vars_tile = slice_static_like(i_start, i_end, j_start, j_end, surf_ds, ps)\n",
    "\n",
    "# ------------------ single full-window inference ------------------\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "surf_vars = {\n",
    "    \"2t\":    torch.from_numpy(surf_ds[\"t2m\" ].values[None]),\n",
    "    \"10u\":   torch.from_numpy(surf_ds[\"u10\" ].values[None]),\n",
    "    \"10v\":   torch.from_numpy(surf_ds[\"v10\" ].values[None]),\n",
    "    \"msl\":   torch.from_numpy(surf_ds[\"msl\" ].values[None]),\n",
    "    \"pm1\":   torch.from_numpy(surf_ds[\"pm1\" ].values[None]),\n",
    "    \"pm2p5\": torch.from_numpy(surf_ds[\"pm2p5\"].values[None]),\n",
    "    \"pm10\":  torch.from_numpy(surf_ds[\"pm10\"].values[None]),\n",
    "    \"tcco\":  torch.from_numpy(surf_ds[\"tcco\"].values[None]),\n",
    "    \"tc_no\": torch.from_numpy(surf_ds[\"tc_no\"].values[None]),\n",
    "    \"tcno2\": torch.from_numpy(surf_ds[\"tcno2\"].values[None]),\n",
    "    \"gtco3\": torch.from_numpy(surf_ds[\"gtco3\"].values[None]),\n",
    "    \"tcso2\": torch.from_numpy(surf_ds[\"tcso2\"].values[None]),\n",
    "}\n",
    "atmos_vars = {\n",
    "    \"t\":   torch.from_numpy(atmo_ds[\"t\" ].values[None]),\n",
    "    \"u\":   torch.from_numpy(atmo_ds[\"u\" ].values[None]),\n",
    "    \"v\":   torch.from_numpy(atmo_ds[\"v\" ].values[None]),\n",
    "    \"q\":   torch.from_numpy(atmo_ds[\"q\" ].values[None]),\n",
    "    \"z\":   torch.from_numpy(atmo_ds[\"z\"].values[None]),\n",
    "    \"co\":  torch.from_numpy(atmo_ds[\"co\"].values[None]),\n",
    "    \"no\":  torch.from_numpy(atmo_ds[\"no\"].values[None]),\n",
    "    \"no2\": torch.from_numpy(atmo_ds[\"no2\"].values[None]),\n",
    "    \"go3\": torch.from_numpy(atmo_ds[\"go3\"].values[None]),\n",
    "    \"so2\": torch.from_numpy(atmo_ds[\"so2\"].values[None]),\n",
    "}\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars=surf_vars,\n",
    "    static_vars={k: torch.from_numpy(v) for k, v in static_vars_tile.items()},\n",
    "    atmos_vars=atmos_vars,\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(atmo_ds.latitude.values),   # strictly decreasing\n",
    "        lon=torch.from_numpy(atmo_ds.longitude.values),  # strictly increasing\n",
    "        time=safe_time_tuple(atmo_ds),\n",
    "        atmos_levels=tuple(int(x) for x in atmo_ds.pressure_level.values),\n",
    "    ),\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    pred = model(batch)\n",
    "\n",
    "# PM2.5 prediction (kg/m^3 -> µg/m^3)\n",
    "pm25_kgm3 = pred.surf_vars[\"pm2p5\"][0, 0].cpu().numpy().astype(np.float32)\n",
    "pm25_ugm3 = pm25_kgm3 / 1e-9\n",
    "\n",
    "# ------------------ assemble CSV (no plotting) ------------------\n",
    "# Consistent orientation for export: lat increasing for human readability\n",
    "lat_vec = atmo_ds.latitude.values.copy()   # decreasing\n",
    "lon_vec = atmo_ds.longitude.values.copy()  # increasing\n",
    "lat_out = lat_vec.copy()\n",
    "pm25_out = pm25_ugm3.copy()\n",
    "flip_lat = lat_out[0] > lat_out[-1]\n",
    "if flip_lat:\n",
    "    lat_out = lat_out[::-1]\n",
    "    pm25_out = pm25_out[::-1, :]\n",
    "\n",
    "# Helper to extract + orient 2D fields like PM1/PM10 (surface) in µg/m^3\n",
    "def surf_pm_as_ugm3(varname):\n",
    "    arr = surf_ds[varname].values  # (time?, H, W) or (H,W)\n",
    "    if arr.ndim == 3: arr = arr[0]\n",
    "    arr = arr / 1e-9  # kg/m^3 -> µg/m^3\n",
    "    if flip_lat: arr = arr[::-1, :]\n",
    "    return arr\n",
    "\n",
    "pm1_ugm3  = surf_pm_as_ugm3(\"pm1\")\n",
    "pm10_ugm3 = surf_pm_as_ugm3(\"pm10\")\n",
    "\n",
    "# Near-surface gases from the atmospheric cube:\n",
    "# Pick the highest pressure level (closest to the surface)\n",
    "plevs = atmo_ds.pressure_level.values\n",
    "k_surface = int(np.argmax(plevs))  # highest pressure\n",
    "\n",
    "def atmo_surface_field(name):\n",
    "    v = atmo_ds[name].values  # (plev, H, W) or (time?,plev,H,W)\n",
    "    if v.ndim == 4: v = v[0, k_surface]\n",
    "    elif v.ndim == 3: v = v[k_surface]\n",
    "    else: raise RuntimeError(f\"Unexpected dims for {name}: {v.shape}\")\n",
    "    if flip_lat: v = v[::-1, :]\n",
    "    return v\n",
    "\n",
    "ozone = atmo_surface_field(\"go3\")\n",
    "nitrogen_dioxide = atmo_surface_field(\"no2\")\n",
    "nitrogen_monoxide = atmo_surface_field(\"no\")\n",
    "carbon_monoxide   = atmo_surface_field(\"co\")\n",
    "sulfur_dioxide    = atmo_surface_field(\"so2\")\n",
    "\n",
    "# Build 2D lon/lat grids\n",
    "LON2D, LAT2D = np.meshgrid(lon_vec, lat_out)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"latitude\":           LAT2D.ravel(),\n",
    "    \"longitude\":          LON2D.ravel(),\n",
    "    \"pm2p5_ugm3\":         pm25_out.ravel(),\n",
    "    \"pm1_ugm3\":           pm1_ugm3.ravel(),\n",
    "    \"pm10_ugm3\":          pm10_ugm3.ravel(),\n",
    "    \"ozone\":              ozone.ravel(),\n",
    "    \"nitrogen_dioxide\":   nitrogen_dioxide.ravel(),\n",
    "    \"nitrogen_monoxide\":  nitrogen_monoxide.ravel(),\n",
    "    \"carbon_monoxide\":    carbon_monoxide.ravel(),\n",
    "    \"sulfur_dioxide\":     sulfur_dioxide.ravel(),\n",
    "})\n",
    "\n",
    "# Timestamp (if present)\n",
    "try:\n",
    "    vt = np.asarray(atmo_ds.valid_time.values)\n",
    "    if vt.ndim == 0: vt = vt[0:1]\n",
    "    sel_time = pd.to_datetime(vt[0]).to_pydatetime()\n",
    "except Exception:\n",
    "    sel_time = None\n",
    "if sel_time is not None:\n",
    "    df.insert(0, \"time\", sel_time)\n",
    "\n",
    "# Drop NaNs and save\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved CSV: {csv_path}\")\n",
    "print(f\"Final working size (HxW): {pm25_kgm3.shape[0]} x {pm25_kgm3.shape[1]}  (ps={ps}, min required: {MIN_PATCHES_PER_DIM*ps})\")\n",
    "print(df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
