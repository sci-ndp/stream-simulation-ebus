{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff9ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static variables downloaded!\n",
      "Surface-level and atmospheric variables downloaded!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import cdsapi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Data will be downloaded here.\n",
    "download_path = Path(\"~/downloads/cams\")\n",
    "\n",
    "download_path = download_path.expanduser()\n",
    "download_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the static variables from HuggingFace.\n",
    "static_path = hf_hub_download(\n",
    "    repo_id=\"microsoft/aurora\",\n",
    "    filename=\"aurora-0.4-air-pollution-static.pickle\",\n",
    "    local_dir=\"~/downloads/cams\"\n",
    ")\n",
    "print(\"Static variables downloaded!\")\n",
    "\n",
    "# Download the surface-level variables.\n",
    "if not (download_path / \"2025-10-20_cams_24h_forecast_utah.nc.zip\").exists():\n",
    "    c = cdsapi.Client()\n",
    "    c.retrieve(\n",
    "        \"cams-global-atmospheric-composition-forecasts\",\n",
    "        {\n",
    "            \"type\": \"forecast\",\n",
    "            \"format\": \"netcdf_zip\",\n",
    "            \"date\": \"2025-10-20\",\n",
    "            \"time\": [\"00:00\"],  # ONE forecast initialization\n",
    "            \"leadtime_hour\": [str(h) for h in range(0, 25)],  # 0–24h forecast\n",
    "            \"area\": [42.1, -112.0, 37.0, -107.0],  # North, West, South, East (Utah)\n",
    "            \"variable\": [\n",
    "                \"10m_u_component_of_wind\", \"10m_v_component_of_wind\",\n",
    "                \"2m_temperature\", \"mean_sea_level_pressure\",\n",
    "                \"particulate_matter_1um\", \"particulate_matter_2.5um\",\n",
    "                \"particulate_matter_10um\",\n",
    "                \"total_column_carbon_monoxide\", \"total_column_nitrogen_monoxide\",\n",
    "                \"total_column_nitrogen_dioxide\", \"total_column_ozone\",\n",
    "                \"total_column_sulphur_dioxide\",\n",
    "                \"u_component_of_wind\", \"v_component_of_wind\",\n",
    "                \"temperature\", \"geopotential\", \"specific_humidity\",\n",
    "                \"carbon_monoxide\", \"nitrogen_dioxide\", \"nitrogen_monoxide\",\n",
    "                \"ozone\", \"sulphur_dioxide\",\n",
    "            ],\n",
    "            \"pressure_level\": [\n",
    "                \"50\",\"100\",\"150\",\"200\",\"250\",\"300\",\"400\",\n",
    "                \"500\",\"600\",\"700\",\"850\",\"925\",\"1000\",\n",
    "            ],\n",
    "        },\n",
    "        str(download_path / \"2025-10-20_cams_24h_forecast_utah.nc.zip\"),\n",
    "    )\n",
    "# Unpack the ZIP. It should contain the surface-level and atmospheric data in separate\n",
    "# files.\n",
    "if not (download_path / \"2025-10-20-cams_24h_forecast_utah-surface-level.nc\").exists():\n",
    "    with zipfile.ZipFile(download_path / \"2025-10-20_cams_24h_forecast_utah.nc.zip\", \"r\") as zf, open(\n",
    "        download_path / \"2025-10-20-cams_24h_forecast_utah-surface-level.nc\", \"wb\"\n",
    "    ) as f:\n",
    "        f.write(zf.read(\"data_sfc.nc\"))\n",
    "if not (download_path / \"2025-10-20-cams_24h_forecast_utah-atmospheric.nc\").exists():\n",
    "    with zipfile.ZipFile(download_path / \"2025-10-20_cams_24h_forecast_utah.nc.zip\", \"r\") as zf, open(\n",
    "        download_path / \"2025-10-20-cams_24h_forecast_utah-atmospheric.nc\", \"wb\"\n",
    "    ) as f:\n",
    "        f.write(zf.read(\"data_plev.nc\"))\n",
    "print(\"Surface-level and atmospheric variables downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bdd6d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_size: 3\n",
      "Lead coord: forecast_period | periods: 25\n",
      "Final grid size (H x W): 15 x 15\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 227\u001b[39m\n\u001b[32m    214\u001b[39m batch = Batch(\n\u001b[32m    215\u001b[39m     surf_vars=surf_vars,\n\u001b[32m    216\u001b[39m     static_vars=static_tile_torch,  \u001b[38;5;66;03m# (H,W) each; Batch will broadcast as needed\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    223\u001b[39m     ),\n\u001b[32m    224\u001b[39m )\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Take the **current** step surface predictions (index 0, current history index is 1 internally)\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtake_curr\u001b[39m(arr):\n\u001b[32m    231\u001b[39m     \u001b[38;5;66;03m# pred.surf_vars[...] has shape (B, T_out, H, W); for single-step it's (1,1,H,W)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/aurora/model/aurora.py:321\u001b[39m, in \u001b[36mAurora.forward\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    318\u001b[39m transformed_batch = \u001b[38;5;28mself\u001b[39m._pre_encoder_hook(transformed_batch)\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# The encoder is always just run.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformed_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlead_time\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[38;5;66;03m# In BF16 mode, the backbone is run in pure BF16.\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16_mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/stream-simulation-ebus/.venv/lib/python3.12/site-packages/aurora/model/encoder.py:217\u001b[39m, in \u001b[36mPerceiver3DEncoder.forward\u001b[39m\u001b[34m(self, batch, lead_time)\u001b[39m\n\u001b[32m    214\u001b[39m x_static = torch.stack(\u001b[38;5;28mtuple\u001b[39m(batch.static_vars.values()), dim=\u001b[32m2\u001b[39m)\n\u001b[32m    215\u001b[39m x_atmos = torch.stack(\u001b[38;5;28mtuple\u001b[39m(batch.atmos_vars.values()), dim=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m B, T, _, C, H, W = x_atmos.size()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m x_surf.shape[:\u001b[32m2\u001b[39m] == (B, T), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(B,\u001b[38;5;250m \u001b[39mT)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_surf.shape[:\u001b[32m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m static_vars \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 6)"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import xarray as xr\n",
    "\n",
    "from aurora import Batch, Metadata, AuroraAirPollution\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# ------------------ paths ------------------\n",
    "download_path = Path(\"~/downloads/cams\").expanduser()\n",
    "surf_path = download_path / \"2025-10-20-cams_24h_forecast_utah-surface-level.nc\"\n",
    "atmo_path = download_path / \"2025-10-20-cams_24h_forecast_utah-atmospheric.nc\"\n",
    "static_path = download_path / \"aurora-0.4-air-pollution-static.pickle\"\n",
    "\n",
    "out_dir = Path(\"./predictions\").expanduser()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = out_dir / \"aurora_hourly_utah_2025-10-20.csv\"\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def normalize_lat_lon(ds, lat_name=\"latitude\", lon_name=\"longitude\"):\n",
    "    # lon -> [0,360), lon increasing; lat decreasing (north->south)\n",
    "    if lon_name not in ds.coords or lat_name not in ds.coords:\n",
    "        raise RuntimeError(f\"Dataset missing coords '{lat_name}' and/or '{lon_name}'.\")\n",
    "    lon_wrapped = (ds[lon_name] % 360)\n",
    "    ds = ds.assign_coords({lon_name: lon_wrapped})\n",
    "    ds = ds.sortby(lon_name, ascending=True)\n",
    "    lat = ds[lat_name]\n",
    "    if lat.size >= 2 and float(lat[0]) < float(lat[-1]):\n",
    "        ds = ds.sortby(lat_name, ascending=False)\n",
    "    return ds\n",
    "\n",
    "def grid_spacing_1d(arr_1d, default=0.5):\n",
    "    if arr_1d.size >= 2:\n",
    "        return abs(float(arr_1d[-1]) - float(arr_1d[-2]))\n",
    "    return default\n",
    "\n",
    "def pad_to_patch_multiple_1dcoords(ds, patch_size, lat_name=\"latitude\", lon_name=\"longitude\"):\n",
    "    \"\"\"\n",
    "    Pad by edge-replication so H and W are multiples of patch_size and >= patch_size.\n",
    "    Adjust 1-D lat/lon coordinates consistently.\n",
    "    \"\"\"\n",
    "    H = ds.sizes[lat_name]\n",
    "    W = ds.sizes[lon_name]\n",
    "    target_H = max(patch_size, int(np.ceil(H / patch_size)) * patch_size)\n",
    "    target_W = max(patch_size, int(np.ceil(W / patch_size)) * patch_size)\n",
    "    pad_h = max(0, target_H - H)\n",
    "    pad_w = max(0, target_W - W)\n",
    "    if pad_h == 0 and pad_w == 0:\n",
    "        return ds\n",
    "\n",
    "    lat0 = ds[lat_name].values\n",
    "    lon0 = ds[lon_name].values\n",
    "    dlat = grid_spacing_1d(lat0)\n",
    "    dlon = grid_spacing_1d(lon0)\n",
    "\n",
    "    # pad data\n",
    "    ds = ds.pad({lat_name: (0, pad_h), lon_name: (0, pad_w)}, mode=\"edge\")\n",
    "\n",
    "    # extend coords\n",
    "    if pad_h > 0:\n",
    "        lat_tail = np.array([lat0[-1] - dlat * (i + 1) for i in range(pad_h)], dtype=np.float64)\n",
    "        new_lat = np.concatenate([lat0, lat_tail])\n",
    "    else:\n",
    "        new_lat = lat0\n",
    "\n",
    "    if pad_w > 0:\n",
    "        lon_tail = np.array([lon0[-1] + dlon * (i + 1) for i in range(pad_w)], dtype=np.float64)\n",
    "        lon_tail = np.mod(lon_tail, 360.0)\n",
    "        new_lon = np.concatenate([lon0, lon_tail])\n",
    "    else:\n",
    "        new_lon = lon0\n",
    "\n",
    "    ds = ds.assign_coords({lat_name: (lat_name, new_lat), lon_name: (lon_name, new_lon)})\n",
    "    ds = ds.sortby(lon_name, ascending=True)\n",
    "    ds = ds.sortby(lat_name, ascending=False)\n",
    "    return ds\n",
    "\n",
    "def resize_nn(arr2d: np.ndarray, out_h: int, out_w: int) -> np.ndarray:\n",
    "    \"\"\"Nearest-neighbour resize of a 2D array to (out_h, out_w) without external deps.\"\"\"\n",
    "    in_h, in_w = arr2d.shape\n",
    "    y_idx = np.floor(np.linspace(0, in_h - 1, out_h)).astype(int)\n",
    "    x_idx = np.floor(np.linspace(0, in_w - 1, out_w)).astype(int)\n",
    "    return arr2d[y_idx][:, x_idx]\n",
    "\n",
    "def build_static_tile_resized(static_vars_full: dict, target_shape: tuple[int, int]) -> dict:\n",
    "    \"\"\"\n",
    "    Resize each 2D static layer to target_shape via nearest neighbour.\n",
    "    Ignores non-2D entries. Returns dict of np.float32 arrays (H, W).\n",
    "    \"\"\"\n",
    "    Ht, Wt = target_shape\n",
    "    out = {}\n",
    "    for k, v in static_vars_full.items():\n",
    "        arr = np.array(v)\n",
    "        if arr.ndim != 2:\n",
    "            continue\n",
    "        out[k] = resize_nn(arr, Ht, Wt).astype(np.float32)\n",
    "    if not out:\n",
    "        raise RuntimeError(\"No 2D static layers found in the pickle.\")\n",
    "    return out\n",
    "\n",
    "def get_pred_surf(pred, keys):\n",
    "    if isinstance(keys, (list, tuple)):\n",
    "        for k in keys:\n",
    "            if k in pred.surf_vars:\n",
    "                return pred.surf_vars[k][0, 0].cpu().numpy()\n",
    "        return None\n",
    "    else:\n",
    "        k = keys\n",
    "        if k in pred.surf_vars:\n",
    "            return pred.surf_vars[k][0, 0].cpu().numpy()\n",
    "        return None\n",
    "\n",
    "# ------------------ load static ------------------\n",
    "with open(static_path, \"rb\") as f:\n",
    "    static_vars_full = pickle.load(f)\n",
    "\n",
    "# ------------------ open CAMS ------------------\n",
    "surf_all = xr.open_dataset(surf_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "atmo_all = xr.open_dataset(atmo_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "\n",
    "# Normalize coords\n",
    "surf_all = normalize_lat_lon(surf_all, \"latitude\", \"longitude\")\n",
    "atmo_all = normalize_lat_lon(atmo_all, \"latitude\", \"longitude\")\n",
    "\n",
    "# ------------------ model ------------------\n",
    "model = AuroraAirPollution()\n",
    "model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.4-air-pollution.ckpt\")\n",
    "model.eval()\n",
    "\n",
    "ps = getattr(model, \"patch_size\", 16)\n",
    "print(\"patch_size:\", ps)\n",
    "\n",
    "# Pad to multiples of patch size (and ≥ patch size)\n",
    "surf_all = pad_to_patch_multiple_1dcoords(surf_all, ps, \"latitude\", \"longitude\")\n",
    "atmo_all = pad_to_patch_multiple_1dcoords(atmo_all, ps, \"latitude\", \"longitude\")\n",
    "\n",
    "# Lead-time coordinate\n",
    "lead_name = \"forecast_period\" if \"forecast_period\" in surf_all.coords else (\"step\" if \"step\" in surf_all.coords else None)\n",
    "if lead_name is None:\n",
    "    raise RuntimeError(\"Could not find forecast lead coordinate ('forecast_period' or 'step').\")\n",
    "num_periods = int(surf_all.sizes[lead_name])\n",
    "print(f\"Lead coord: {lead_name} | periods: {num_periods}\")\n",
    "\n",
    "# ----- FINAL GRID SHAPE (1-D coords) -----\n",
    "lat_1d = atmo_all.latitude.values  # shape (H,)\n",
    "lon_1d = atmo_all.longitude.values # shape (W,)\n",
    "assert lat_1d.ndim == 1 and lon_1d.ndim == 1\n",
    "assert np.all((lon_1d >= 0) & (lon_1d < 360)), \"Aurora expects lon in [0, 360).\"\n",
    "\n",
    "H, W = lat_1d.shape[0], lon_1d.shape[0]\n",
    "print(f\"Final grid size (H x W): {H} x {W}\")\n",
    "\n",
    "# Build 2-D lat/lon grids for Metadata + CSV\n",
    "lat_2d, lon_2d = np.meshgrid(lat_1d, lon_1d, indexing=\"ij\")  # (H,W) each\n",
    "\n",
    "# Resize static layers ONCE to (H,W)\n",
    "static_tile = build_static_tile_resized(static_vars_full, (H, W))\n",
    "static_tile_torch = {k: torch.from_numpy(v) for k, v in static_tile.items()}\n",
    "\n",
    "# ------------------ inference loop (T=2 history) ------------------\n",
    "rows = []\n",
    "\n",
    "# We need a previous slice and a current slice -> start from period=1\n",
    "for period in range(1, num_periods):\n",
    "    surf_prev  = surf_all.isel({lead_name: period - 1})\n",
    "    surf_curr  = surf_all.isel({lead_name: period})\n",
    "    atmos_prev = atmo_all.isel({lead_name: period - 1})\n",
    "    atmos_curr = atmo_all.isel({lead_name: period})\n",
    "\n",
    "    # times (prev, curr)\n",
    "    t_prev = np.asarray(atmos_prev.valid_time.values, dtype=\"datetime64[s]\").item()\n",
    "    t_curr = np.asarray(atmos_curr.valid_time.values, dtype=\"datetime64[s]\").item()\n",
    "\n",
    "    # Stack T=2 along the time axis -> final shape (B=1, T=2, H, W)\n",
    "    def T2(x_prev, x_curr, var):\n",
    "        a = x_prev[var].values\n",
    "        b = x_curr[var].values\n",
    "        return torch.from_numpy(np.stack([a, b], axis=0)[None])\n",
    "\n",
    "    surf_vars = {\n",
    "        \"2t\":    T2(surf_prev,  surf_curr,  \"t2m\"),\n",
    "        \"10u\":   T2(surf_prev,  surf_curr,  \"u10\"),\n",
    "        \"10v\":   T2(surf_prev,  surf_curr,  \"v10\"),\n",
    "        \"msl\":   T2(surf_prev,  surf_curr,  \"msl\"),\n",
    "        \"pm1\":   T2(surf_prev,  surf_curr,  \"pm1\"),\n",
    "        \"pm2p5\": T2(surf_prev,  surf_curr,  \"pm2p5\"),\n",
    "        \"pm10\":  T2(surf_prev,  surf_curr,  \"pm10\"),\n",
    "        \"tcco\":  T2(surf_prev,  surf_curr,  \"tcco\"),\n",
    "        \"tc_no\": T2(surf_prev,  surf_curr,  \"tc_no\"),\n",
    "        \"tcno2\": T2(surf_prev,  surf_curr,  \"tcno2\"),\n",
    "        \"gtco3\": T2(surf_prev,  surf_curr,  \"gtco3\"),\n",
    "        \"tcso2\": T2(surf_prev,  surf_curr,  \"tcso2\"),\n",
    "    }\n",
    "\n",
    "    atmos_vars = {\n",
    "        \"t\":   T2(atmos_prev, atmos_curr, \"t\"),\n",
    "        \"u\":   T2(atmos_prev, atmos_curr, \"u\"),\n",
    "        \"v\":   T2(atmos_prev, atmos_curr, \"v\"),\n",
    "        \"q\":   T2(atmos_prev, atmos_curr, \"q\"),\n",
    "        \"z\":   T2(atmos_prev, atmos_curr, \"z\"),\n",
    "        \"co\":  T2(atmos_prev, atmos_curr, \"co\"),\n",
    "        \"no\":  T2(atmos_prev, atmos_curr, \"no\"),\n",
    "        \"no2\": T2(atmos_prev, atmos_curr, \"no2\"),\n",
    "        \"go3\": T2(atmos_prev, atmos_curr, \"go3\"),\n",
    "        \"so2\": T2(atmos_prev, atmos_curr, \"so2\"),\n",
    "    }\n",
    "\n",
    "    batch = Batch(\n",
    "        surf_vars=surf_vars,\n",
    "        static_vars=static_tile_torch,  # (H,W) each; Batch will broadcast as needed\n",
    "        atmos_vars=atmos_vars,\n",
    "        metadata=Metadata(\n",
    "            lat=torch.from_numpy(lat_2d),   # (H,W)\n",
    "            lon=torch.from_numpy(lon_2d),   # (H,W)\n",
    "            time=(t_prev, t_curr),          # T=2 history\n",
    "            atmos_levels=tuple(int(level) for level in atmos_curr.pressure_level.values),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        pred = model(batch)\n",
    "\n",
    "    # Take the **current** step surface predictions (index 0, current history index is 1 internally)\n",
    "    def take_curr(arr):\n",
    "        # pred.surf_vars[...] has shape (B, T_out, H, W); for single-step it's (1,1,H,W)\n",
    "        return arr[0, 0].cpu().numpy()\n",
    "\n",
    "    pm1  = take_curr(pred.surf_vars[\"pm1\"])\n",
    "    pm25 = take_curr(pred.surf_vars[\"pm2p5\"])\n",
    "    pm10 = take_curr(pred.surf_vars[\"pm10\"])\n",
    "    co   = take_curr(pred.surf_vars[\"co\"])\n",
    "    no   = take_curr(pred.surf_vars[\"no\"])\n",
    "    no2  = take_curr(pred.surf_vars[\"no2\"])\n",
    "    o3   = take_curr(pred.surf_vars.get(\"o3\", pred.surf_vars.get(\"go3\")))\n",
    "    so2  = take_curr(pred.surf_vars[\"so2\"])\n",
    "\n",
    "    # Convert PM kg/m^3 -> µg/m^3\n",
    "    pm1  = pm1  * 1e9\n",
    "    pm25 = pm25 * 1e9\n",
    "    pm10 = pm10 * 1e9\n",
    "\n",
    "    # Flatten -> rows (use our 2-D grids)\n",
    "    size = H * W\n",
    "    row_df = pd.DataFrame({\n",
    "        \"timestamp\": np.repeat(np.datetime64(t_curr), size),\n",
    "        \"lat\": lat_2d.ravel(),\n",
    "        \"lon\": lon_2d.ravel(),\n",
    "        \"pm1_ugm3\":   pm1.ravel(),\n",
    "        \"pm2p5_ugm3\": pm25.ravel(),\n",
    "        \"pm10_ugm3\":  pm10.ravel(),\n",
    "        \"co\":         co.ravel(),\n",
    "        \"no\":         no.ravel(),\n",
    "        \"no2\":        no2.ravel(),\n",
    "        \"o3\":         o3.ravel(),\n",
    "        \"so2\":        so2.ravel(),\n",
    "    })\n",
    "    rows.append(row_df)\n",
    "\n",
    "    # cleanup\n",
    "    del batch, pred, row_df, pm1, pm25, pm10, co, no, no2, o3, so2\n",
    "    gc.collect()\n",
    "\n",
    "# ------------------ write CSV ------------------\n",
    "df = pd.concat(rows, ignore_index=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved: {csv_path}  (rows={len(df)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "159e29db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static variables downloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 11:19:09,439 INFO Request ID is 3c5c3810-3da7-435d-b9ad-3fdef37fa322\n",
      "2025-10-28 11:19:09,615 INFO status has been updated to accepted\n",
      "2025-10-28 11:19:23,677 INFO status has been updated to running\n",
      "2025-10-28 11:20:26,068 INFO status has been updated to successful\n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface-level and atmospheric variables downloaded!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import cdsapi\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Data will be downloaded here.\n",
    "download_path = Path(\"~/downloads/cams\")\n",
    "\n",
    "download_path = download_path.expanduser()\n",
    "download_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the static variables from HuggingFace.\n",
    "static_path = hf_hub_download(\n",
    "    repo_id=\"microsoft/aurora\",\n",
    "    filename=\"aurora-0.4-air-pollution-static.pickle\",\n",
    "    local_dir=\"~/downloads/cams\"\n",
    ")\n",
    "print(\"Static variables downloaded!\")\n",
    "\n",
    "# Download the surface-level variables.\n",
    "if not (download_path / \"2025-10-20-cams.nc.zip\").exists():\n",
    "    c = cdsapi.Client()\n",
    "    c.retrieve(\n",
    "        \"cams-global-atmospheric-composition-forecasts\",\n",
    "        {\n",
    "            \"type\": \"forecast\",\n",
    "            \"leadtime_hour\": \"0\",\n",
    "            \"variable\": [\n",
    "                # Meteorological surface-level variables:\n",
    "                \"10m_u_component_of_wind\",\n",
    "                \"10m_v_component_of_wind\",\n",
    "                \"2m_temperature\",\n",
    "                \"mean_sea_level_pressure\",\n",
    "                # Pollution surface-level variables:\n",
    "                \"particulate_matter_1um\",\n",
    "                \"particulate_matter_2.5um\",\n",
    "                \"particulate_matter_10um\",\n",
    "                \"total_column_carbon_monoxide\",\n",
    "                \"total_column_nitrogen_monoxide\",\n",
    "                \"total_column_nitrogen_dioxide\",\n",
    "                \"total_column_ozone\",\n",
    "                \"total_column_sulphur_dioxide\",\n",
    "                # Meteorological atmospheric variables:\n",
    "                \"u_component_of_wind\",\n",
    "                \"v_component_of_wind\",\n",
    "                \"temperature\",\n",
    "                \"geopotential\",\n",
    "                \"specific_humidity\",\n",
    "                # Pollution atmospheric variables:\n",
    "                \"carbon_monoxide\",\n",
    "                \"nitrogen_dioxide\",\n",
    "                \"nitrogen_monoxide\",\n",
    "                \"ozone\",\n",
    "                \"sulphur_dioxide\",\n",
    "            ],\n",
    "            \"pressure_level\": [\n",
    "                \"50\",\n",
    "                \"100\",\n",
    "                \"150\",\n",
    "                \"200\",\n",
    "                \"250\",\n",
    "                \"300\",\n",
    "                \"400\",\n",
    "                \"500\",\n",
    "                \"600\",\n",
    "                \"700\",\n",
    "                \"850\",\n",
    "                \"925\",\n",
    "                \"1000\",\n",
    "            ],\n",
    "            \"date\": \"2025-10-20\",\n",
    "            \"time\": [\"00:00\", \"12:00\"],\n",
    "            \"format\": \"netcdf_zip\",\n",
    "        },\n",
    "        str(download_path / \"2025-10-20-cams.nc.zip\"),\n",
    "    )\n",
    "# Unpack the ZIP. It should contain the surface-level and atmospheric data in separate\n",
    "# files.\n",
    "if not (download_path / \"2025-10-20-cams-surface-level.nc\").exists():\n",
    "    with zipfile.ZipFile(download_path / \"2025-10-20-cams.nc.zip\", \"r\") as zf, open(\n",
    "        download_path / \"2025-10-20-cams-surface-level.nc\", \"wb\"\n",
    "    ) as f:\n",
    "        f.write(zf.read(\"data_sfc.nc\"))\n",
    "if not (download_path / \"2025-10-20-cams-atmospheric.nc\").exists():\n",
    "    with zipfile.ZipFile(download_path / \"2025-10-20-cams.nc.zip\", \"r\") as zf, open(\n",
    "        download_path / \"2025-10-20-cams-atmospheric.nc\", \"wb\"\n",
    "    ) as f:\n",
    "        f.write(zf.read(\"data_plev.nc\"))\n",
    "print(\"Surface-level and atmospheric variables downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_size: 3\n",
      "Prelim slice size (HxW): 1 x 1\n",
      "Window after crop (HxW): 24 x 24\n",
      "Saved CSV: predictions/pollutant_prediction_utah.csv\n",
      "Final working size (HxW): 24 x 24  (ps=3, min required: 24)\n",
      "        time  latitude  longitude  pm2p5_ugm3  pm1_ugm3  pm10_ugm3  \\\n",
      "0 2025-10-20      36.4      243.2    6.144454  2.515953   4.962630   \n",
      "1 2025-10-20      36.4      243.6    6.554677  3.670983   7.366880   \n",
      "2 2025-10-20      36.4      244.0    7.013986  3.739067   7.631770   \n",
      "3 2025-10-20      36.4      244.4    6.963577  3.815977   7.465333   \n",
      "4 2025-10-20      36.4      244.8    7.344985  4.195292   7.300032   \n",
      "5 2025-10-20      36.4      245.2    7.183073  3.435723   5.515376   \n",
      "6 2025-10-20      36.4      245.6    6.622933  2.241485   3.401483   \n",
      "7 2025-10-20      36.4      246.0    6.147038  1.801929   2.471069   \n",
      "8 2025-10-20      36.4      246.4    5.931744  1.503060   2.060887   \n",
      "9 2025-10-20      36.4      246.8    5.405423  1.307036   2.122733   \n",
      "\n",
      "          ozone  nitrogen_dioxide  nitrogen_monoxide  carbon_monoxide  \\\n",
      "0  8.035612e-08      2.233387e-10       1.710365e-11     8.876351e-08   \n",
      "1  8.248930e-08      2.516964e-10       2.042189e-11     9.176075e-08   \n",
      "2  8.449174e-08      2.454010e-10       2.061373e-11     9.413385e-08   \n",
      "3  8.892888e-08      7.356116e-10       5.587086e-11     9.794282e-08   \n",
      "4  9.233644e-08      3.221360e-09       2.144214e-10     1.080116e-07   \n",
      "5  9.386481e-08      3.387293e-09       2.138529e-10     1.106657e-07   \n",
      "6  9.487135e-08      1.194914e-09       6.709033e-11     1.062586e-07   \n",
      "7  9.228872e-08      5.552474e-10       3.245848e-11     1.049632e-07   \n",
      "8  8.761069e-08      2.321991e-10       1.594547e-11     1.012961e-07   \n",
      "9  8.507983e-08      1.718741e-10       1.178169e-11     9.576026e-08   \n",
      "\n",
      "   sulfur_dioxide  \n",
      "0    2.182583e-10  \n",
      "1    2.265432e-10  \n",
      "2    2.229337e-10  \n",
      "3    5.737713e-10  \n",
      "4    1.104930e-09  \n",
      "5    9.973826e-10  \n",
      "6    5.673906e-10  \n",
      "7    4.039089e-10  \n",
      "8    2.859304e-10  \n",
      "9    2.378551e-10  \n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "\n",
    "# ALL DATA Create UTAH SLICE \n",
    "# PM2.5 from CAMS via Microsoft Aurora — Utah/SLC subset\n",
    "# Robust single full-window inference with >= 8 * patch_size per dim\n",
    "# - Ensures sizes are multiples of patch_size and lat↓ / lon↑\n",
    "# - Returns index slices to cut static grids consistently\n",
    "# - NO PLOT; CSV includes pm1, pm10, ozone, NO2, NO, CO, SO2 columns\n",
    "\n",
    "#####################################\n",
    "\n",
    "import gc, math, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "\n",
    "from aurora import Batch, Metadata, AuroraAirPollution\n",
    "\n",
    "# ------------------ config / paths ------------------\n",
    "download_path = Path(\"~/downloads/cams\").expanduser()\n",
    "surf_path = download_path / \"2025-10-20-cams-surface-level.nc\"\n",
    "atmo_path = download_path / \"2025-10-20-cams-atmospheric.nc\"\n",
    "static_path = hf_hub_download(\"microsoft/aurora\", \"aurora-0.4-air-pollution-static.pickle\")\n",
    "\n",
    "out_dir = Path(\"./predictions\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = out_dir / \"pollutant_prediction_utah.csv\"\n",
    "\n",
    "# ------------------ REGION BOUNDS (degrees) ------------------\n",
    "# Use 0..360 longitudes \n",
    "UTAH_BBOX = dict(lat_min=37.0, lat_max=42.1, lon_min=245.9, lon_max=251.0)\n",
    "SLC_BBOX  = dict(lat_min=40.4, lat_max=41.1, lon_min=247.7, lon_max=248.4)\n",
    "\n",
    "# Choose region here:\n",
    "BBOX = SLC_BBOX   # or UTAH_BBOX\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def detect_lon_domain(ds_lon: np.ndarray) -> str:\n",
    "    lon_min = float(np.nanmin(ds_lon)); lon_max = float(np.nanmax(ds_lon))\n",
    "    return \"0_360\" if lon_min >= 0 and lon_max > 180 else \"-180_180\"\n",
    "\n",
    "def to_dataset_lon(lon_vals, target_domain: str):\n",
    "    \"\"\"Map longitude(s) into the dataset's domain. Accepts scalar/array.\"\"\"\n",
    "    arr = np.asarray(lon_vals, dtype=float)\n",
    "    if target_domain == \"0_360\":\n",
    "        arr = arr % 360.0\n",
    "        arr = np.where(arr < 0, arr + 360.0, arr)\n",
    "    else:\n",
    "        arr = ((arr + 180.0) % 360.0) - 180.0\n",
    "    return float(arr) if np.ndim(lon_vals) == 0 else arr\n",
    "\n",
    "def ensure_model_orientation(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Ensure lon strictly increasing, lat strictly decreasing (Aurora requirement).\"\"\"\n",
    "    out = ds\n",
    "    if out.longitude.size >= 2 and not np.all(np.diff(out.longitude.values) > 0):\n",
    "        out = out.isel(longitude=np.argsort(out.longitude.values))\n",
    "    if out.latitude.size >= 2 and not np.all(np.diff(out.latitude.values) < 0):\n",
    "        out = out.isel(latitude=slice(None, None, -1))\n",
    "    if out.latitude.size >= 2:\n",
    "        assert np.all(np.diff(out.latitude.values) < 0), \"lat must be strictly decreasing\"\n",
    "    if out.longitude.size >= 2:\n",
    "        assert np.all(np.diff(out.longitude.values) > 0), \"lon must be strictly increasing\"\n",
    "    return out\n",
    "\n",
    "def slice_bbox_value(ds: xr.Dataset, bbox: dict) -> xr.Dataset:\n",
    "    \"\"\"Value-based bbox slice (handles lon domain + any lat order).\"\"\"\n",
    "    lat = ds.latitude; lon = ds.longitude\n",
    "    lon_domain = detect_lon_domain(lon.values)\n",
    "    lon_min_ds, lon_max_ds = to_dataset_lon([bbox[\"lon_min\"], bbox[\"lon_max\"]], lon_domain)\n",
    "    lat_min, lat_max = bbox[\"lat_min\"], bbox[\"lat_max\"]\n",
    "\n",
    "    if lat[0] > lat[-1]:\n",
    "        lat_slice = slice(lat_max, lat_min)  # descending\n",
    "    else:\n",
    "        lat_slice = slice(lat_min, lat_max)  # ascending\n",
    "\n",
    "    if lon_min_ds <= lon_max_ds:\n",
    "        lon_slice = slice(lon_min_ds, lon_max_ds)\n",
    "        out = ds.sel(latitude=lat_slice, longitude=lon_slice)\n",
    "    else:\n",
    "        left  = ds.sel(latitude=lat_slice, longitude=slice(lon_min_ds, float(lon.values.max())))\n",
    "        right = ds.sel(latitude=lat_slice, longitude=slice(float(lon.values.min()), lon_max_ds))\n",
    "        out = xr.concat([left, right], dim=\"longitude\")\n",
    "\n",
    "    if out.sizes.get(\"latitude\", 0) == 0 or out.sizes.get(\"longitude\", 0) == 0:\n",
    "        raise RuntimeError(\"BBox slice returned empty selection. Check bounds & lon domain.\")\n",
    "    return out\n",
    "\n",
    "def nearest_index(vec, value):\n",
    "    return int(np.abs(vec - value).argmin())\n",
    "\n",
    "def roundup_to_multiple(n, m):\n",
    "    \"\"\"Smallest multiple of m that is >= n.\"\"\"\n",
    "    return ((int(n) + m - 1) // m) * m\n",
    "\n",
    "def expand_region_indices(base_lat, base_lon, center_lat_raw, center_lon_raw, target_h, target_w):\n",
    "    \"\"\"\n",
    "    Compute (i_start, i_end, j_start, j_end) on the ORIGINAL grid\n",
    "    around (center_lat_raw, center_lon_raw) for a target_h x target_w window.\n",
    "    \"\"\"\n",
    "    latv = base_lat; lonv = base_lon\n",
    "    dom = detect_lon_domain(lonv)\n",
    "    center_lon = float(to_dataset_lon(center_lon_raw, dom))\n",
    "    center_lat = float(center_lat_raw)\n",
    "\n",
    "    i0 = nearest_index(latv, center_lat)\n",
    "    j0 = nearest_index(lonv, center_lon)\n",
    "\n",
    "    half_h = target_h // 2\n",
    "    half_w = target_w // 2\n",
    "\n",
    "    i_start = max(0, i0 - half_h)\n",
    "    i_end   = min(latv.size, i0 + half_h + (target_h % 2 != 0))\n",
    "    if i_end - i_start < target_h:\n",
    "        deficit = target_h - (i_end - i_start)\n",
    "        i_start = max(0, i_start - deficit//2)\n",
    "        i_end   = min(latv.size, i_end + math.ceil(deficit/2))\n",
    "\n",
    "    j_start = max(0, j0 - half_w)\n",
    "    j_end   = min(lonv.size, j0 + half_w + (target_w % 2 != 0))\n",
    "    if j_end - j_start < target_w:\n",
    "        deficit = target_w - (j_end - j_start)\n",
    "        j_start = max(0, j_start - deficit//2)\n",
    "        j_end   = min(lonv.size, j_end + math.ceil(deficit/2))\n",
    "\n",
    "    # clamp + ensure non-empty\n",
    "    i_start = max(0, min(i_start, latv.size-1))\n",
    "    i_end   = max(i_start+1, min(i_end,   latv.size))\n",
    "    j_start = max(0, min(j_start, lonv.size-1))\n",
    "    j_end   = max(j_start+1, min(j_end,   lonv.size))\n",
    "    return i_start, i_end, j_start, j_end\n",
    "\n",
    "def sizes_multiple_of_ps(ds: xr.Dataset, ps: int) -> bool:\n",
    "    H = ds.sizes[\"latitude\"]; W = ds.sizes[\"longitude\"]\n",
    "    return (H % ps == 0) and (W % ps == 0)\n",
    "\n",
    "def crop_to_multiple_of_ps(ds: xr.Dataset, ps: int) -> xr.Dataset:\n",
    "    \"\"\"Crop dataset so H and W are multiples of patch size (keeps top-left corner).\"\"\"\n",
    "    H = ds.sizes[\"latitude\"]; W = ds.sizes[\"longitude\"]\n",
    "    Hc = (H // ps) * ps\n",
    "    Wc = (W // ps) * ps\n",
    "    return ds.isel(latitude=slice(0, Hc), longitude=slice(0, Wc))\n",
    "\n",
    "def safe_time_tuple(ds):\n",
    "    if \"valid_time\" in ds:\n",
    "        vt = np.asarray(ds.valid_time.values)\n",
    "        if vt.ndim == 0: vt = vt[None]\n",
    "        return tuple(pd.to_datetime(vt).to_pydatetime())\n",
    "    return tuple()\n",
    "\n",
    "# ------------------ load static vars ------------------\n",
    "with open(static_path, \"rb\") as f:\n",
    "    static_vars_full = pickle.load(f)\n",
    "\n",
    "# ------------------ open datasets ------------------\n",
    "surf_base = xr.open_dataset(surf_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "atmo_base = xr.open_dataset(atmo_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "if \"forecast_period\" in surf_base.dims: surf_base = surf_base.isel(forecast_period=0)\n",
    "if \"forecast_period\" in atmo_base.dims: atmo_base = atmo_base.isel(forecast_period=0)\n",
    "\n",
    "# ------------------ model & patch size ------------------\n",
    "model = AuroraAirPollution()\n",
    "model.load_checkpoint()\n",
    "model.eval()\n",
    "ps = int(model.patch_size)\n",
    "print(\"patch_size:\", ps)\n",
    "\n",
    "# Swin3D uses multiple spatial downsamplings; require >= 8 patches per dim\n",
    "MIN_PATCHES_PER_DIM = 8\n",
    "min_cells = MIN_PATCHES_PER_DIM * ps\n",
    "\n",
    "# ------------------ prelim slice (diag) ------------------\n",
    "surf_reg0 = slice_bbox_value(surf_base, BBOX)\n",
    "atmo_reg0 = slice_bbox_value(atmo_base, BBOX)\n",
    "H0, W0 = surf_reg0.sizes[\"latitude\"], surf_reg0.sizes[\"longitude\"]\n",
    "print(f\"Prelim slice size (HxW): {H0} x {W0}\")\n",
    "\n",
    "# ------------------ compute target sizes ------------------\n",
    "target_H = max(min_cells, roundup_to_multiple(max(H0, min_cells), ps))\n",
    "target_W = max(min_cells, roundup_to_multiple(max(W0, min_cells), ps))\n",
    "\n",
    "# ------------------ build single window by indices (and keep slices) ----------\n",
    "lat_center_raw = 0.5 * (BBOX[\"lat_min\"] + BBOX[\"lat_max\"])\n",
    "lon_center_raw = 0.5 * (BBOX[\"lon_min\"] + BBOX[\"lon_max\"])\n",
    "\n",
    "# index expansion on ORIGINAL grids (use surf_base as reference for both)\n",
    "i_start, i_end, j_start, j_end = expand_region_indices(\n",
    "    surf_base.latitude.values, surf_base.longitude.values,\n",
    "    lat_center_raw, lon_center_raw,\n",
    "    target_H, target_W\n",
    ")\n",
    "\n",
    "surf_win = surf_base.isel(latitude=slice(i_start, i_end), longitude=slice(j_start, j_end))\n",
    "atmo_win = atmo_base.isel(latitude=slice(i_start, i_end), longitude=slice(j_start, j_end))\n",
    "\n",
    "# enforce orientation\n",
    "surf_win = ensure_model_orientation(surf_win)\n",
    "atmo_win = ensure_model_orientation(atmo_win)\n",
    "\n",
    "# crop to multiples of ps\n",
    "surf_ds = crop_to_multiple_of_ps(surf_win, ps)\n",
    "atmo_ds = crop_to_multiple_of_ps(atmo_win, ps)\n",
    "\n",
    "H, W = surf_ds.sizes[\"latitude\"], surf_ds.sizes[\"longitude\"]\n",
    "print(f\"Window after crop (HxW): {H} x {W}\")\n",
    "assert H >= min_cells and W >= min_cells, f\"Need >= {min_cells} cells per dim (got {H}x{W}, ps={ps})\"\n",
    "assert sizes_multiple_of_ps(surf_ds, ps) and sizes_multiple_of_ps(atmo_ds, ps), \"Not multiples of patch size.\"\n",
    "\n",
    "# ------------------ cut static grids to the same indices ------------------\n",
    "def slice_static_like(i_start, i_end, j_start, j_end, orient_like: xr.Dataset, ps: int):\n",
    "    cut = {}\n",
    "    for k, arr in static_vars_full.items():\n",
    "        if arr.ndim == 3:  # (C,H,W)\n",
    "            arr_cut = arr[:, i_start:i_end, j_start:j_end]\n",
    "        elif arr.ndim == 2:  # (H,W)\n",
    "            arr_cut = arr[i_start:i_end, j_start:j_end]\n",
    "        else:\n",
    "            cut[k] = arr\n",
    "            continue\n",
    "        # mirror latitude flip done by ensure_model_orientation (if any)\n",
    "        flipped_lat = not (np.all(np.diff(surf_win.latitude.values) < 0)) if surf_win.latitude.size >= 2 else False\n",
    "        if flipped_lat:\n",
    "            if arr_cut.ndim == 3:\n",
    "                arr_cut = arr_cut[:, ::-1, :]\n",
    "            else:\n",
    "                arr_cut = arr_cut[::-1, :]\n",
    "        # crop to multiples of ps\n",
    "        if arr_cut.ndim == 3:\n",
    "            Hc = (arr_cut.shape[1] // ps) * ps\n",
    "            Wc = (arr_cut.shape[2] // ps) * ps\n",
    "            arr_cut = arr_cut[:, :Hc, :Wc]\n",
    "        elif arr_cut.ndim == 2:\n",
    "            Hc = (arr_cut.shape[0] // ps) * ps\n",
    "            Wc = (arr_cut.shape[1] // ps) * ps\n",
    "            arr_cut = arr_cut[:Hc, :Wc]\n",
    "        cut[k] = arr_cut\n",
    "    return cut\n",
    "\n",
    "static_vars_tile = slice_static_like(i_start, i_end, j_start, j_end, surf_ds, ps)\n",
    "\n",
    "# ------------------ single full-window inference ------------------\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "surf_vars = {\n",
    "    \"2t\":    torch.from_numpy(surf_ds[\"t2m\" ].values[None]),\n",
    "    \"10u\":   torch.from_numpy(surf_ds[\"u10\" ].values[None]),\n",
    "    \"10v\":   torch.from_numpy(surf_ds[\"v10\" ].values[None]),\n",
    "    \"msl\":   torch.from_numpy(surf_ds[\"msl\" ].values[None]),\n",
    "    \"pm1\":   torch.from_numpy(surf_ds[\"pm1\" ].values[None]),\n",
    "    \"pm2p5\": torch.from_numpy(surf_ds[\"pm2p5\"].values[None]),\n",
    "    \"pm10\":  torch.from_numpy(surf_ds[\"pm10\"].values[None]),\n",
    "    \"tcco\":  torch.from_numpy(surf_ds[\"tcco\"].values[None]),\n",
    "    \"tc_no\": torch.from_numpy(surf_ds[\"tc_no\"].values[None]),\n",
    "    \"tcno2\": torch.from_numpy(surf_ds[\"tcno2\"].values[None]),\n",
    "    \"gtco3\": torch.from_numpy(surf_ds[\"gtco3\"].values[None]),\n",
    "    \"tcso2\": torch.from_numpy(surf_ds[\"tcso2\"].values[None]),\n",
    "}\n",
    "atmos_vars = {\n",
    "    \"t\":   torch.from_numpy(atmo_ds[\"t\" ].values[None]),\n",
    "    \"u\":   torch.from_numpy(atmo_ds[\"u\" ].values[None]),\n",
    "    \"v\":   torch.from_numpy(atmo_ds[\"v\" ].values[None]),\n",
    "    \"q\":   torch.from_numpy(atmo_ds[\"q\" ].values[None]),\n",
    "    \"z\":   torch.from_numpy(atmo_ds[\"z\"].values[None]),\n",
    "    \"co\":  torch.from_numpy(atmo_ds[\"co\"].values[None]),\n",
    "    \"no\":  torch.from_numpy(atmo_ds[\"no\"].values[None]),\n",
    "    \"no2\": torch.from_numpy(atmo_ds[\"no2\"].values[None]),\n",
    "    \"go3\": torch.from_numpy(atmo_ds[\"go3\"].values[None]),\n",
    "    \"so2\": torch.from_numpy(atmo_ds[\"so2\"].values[None]),\n",
    "}\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars=surf_vars,\n",
    "    static_vars={k: torch.from_numpy(v) for k, v in static_vars_tile.items()},\n",
    "    atmos_vars=atmos_vars,\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(atmo_ds.latitude.values),   # strictly decreasing\n",
    "        lon=torch.from_numpy(atmo_ds.longitude.values),  # strictly increasing\n",
    "        time=safe_time_tuple(atmo_ds),\n",
    "        atmos_levels=tuple(int(x) for x in atmo_ds.pressure_level.values),\n",
    "    ),\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    pred = model(batch)\n",
    "\n",
    "# PM2.5 prediction (kg/m^3 -> µg/m^3)\n",
    "pm25_kgm3 = pred.surf_vars[\"pm2p5\"][0, 0].cpu().numpy().astype(np.float32)\n",
    "pm25_ugm3 = pm25_kgm3 / 1e-9\n",
    "\n",
    "# ------------------ assemble CSV (no plotting) ------------------\n",
    "# Consistent orientation for export: lat increasing for human readability\n",
    "lat_vec = atmo_ds.latitude.values.copy()   # decreasing\n",
    "lon_vec = atmo_ds.longitude.values.copy()  # increasing\n",
    "lat_out = lat_vec.copy()\n",
    "pm25_out = pm25_ugm3.copy()\n",
    "flip_lat = lat_out[0] > lat_out[-1]\n",
    "if flip_lat:\n",
    "    lat_out = lat_out[::-1]\n",
    "    pm25_out = pm25_out[::-1, :]\n",
    "\n",
    "# Helper to extract + orient 2D fields like PM1/PM10 (surface) in µg/m^3\n",
    "def surf_pm_as_ugm3(varname):\n",
    "    arr = surf_ds[varname].values  # (time?, H, W) or (H,W)\n",
    "    if arr.ndim == 3: arr = arr[0]\n",
    "    arr = arr / 1e-9  # kg/m^3 -> µg/m^3\n",
    "    if flip_lat: arr = arr[::-1, :]\n",
    "    return arr\n",
    "\n",
    "pm1_ugm3  = surf_pm_as_ugm3(\"pm1\")\n",
    "pm10_ugm3 = surf_pm_as_ugm3(\"pm10\")\n",
    "\n",
    "# Near-surface gases from the atmospheric cube:\n",
    "# Pick the highest pressure level (closest to the surface)\n",
    "plevs = atmo_ds.pressure_level.values\n",
    "k_surface = int(np.argmax(plevs))  # highest pressure\n",
    "\n",
    "def atmo_surface_field(name):\n",
    "    v = atmo_ds[name].values  # (plev, H, W) or (time?,plev,H,W)\n",
    "    if v.ndim == 4: v = v[0, k_surface]\n",
    "    elif v.ndim == 3: v = v[k_surface]\n",
    "    else: raise RuntimeError(f\"Unexpected dims for {name}: {v.shape}\")\n",
    "    if flip_lat: v = v[::-1, :]\n",
    "    return v\n",
    "\n",
    "ozone = atmo_surface_field(\"go3\")\n",
    "nitrogen_dioxide = atmo_surface_field(\"no2\")\n",
    "nitrogen_monoxide = atmo_surface_field(\"no\")\n",
    "carbon_monoxide   = atmo_surface_field(\"co\")\n",
    "sulfur_dioxide    = atmo_surface_field(\"so2\")\n",
    "\n",
    "# Build 2D lon/lat grids\n",
    "LON2D, LAT2D = np.meshgrid(lon_vec, lat_out)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"latitude\":           LAT2D.ravel(),\n",
    "    \"longitude\":          LON2D.ravel(),\n",
    "    \"pm2p5_ugm3\":         pm25_out.ravel(),\n",
    "    \"pm1_ugm3\":           pm1_ugm3.ravel(),\n",
    "    \"pm10_ugm3\":          pm10_ugm3.ravel(),\n",
    "    \"ozone\":              ozone.ravel(),\n",
    "    \"nitrogen_dioxide\":   nitrogen_dioxide.ravel(),\n",
    "    \"nitrogen_monoxide\":  nitrogen_monoxide.ravel(),\n",
    "    \"carbon_monoxide\":    carbon_monoxide.ravel(),\n",
    "    \"sulfur_dioxide\":     sulfur_dioxide.ravel(),\n",
    "})\n",
    "\n",
    "# Timestamp (if present)\n",
    "try:\n",
    "    vt = np.asarray(atmo_ds.valid_time.values)\n",
    "    if vt.ndim == 0: vt = vt[0:1]\n",
    "    sel_time = pd.to_datetime(vt[0]).to_pydatetime()\n",
    "except Exception:\n",
    "    sel_time = None\n",
    "if sel_time is not None:\n",
    "    df.insert(0, \"time\", sel_time)\n",
    "\n",
    "# Drop NaNs and save\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved CSV: {csv_path}\")\n",
    "print(f\"Final working size (HxW): {pm25_kgm3.shape[0]} x {pm25_kgm3.shape[1]}  (ps={ps}, min required: {MIN_PATCHES_PER_DIM*ps})\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fdfb9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_size: 3\n",
      "Prelim slice size (HxW): 2 x 1\n",
      "Window after crop (HxW): 12 x 12\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Need >= 24 cells per dim (got 12x12, ps=3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 210\u001b[39m\n\u001b[32m    208\u001b[39m H, W = surf_ds.sizes[\u001b[33m\"\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m\"\u001b[39m], surf_ds.sizes[\u001b[33m\"\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    209\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWindow after crop (HxW): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m x \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m H >= min_cells \u001b[38;5;129;01mand\u001b[39;00m W >= min_cells, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNeed >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cells per dim (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, ps=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sizes_multiple_of_ps(surf_ds, ps) \u001b[38;5;129;01mand\u001b[39;00m sizes_multiple_of_ps(atmo_ds, ps), \u001b[33m\"\u001b[39m\u001b[33mNot multiples of patch size.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# ------------------ cut static grids to the same indices ------------------\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: Need >= 24 cells per dim (got 12x12, ps=3)"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "\n",
    "# UTAH DATA with LEAD TIME\n",
    "# PM2.5 from CAMS via Microsoft Aurora — Utah/SLC subset\n",
    "# Robust single full-window inference with >= 8 * patch_size per dim\n",
    "# - Ensures sizes are multiples of patch_size and lat↓ / lon↑\n",
    "# - Returns index slices to cut static grids consistently\n",
    "# - NO PLOT; CSV includes pm1, pm10, ozone, NO2, NO, CO, SO2 columns\n",
    "\n",
    "#####################################\n",
    "\n",
    "import gc, math, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "\n",
    "from aurora import Batch, Metadata, AuroraAirPollution\n",
    "\n",
    "# ------------------ config / paths ------------------\n",
    "download_path = Path(\"~/downloads/cams\").expanduser()\n",
    "surf_path = download_path / \"2025-10-20-cams_24h_forecast_utah-surface-level.nc\"\n",
    "atmo_path = download_path / \"2025-10-20-cams_24h_forecast_utah-atmospheric.nc\"\n",
    "static_path = hf_hub_download(\"microsoft/aurora\", \"aurora-0.4-air-pollution-static.pickle\")\n",
    "\n",
    "out_dir = Path(\"./predictions\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = out_dir / \"pollutant_prediction_utah_hourly.csv\"\n",
    "\n",
    "# ------------------ REGION BOUNDS (degrees) ------------------\n",
    "# Use 0..360 longitudes \n",
    "UTAH_BBOX = dict(lat_min=37.0, lat_max=42.1, lon_min=245.9, lon_max=251.0)\n",
    "SLC_BBOX  = dict(lat_min=40.4, lat_max=41.1, lon_min=247.7, lon_max=248.4)\n",
    "\n",
    "# Choose region here:\n",
    "BBOX = SLC_BBOX   # or UTAH_BBOX\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def detect_lon_domain(ds_lon: np.ndarray) -> str:\n",
    "    lon_min = float(np.nanmin(ds_lon)); lon_max = float(np.nanmax(ds_lon))\n",
    "    return \"0_360\" if lon_min >= 0 and lon_max > 180 else \"-180_180\"\n",
    "\n",
    "def to_dataset_lon(lon_vals, target_domain: str):\n",
    "    \"\"\"Map longitude(s) into the dataset's domain. Accepts scalar/array.\"\"\"\n",
    "    arr = np.asarray(lon_vals, dtype=float)\n",
    "    if target_domain == \"0_360\":\n",
    "        arr = arr % 360.0\n",
    "        arr = np.where(arr < 0, arr + 360.0, arr)\n",
    "    else:\n",
    "        arr = ((arr + 180.0) % 360.0) - 180.0\n",
    "    return float(arr) if np.ndim(lon_vals) == 0 else arr\n",
    "\n",
    "def ensure_model_orientation(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Ensure lon strictly increasing, lat strictly decreasing (Aurora requirement).\"\"\"\n",
    "    out = ds\n",
    "    if out.longitude.size >= 2 and not np.all(np.diff(out.longitude.values) > 0):\n",
    "        out = out.isel(longitude=np.argsort(out.longitude.values))\n",
    "    if out.latitude.size >= 2 and not np.all(np.diff(out.latitude.values) < 0):\n",
    "        out = out.isel(latitude=slice(None, None, -1))\n",
    "    if out.latitude.size >= 2:\n",
    "        assert np.all(np.diff(out.latitude.values) < 0), \"lat must be strictly decreasing\"\n",
    "    if out.longitude.size >= 2:\n",
    "        assert np.all(np.diff(out.longitude.values) > 0), \"lon must be strictly increasing\"\n",
    "    return out\n",
    "\n",
    "def slice_bbox_value(ds: xr.Dataset, bbox: dict) -> xr.Dataset:\n",
    "    \"\"\"Value-based bbox slice (handles lon domain + any lat order).\"\"\"\n",
    "    lat = ds.latitude; lon = ds.longitude\n",
    "    lon_domain = detect_lon_domain(lon.values)\n",
    "    lon_min_ds, lon_max_ds = to_dataset_lon([bbox[\"lon_min\"], bbox[\"lon_max\"]], lon_domain)\n",
    "    lat_min, lat_max = bbox[\"lat_min\"], bbox[\"lat_max\"]\n",
    "\n",
    "    if lat[0] > lat[-1]:\n",
    "        lat_slice = slice(lat_max, lat_min)  # descending\n",
    "    else:\n",
    "        lat_slice = slice(lat_min, lat_max)  # ascending\n",
    "\n",
    "    if lon_min_ds <= lon_max_ds:\n",
    "        lon_slice = slice(lon_min_ds, lon_max_ds)\n",
    "        out = ds.sel(latitude=lat_slice, longitude=lon_slice)\n",
    "    else:\n",
    "        left  = ds.sel(latitude=lat_slice, longitude=slice(lon_min_ds, float(lon.values.max())))\n",
    "        right = ds.sel(latitude=lat_slice, longitude=slice(float(lon.values.min()), lon_max_ds))\n",
    "        out = xr.concat([left, right], dim=\"longitude\")\n",
    "\n",
    "    if out.sizes.get(\"latitude\", 0) == 0 or out.sizes.get(\"longitude\", 0) == 0:\n",
    "        raise RuntimeError(\"BBox slice returned empty selection. Check bounds & lon domain.\")\n",
    "    return out\n",
    "\n",
    "def nearest_index(vec, value):\n",
    "    return int(np.abs(vec - value).argmin())\n",
    "\n",
    "def roundup_to_multiple(n, m):\n",
    "    \"\"\"Smallest multiple of m that is >= n.\"\"\"\n",
    "    return ((int(n) + m - 1) // m) * m\n",
    "\n",
    "def expand_region_indices(base_lat, base_lon, center_lat_raw, center_lon_raw, target_h, target_w):\n",
    "    \"\"\"\n",
    "    Compute (i_start, i_end, j_start, j_end) on the ORIGINAL grid\n",
    "    around (center_lat_raw, center_lon_raw) for a target_h x target_w window.\n",
    "    \"\"\"\n",
    "    latv = base_lat; lonv = base_lon\n",
    "    dom = detect_lon_domain(lonv)\n",
    "    center_lon = float(to_dataset_lon(center_lon_raw, dom))\n",
    "    center_lat = float(center_lat_raw)\n",
    "\n",
    "    i0 = nearest_index(latv, center_lat)\n",
    "    j0 = nearest_index(lonv, center_lon)\n",
    "\n",
    "    half_h = target_h // 2\n",
    "    half_w = target_w // 2\n",
    "\n",
    "    i_start = max(0, i0 - half_h)\n",
    "    i_end   = min(latv.size, i0 + half_h + (target_h % 2 != 0))\n",
    "    if i_end - i_start < target_h:\n",
    "        deficit = target_h - (i_end - i_start)\n",
    "        i_start = max(0, i_start - deficit//2)\n",
    "        i_end   = min(latv.size, i_end + math.ceil(deficit/2))\n",
    "\n",
    "    j_start = max(0, j0 - half_w)\n",
    "    j_end   = min(lonv.size, j0 + half_w + (target_w % 2 != 0))\n",
    "    if j_end - j_start < target_w:\n",
    "        deficit = target_w - (j_end - j_start)\n",
    "        j_start = max(0, j_start - deficit//2)\n",
    "        j_end   = min(lonv.size, j_end + math.ceil(deficit/2))\n",
    "\n",
    "    # clamp + ensure non-empty\n",
    "    i_start = max(0, min(i_start, latv.size-1))\n",
    "    i_end   = max(i_start+1, min(i_end,   latv.size))\n",
    "    j_start = max(0, min(j_start, lonv.size-1))\n",
    "    j_end   = max(j_start+1, min(j_end,   lonv.size))\n",
    "    return i_start, i_end, j_start, j_end\n",
    "\n",
    "def sizes_multiple_of_ps(ds: xr.Dataset, ps: int) -> bool:\n",
    "    H = ds.sizes[\"latitude\"]; W = ds.sizes[\"longitude\"]\n",
    "    return (H % ps == 0) and (W % ps == 0)\n",
    "\n",
    "def crop_to_multiple_of_ps(ds: xr.Dataset, ps: int) -> xr.Dataset:\n",
    "    \"\"\"Crop dataset so H and W are multiples of patch size (keeps top-left corner).\"\"\"\n",
    "    H = ds.sizes[\"latitude\"]; W = ds.sizes[\"longitude\"]\n",
    "    Hc = (H // ps) * ps\n",
    "    Wc = (W // ps) * ps\n",
    "    return ds.isel(latitude=slice(0, Hc), longitude=slice(0, Wc))\n",
    "\n",
    "def safe_time_tuple(ds):\n",
    "    if \"valid_time\" in ds:\n",
    "        vt = np.asarray(ds.valid_time.values)\n",
    "        if vt.ndim == 0: vt = vt[None]\n",
    "        return tuple(pd.to_datetime(vt).to_pydatetime())\n",
    "    return tuple()\n",
    "\n",
    "# ------------------ load static vars ------------------\n",
    "with open(static_path, \"rb\") as f:\n",
    "    static_vars_full = pickle.load(f)\n",
    "\n",
    "# ------------------ open datasets ------------------\n",
    "surf_base = xr.open_dataset(surf_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "atmo_base = xr.open_dataset(atmo_path, engine=\"netcdf4\", decode_timedelta=True)\n",
    "if \"forecast_period\" in surf_base.dims: surf_base = surf_base.isel(forecast_period=0)\n",
    "if \"forecast_period\" in atmo_base.dims: atmo_base = atmo_base.isel(forecast_period=0)\n",
    "\n",
    "# ------------------ model & patch size ------------------\n",
    "model = AuroraAirPollution()\n",
    "model.load_checkpoint()\n",
    "model.eval()\n",
    "ps = int(model.patch_size)\n",
    "print(\"patch_size:\", ps)\n",
    "\n",
    "# Swin3D uses multiple spatial downsamplings; require >= 8 patches per dim\n",
    "MIN_PATCHES_PER_DIM = 8\n",
    "min_cells = MIN_PATCHES_PER_DIM * ps\n",
    "\n",
    "# ------------------ prelim slice (diag) ------------------\n",
    "surf_reg0 = slice_bbox_value(surf_base, BBOX)\n",
    "atmo_reg0 = slice_bbox_value(atmo_base, BBOX)\n",
    "H0, W0 = surf_reg0.sizes[\"latitude\"], surf_reg0.sizes[\"longitude\"]\n",
    "print(f\"Prelim slice size (HxW): {H0} x {W0}\")\n",
    "\n",
    "# ------------------ compute target sizes ------------------\n",
    "target_H = max(min_cells, roundup_to_multiple(max(H0, min_cells), ps))\n",
    "target_W = max(min_cells, roundup_to_multiple(max(W0, min_cells), ps))\n",
    "\n",
    "# ------------------ build single window by indices (and keep slices) ----------\n",
    "lat_center_raw = 0.5 * (BBOX[\"lat_min\"] + BBOX[\"lat_max\"])\n",
    "lon_center_raw = 0.5 * (BBOX[\"lon_min\"] + BBOX[\"lon_max\"])\n",
    "\n",
    "# index expansion on ORIGINAL grids (use surf_base as reference for both)\n",
    "i_start, i_end, j_start, j_end = expand_region_indices(\n",
    "    surf_base.latitude.values, surf_base.longitude.values,\n",
    "    lat_center_raw, lon_center_raw,\n",
    "    target_H, target_W\n",
    ")\n",
    "\n",
    "surf_win = surf_base.isel(latitude=slice(i_start, i_end), longitude=slice(j_start, j_end))\n",
    "atmo_win = atmo_base.isel(latitude=slice(i_start, i_end), longitude=slice(j_start, j_end))\n",
    "\n",
    "# enforce orientation\n",
    "surf_win = ensure_model_orientation(surf_win)\n",
    "atmo_win = ensure_model_orientation(atmo_win)\n",
    "\n",
    "# crop to multiples of ps\n",
    "surf_ds = crop_to_multiple_of_ps(surf_win, ps)\n",
    "atmo_ds = crop_to_multiple_of_ps(atmo_win, ps)\n",
    "\n",
    "H, W = surf_ds.sizes[\"latitude\"], surf_ds.sizes[\"longitude\"]\n",
    "print(f\"Window after crop (HxW): {H} x {W}\")\n",
    "assert H >= min_cells and W >= min_cells, f\"Need >= {min_cells} cells per dim (got {H}x{W}, ps={ps})\"\n",
    "assert sizes_multiple_of_ps(surf_ds, ps) and sizes_multiple_of_ps(atmo_ds, ps), \"Not multiples of patch size.\"\n",
    "\n",
    "# ------------------ cut static grids to the same indices ------------------\n",
    "def slice_static_like(i_start, i_end, j_start, j_end, orient_like: xr.Dataset, ps: int):\n",
    "    cut = {}\n",
    "    for k, arr in static_vars_full.items():\n",
    "        if arr.ndim == 3:  # (C,H,W)\n",
    "            arr_cut = arr[:, i_start:i_end, j_start:j_end]\n",
    "        elif arr.ndim == 2:  # (H,W)\n",
    "            arr_cut = arr[i_start:i_end, j_start:j_end]\n",
    "        else:\n",
    "            cut[k] = arr\n",
    "            continue\n",
    "        # mirror latitude flip done by ensure_model_orientation (if any)\n",
    "        flipped_lat = not (np.all(np.diff(surf_win.latitude.values) < 0)) if surf_win.latitude.size >= 2 else False\n",
    "        if flipped_lat:\n",
    "            if arr_cut.ndim == 3:\n",
    "                arr_cut = arr_cut[:, ::-1, :]\n",
    "            else:\n",
    "                arr_cut = arr_cut[::-1, :]\n",
    "        # crop to multiples of ps\n",
    "        if arr_cut.ndim == 3:\n",
    "            Hc = (arr_cut.shape[1] // ps) * ps\n",
    "            Wc = (arr_cut.shape[2] // ps) * ps\n",
    "            arr_cut = arr_cut[:, :Hc, :Wc]\n",
    "        elif arr_cut.ndim == 2:\n",
    "            Hc = (arr_cut.shape[0] // ps) * ps\n",
    "            Wc = (arr_cut.shape[1] // ps) * ps\n",
    "            arr_cut = arr_cut[:Hc, :Wc]\n",
    "        cut[k] = arr_cut\n",
    "    return cut\n",
    "\n",
    "static_vars_tile = slice_static_like(i_start, i_end, j_start, j_end, surf_ds, ps)\n",
    "\n",
    "# ------------------ single full-window inference ------------------\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "surf_vars = {\n",
    "    \"2t\":    torch.from_numpy(surf_ds[\"t2m\" ].values[None]),\n",
    "    \"10u\":   torch.from_numpy(surf_ds[\"u10\" ].values[None]),\n",
    "    \"10v\":   torch.from_numpy(surf_ds[\"v10\" ].values[None]),\n",
    "    \"msl\":   torch.from_numpy(surf_ds[\"msl\" ].values[None]),\n",
    "    \"pm1\":   torch.from_numpy(surf_ds[\"pm1\" ].values[None]),\n",
    "    \"pm2p5\": torch.from_numpy(surf_ds[\"pm2p5\"].values[None]),\n",
    "    \"pm10\":  torch.from_numpy(surf_ds[\"pm10\"].values[None]),\n",
    "    \"tcco\":  torch.from_numpy(surf_ds[\"tcco\"].values[None]),\n",
    "    \"tc_no\": torch.from_numpy(surf_ds[\"tc_no\"].values[None]),\n",
    "    \"tcno2\": torch.from_numpy(surf_ds[\"tcno2\"].values[None]),\n",
    "    \"gtco3\": torch.from_numpy(surf_ds[\"gtco3\"].values[None]),\n",
    "    \"tcso2\": torch.from_numpy(surf_ds[\"tcso2\"].values[None]),\n",
    "}\n",
    "atmos_vars = {\n",
    "    \"t\":   torch.from_numpy(atmo_ds[\"t\" ].values[None]),\n",
    "    \"u\":   torch.from_numpy(atmo_ds[\"u\" ].values[None]),\n",
    "    \"v\":   torch.from_numpy(atmo_ds[\"v\" ].values[None]),\n",
    "    \"q\":   torch.from_numpy(atmo_ds[\"q\" ].values[None]),\n",
    "    \"z\":   torch.from_numpy(atmo_ds[\"z\"].values[None]),\n",
    "    \"co\":  torch.from_numpy(atmo_ds[\"co\"].values[None]),\n",
    "    \"no\":  torch.from_numpy(atmo_ds[\"no\"].values[None]),\n",
    "    \"no2\": torch.from_numpy(atmo_ds[\"no2\"].values[None]),\n",
    "    \"go3\": torch.from_numpy(atmo_ds[\"go3\"].values[None]),\n",
    "    \"so2\": torch.from_numpy(atmo_ds[\"so2\"].values[None]),\n",
    "}\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars=surf_vars,\n",
    "    static_vars={k: torch.from_numpy(v) for k, v in static_vars_tile.items()},\n",
    "    atmos_vars=atmos_vars,\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(atmo_ds.latitude.values),   # strictly decreasing\n",
    "        lon=torch.from_numpy(atmo_ds.longitude.values),  # strictly increasing\n",
    "        time=safe_time_tuple(atmo_ds),\n",
    "        atmos_levels=tuple(int(x) for x in atmo_ds.pressure_level.values),\n",
    "    ),\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    pred = model(batch)\n",
    "\n",
    "# PM2.5 prediction (kg/m^3 -> µg/m^3)\n",
    "pm25_kgm3 = pred.surf_vars[\"pm2p5\"][0, 0].cpu().numpy().astype(np.float32)\n",
    "pm25_ugm3 = pm25_kgm3 / 1e-9\n",
    "\n",
    "# ------------------ assemble CSV (no plotting) ------------------\n",
    "# Consistent orientation for export: lat increasing for human readability\n",
    "lat_vec = atmo_ds.latitude.values.copy()   # decreasing\n",
    "lon_vec = atmo_ds.longitude.values.copy()  # increasing\n",
    "lat_out = lat_vec.copy()\n",
    "pm25_out = pm25_ugm3.copy()\n",
    "flip_lat = lat_out[0] > lat_out[-1]\n",
    "if flip_lat:\n",
    "    lat_out = lat_out[::-1]\n",
    "    pm25_out = pm25_out[::-1, :]\n",
    "\n",
    "# Helper to extract + orient 2D fields like PM1/PM10 (surface) in µg/m^3\n",
    "def surf_pm_as_ugm3(varname):\n",
    "    arr = surf_ds[varname].values  # (time?, H, W) or (H,W)\n",
    "    if arr.ndim == 3: arr = arr[0]\n",
    "    arr = arr / 1e-9  # kg/m^3 -> µg/m^3\n",
    "    if flip_lat: arr = arr[::-1, :]\n",
    "    return arr\n",
    "\n",
    "pm1_ugm3  = surf_pm_as_ugm3(\"pm1\")\n",
    "pm10_ugm3 = surf_pm_as_ugm3(\"pm10\")\n",
    "\n",
    "# Near-surface gases from the atmospheric cube:\n",
    "# Pick the highest pressure level (closest to the surface)\n",
    "plevs = atmo_ds.pressure_level.values\n",
    "k_surface = int(np.argmax(plevs))  # highest pressure\n",
    "\n",
    "def atmo_surface_field(name):\n",
    "    v = atmo_ds[name].values  # (plev, H, W) or (time?,plev,H,W)\n",
    "    if v.ndim == 4: v = v[0, k_surface]\n",
    "    elif v.ndim == 3: v = v[k_surface]\n",
    "    else: raise RuntimeError(f\"Unexpected dims for {name}: {v.shape}\")\n",
    "    if flip_lat: v = v[::-1, :]\n",
    "    return v\n",
    "\n",
    "ozone = atmo_surface_field(\"go3\")\n",
    "nitrogen_dioxide = atmo_surface_field(\"no2\")\n",
    "nitrogen_monoxide = atmo_surface_field(\"no\")\n",
    "carbon_monoxide   = atmo_surface_field(\"co\")\n",
    "sulfur_dioxide    = atmo_surface_field(\"so2\")\n",
    "\n",
    "# Build 2D lon/lat grids\n",
    "LON2D, LAT2D = np.meshgrid(lon_vec, lat_out)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"latitude\":           LAT2D.ravel(),\n",
    "    \"longitude\":          LON2D.ravel(),\n",
    "    \"pm2p5_ugm3\":         pm25_out.ravel(),\n",
    "    \"pm1_ugm3\":           pm1_ugm3.ravel(),\n",
    "    \"pm10_ugm3\":          pm10_ugm3.ravel(),\n",
    "    \"ozone\":              ozone.ravel(),\n",
    "    \"nitrogen_dioxide\":   nitrogen_dioxide.ravel(),\n",
    "    \"nitrogen_monoxide\":  nitrogen_monoxide.ravel(),\n",
    "    \"carbon_monoxide\":    carbon_monoxide.ravel(),\n",
    "    \"sulfur_dioxide\":     sulfur_dioxide.ravel(),\n",
    "})\n",
    "\n",
    "# Timestamp (if present)\n",
    "try:\n",
    "    vt = np.asarray(atmo_ds.valid_time.values)\n",
    "    if vt.ndim == 0: vt = vt[0:1]\n",
    "    sel_time = pd.to_datetime(vt[0]).to_pydatetime()\n",
    "except Exception:\n",
    "    sel_time = None\n",
    "if sel_time is not None:\n",
    "    df.insert(0, \"time\", sel_time)\n",
    "\n",
    "# Drop NaNs and save\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved CSV: {csv_path}\")\n",
    "print(f\"Final working size (HxW): {pm25_kgm3.shape[0]} x {pm25_kgm3.shape[1]}  (ps={ps}, min required: {MIN_PATCHES_PER_DIM*ps})\")\n",
    "print(df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
